<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Giovanni Norbedo" />
  <title>Analisi Numerica</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  
  
  
  
  
  
  
  
  
  
  
  
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Analisi Numerica</h1>
<p class="subtitle">Intelligenza Artificiale &amp; Data Analytics</p>
<p class="author">Giovanni Norbedo</p>
<p class="date">2024-2025</p>
</header>
<nav id="TOC" role="doc-toc">

</nav>
<h2 id="introduzione">Introduzione</h2>
<h3 id="tipi-di-errori">Tipi di errori</h3>
<ul>
<li><p><strong>Errori di modellazione matematica</strong> del problema
reale ed <strong>errori presenti nei dati</strong>
sperimentali.</p></li>
<li><p><strong>Errori di troncamento</strong>: da problema matematico
(dimensione infinita) a problema numerico (dimensione finita).</p></li>
<li><p><strong>Errori di arrotondamento</strong>: sul calcolatore, posso
rappresentare solo un sottoinsieme finito dei numeri reali.</p></li>
</ul>
<h3 id="sistema-posizionale">Sistema posizionale</h3>
<p><strong>Definizione</strong>: Fissata la <em>base</em> <span
class="math inline">\(B \in \mathbb{N}\)</span>, <span
class="math inline">\(B &gt; 1\)</span>, e un numero <span
class="math inline">\(x \in \mathbb{R}\)</span> finito di cifre <span
class="math inline">\(d_k, k = -m, -m + 1, \ldots, n - 1, n\)</span>, di
definisce <span class="math inline">\(x_B\)</span> la
<strong>rappresentazione posizionale</strong> di <span
class="math inline">\(x\)</span> in base <span
class="math inline">\(B\)</span>:</p>
<p><span class="math inline">\(\displaystyle x_B = (-1)^s \cdot \sum_{k
= -m}^n d_k \cdot B^k \quad d_n \neq 0\)</span></p>
<p><strong>Esempi</strong>:<br />
<span class="math inline">\((867.0985)_{10} = (-1)^0 \left( 8 \cdot 10^2
+ 6 \cdot 10^1 + 7 \cdot 10^0 + 0 \cdot 10^{-1} + 9 \cdot 10^{-2} + 8
\cdot 10^{-3} + 5 \cdot 10^{-4} \right)\)</span><br />
<span class="math inline">\((-10110.0001)_2 = (-1)^1 \left( 1 \cdot 2^4
+ 1 \cdot 2^2 + 1 \cdot 2^1 + 1 \cdot 2^0 + 1 \cdot 2^{-4}
\right)\)</span></p>
<p><strong>Osservazione</strong>:<br />
<span class="math inline">\(\forall x \in \mathbb{R}\)</span>, fissata
<span class="math inline">\(B\)</span>,</p>
<p><span class="math inline">\(\displaystyle x_B = (-1)^s \cdot \sum_{k
= 0}^n d_k \cdot B^k + \sum_{k = 1}^{\infty} d_{-k} \cdot
B^{-k}\)</span></p>
<h3 id="perché-la-serie-della-parte-frazionaria-converge">Perché la
serie della parte frazionaria converge?</h3>
<p><strong>Dimostrazione</strong>:<br />
Considero la serie geometrica di ragione <span
class="math inline">\(B^{-1}\)</span> (criterio di confronto tra serie a
termini non negativi).<br />
Poiché <span class="math inline">\(d_{-k} \leq B - 1, \quad \forall k
\in \mathbb{N}\)</span>, allora</p>
<p><span class="math inline">\(\displaystyle \sum_{k = 1}^{\infty} (B -
1) \cdot B^{-k} = (B - 1) \cdot \sum_{k = 1}^{\infty} B^{-k} = (B - 1)
\cdot \dfrac{B^{-1}}{1 - B^{-1}}\)</span> convergente.</p>
<p><strong>Nota</strong>: <span class="math inline">\((0.999\ldots)_{10}
= (0.111\ldots)_2 = 1\)</span><br />
Un numero razionale può avere rappresentazione data da un numero finito
di cifre in una base e infinito in un’altra:<br />
<span class="math inline">\(\displaystyle \dfrac{1}{3} = 0.333\ldots =
0.1_3\)</span></p>
<h3 id="cambio-di-base-da-base-2-a-base-10">Cambio di base da base 2 a
base 10</h3>
<p><strong>Definizione</strong>: Dato un numero <span
class="math inline">\(x\)</span> in base 2, si può convertire in base 10
tramite la formula</p>
<p><span class="math inline">\(\displaystyle x_{10} = \sum_{k = 0}^n d_k
\cdot 2^k + \sum_{k = 1}^{\infty} d_{-k} \cdot 2^{-k}\)</span></p>
<p><strong>Esempio</strong>: <span class="math inline">\((10001000.01)_2
= 2^7 + 2^3 + 2^{-2} = 128 + 8 + 0.25 = 136.25\)</span></p>
<h3 id="cambio-di-base-da-base-10-a-base-2">Cambio di base da base 10 a
base 2</h3>
<h4 id="parte-intera">Parte intera</h4>
<p><strong>Definizione</strong>: Dato un numero <span
class="math inline">\(x\)</span> in base 10, si può convertire in base 2
tramite le seguenti operazioni:</p>
<ol type="1">
<li>Divido la parte intera di <span class="math inline">\(x\)</span> per
2 e scrivo il resto.</li>
<li>Divido il quoziente precedente per 2 e scrivo il resto.</li>
<li>Continuo fino a che il quoziente è 0.</li>
<li>Leggo i resti in ordine inverso.</li>
</ol>
<h4 id="parte-decimale">Parte decimale</h4>
<p><strong>Definizione</strong>: Dato un numero <span
class="math inline">\(x\)</span> in base 10, si può convertire in base 2
tramite le seguenti operazioni:</p>
<ol type="1">
<li>Moltiplico la parte decimale di <span
class="math inline">\(x\)</span> per 2 e scrivo la parte intera.</li>
<li>Continuo fino a che la parte decimale è 0 oppure se si ripetono
periodicamente le stesse cifre.</li>
<li>Leggo le parti intere in ordine.</li>
</ol>
<p><strong>Esempio</strong>: <span
class="math inline">\(389.1_{10}\)</span></p>
<p>Parte intera:</p>
<p><span class="math inline">\(\begin{array}{c|c} 389 &amp; 1 \\ 194
&amp; 0 \\ 97 &amp; 1 \\ 48 &amp; 0 \\ 24 &amp; 0 \\ 12 &amp; 0 \\ 6
&amp; 0 \\ 3 &amp; 1 \\ 1 &amp; 1 \\ 0 &amp; 1 \\ \end{array}\)</span>
<span class="math inline">\(\Rightarrow (110000101)_2\)</span></p>
<p>Parte decimale:</p>
<p><span class="math inline">\(\begin{array}{c|c} 0.2 &amp; 0 \\ 0.4
&amp; 0 \\ 0.8 &amp; 0 \\ 1.6 &amp; 1 \\ 1.2 &amp; 1 \\ 0.4 &amp; 0 \\
0.8 &amp; 0 \\ 1.6 &amp; 1 \\ 1.2 &amp; 1 \\ \end{array}\)</span> <span
class="math inline">\(\Rightarrow
(00011001100110011\ldots)_2\)</span></p>
<p><span class="math inline">\(\Rightarrow 389.1_{10} = (110000101.0
\overline{0011})_2\)</span></p>
<h3
id="rappresentazione-in-virgola-mobile-normalizzata">Rappresentazione in
virgola mobile normalizzata</h3>
<p><strong>Definizione</strong>: Dato un numero <span
class="math inline">\(x \in \mathbb{R}\)</span>, si definisce la
<strong>rappresentazione in virgola mobile normalizzata</strong>
come</p>
<p><span class="math inline">\(\displaystyle x = (-1)^s \cdot B^e \cdot
\sum_{k = 1}^{\infty} d_k \cdot B^{-k} \quad\)</span> con <span
class="math inline">\(\begin{cases} d_1 &gt; 0 \\ 0 \leq d_k &lt; B - 1
\\ e \in \mathbb{Z} \end{cases}\)</span></p>
<p>oppure si può scrivere come</p>
<p><span class="math inline">\(x = \pm p \cdot B^e \quad\)</span> con
<span class="math inline">\(B^{-1} \leq p &lt; 1\)</span></p>
<p>dove <span class="math inline">\(p\)</span> è detto
<strong>mantissa</strong> e <span class="math inline">\(e\)</span> è
detto <strong>esponente</strong>.</p>
<p><strong>Esempi</strong>:<br />
In base 10:<br />
<span class="math inline">\(x = 0.00745 \Rightarrow x = 0.745 \cdot
10^{-2}\)</span><br />
<span class="math inline">\(x = 70408.102 \Rightarrow x = 0.70408102
\cdot 10^5\)</span></p>
<p>In base 2: <span class="math inline">\(x = 11001.111 \Rightarrow x =
0.11001111 \cdot 2^5\)</span></p>
<h3 id="numeri-macchina">Numeri macchina</h3>
<p>Nel calcolatore i numeri reali sono rappresentati in virgola mobile
normalizzata, con <span class="math inline">\(t\)</span> cifre di
mantissa e <span class="math inline">\(e\)</span> esponente, con <span
class="math inline">\(L \leq e \leq U\)</span>.</p>
<p>Fissata una base <span class="math inline">\(B\)</span> (di solito
<span class="math inline">\(B = 2\)</span>), fissati <span
class="math inline">\(t, L &lt; 0, U &gt; 0\)</span>, si definisce
l’insieme dei numeri macchina <span class="math inline">\(\mathbb{F}(B,
t, L, U)\)</span> come</p>
<p><span class="math inline">\(\mathbb{F}(B, t, L, U) = \{ x \vert x =
(-1)^s \cdot B^e \cdot \sum_{k = 1}^t d_k \cdot B^{-k} \} \cup \{ 0
\},\)</span> con <span class="math inline">\(d_1 &gt; 0, 0 \leq d_k &lt;
B - 1, e \in [L, U]\)</span></p>
<p>Lo <strong>zero</strong> è rappresentato come <span
class="math inline">\(0 = 0 \cdot B^0 \cdot 0\)</span>.</p>
<p>Un numero <span class="math inline">\(x \in \mathbb{F}(B, t, L,
U)\)</span> è scritto come</p>
<p><span class="math inline">\(x = (-1)^s \cdot (0.d_1d_2d_3\ldots
d_t)_B \cdot B^e\)</span></p>
<h3 id="numeri-nel-calcolatore">Numeri nel calcolatore</h3>
<p>Base 2: <span class="math inline">\(B = 2\)</span>, cifre <span
class="math inline">\(d_k \in \{0, 1\}\)</span></p>
<p><span class="math inline">\(x = (-1)^s \cdot (0.d_1d_2d_3\ldots
d_t)_2 \cdot 2^e\)</span></p>
<ul>
<li>1 bit per il segno</li>
<li><span class="math inline">\(t\)</span> bit per la mantissa</li>
<li>l’esponent</li>
</ul>
<h4 id="singola-precisione">Singola precisione</h4>
<p>32 bit:</p>
<ul>
<li>1 bit per il segno</li>
<li>8 bit per l’esponente</li>
<li>23 bit per la mantissa (1 bit nascosto)</li>
</ul>
<p><span class="math inline">\(F(2, 24, -126, 127)\)</span></p>
<p>Dei <span class="math inline">\(2^8 = 256\)</span> esponenti, 2 sono
riservati per i numeri speciali (infinito e NaN), quindi rimangono <span
class="math inline">\(2^8 - 2 = 254\)</span> esponenti.</p>
<p>I numeri rappresentabili sono: <span class="math inline">\(2 \cdot (U
- L + 1) \cdot (B - 1) \cdot B^{t - 1} + 1 = 2 \cdot 254 \cdot 2^{23} +
1 \approx 4.3 \cdot 10^9\)</span></p>
<h4 id="doppia-precisione">Doppia precisione</h4>
<p>64 bit:</p>
<ul>
<li>1 bit per il segno</li>
<li>11 bit per l’esponente</li>
<li>52 bit per la mantissa (1 bit nascosto)</li>
</ul>
<p><span class="math inline">\(F(2, 53, -1022, 1023)\)</span></p>
<p>Dei <span class="math inline">\(2^{11} = 2048\)</span> esponenti, 2
sono riservati per i numeri speciali (infinito e NaN), quindi rimangono
<span class="math inline">\(2^{11} - 2 = 2046\)</span> esponenti.</p>
<p>I numeri rappresentabili (cardinalità) sono: <span
class="math inline">\(2 \cdot (U - L + 1) \cdot (B - 1) \cdot B^{t - 1}
+ 1 = 2 \cdot 2046 \cdot 2^{52} + 1 \approx 1.8 \cdot
10^{19}\)</span></p>
<p><strong>Nota</strong>: <span class="math inline">\((B - 1)\)</span> è
il numero massimo di cifre rappresentabili in base <span
class="math inline">\(B\)</span>. <span class="math inline">\(B^{t -
1}\)</span> è il numero di cifre rappresentabili nella mantissa.</p>
<h3 id="half-precision">Half precision</h3>
<p>16 bit:</p>
<ul>
<li>1 bit per il segno</li>
<li>5 bit per l’esponente</li>
<li>10 bit per la mantissa (1 bit nascosto)</li>
</ul>
<p><span class="math inline">\(F(2, 11, -14, 15)\)</span></p>
<p><span class="math inline">\(2^5 = 32\)</span> esponenti, 2 sono
riservati per i numeri speciali (infinito e NaN), quindi rimangono <span
class="math inline">\(2^5 - 2 = 30\)</span> esponenti.</p>
<p>I numeri rappresentabili sono: <span class="math inline">\(2 \cdot (U
- L + 1) \cdot (B - 1) \cdot B^{t - 1} + 1 = 2 \cdot 30 \cdot 2^9 + 1
\approx 3.1 \cdot 10^4\)</span></p>
<h2 id="approssimazione-di-un-numero-reale">Approssimazione di un numero
reale</h2>
<p>In <span class="math inline">\(\mathbb{F}(B, t, L, U)\)</span>, dato
un numero reale <span class="math inline">\(x = p \cdot B^e \in
\mathbb{R}\)</span>, se ha più di <span class="math inline">\(t\)</span>
cifre nella mantissa, si approssima con il numero macchina <span
class="math inline">\(fl(x) \in \mathbb{F}(B, t, L, U)\)</span> in due
modi:</p>
<ul>
<li><strong>Troncamento</strong>: nella mantissa <span
class="math inline">\(p\)</span> si cancellano le cifre oltre la <span
class="math inline">\(t\)</span>-esima.<br />
</li>
<li><strong>Arrotondamento</strong>: nella mantissa <span
class="math inline">\(p\)</span> si aggiunge <span
class="math inline">\(\dfrac{B}{2} \cdot B^{-(t + 1)}\)</span> e poi si
tronca a <span class="math inline">\(t\)</span> cifre.</li>
</ul>
<p><strong>Esempio</strong>: Considero <span class="math inline">\(x =
0.745645897, t = 6\)</span>, per troncamento ho che <span
class="math inline">\(fl(x) = tr(x) = 0.745645\)</span> e per
arrotondamento <span class="math inline">\(fl(x) = tr(x + 0.0000005) =
tr(0.745646397) = 0.745646\)</span>.</p>
<p><strong>Osservazione</strong>: Arrontondare equivale a sommare <span
class="math inline">\(1\)</span> alla <span
class="math inline">\(t\)</span>-esima cifra della mantissa, <span
class="math inline">\(d_t\)</span>, se la successiva cifra, <span
class="math inline">\(d_{t + 1}\)</span>, è <span
class="math inline">\(\geq \dfrac{B}{2}\)</span>, altrimenti la cifra
<span class="math inline">\(t\)</span>-esima rimane invariata.</p>
<h3 id="underflow-e-overflow">Underflow e overflow</h3>
<p>In <span class="math inline">\(\mathbb{F}(B, t, L, U)\)</span>,
nell’approssimare <span class="math inline">\(x\)</span> con <span
class="math inline">\(fl(x)\)</span>, si possono verificare due
situazioni:</p>
<ul>
<li>se l’esponente <span class="math inline">\(e &gt; U\)</span>, si ha
<strong>overflow</strong>: <span class="math inline">\(fl(x) =
\infty\)</span><br />
</li>
<li>se l’esponente <span class="math inline">\(e &lt; L\)</span>, si ha
<strong>underflow</strong>: <span class="math inline">\(fl(x) =
0\)</span></li>
</ul>
<h2 id="errori-assoluti-e-relativi">Errori assoluti e relativi</h2>
<h3 id="maggiorazione-dellerrore-assoluto">Maggiorazione dell’errore
assoluto</h3>
<p>Sia <span class="math inline">\(x \in \mathbb{R}\)</span> e <span
class="math inline">\(x^*\)</span> la sua approssimazione in <span
class="math inline">\(\mathbb{F}(B, t, L, U)\)</span>, si
definiscono:</p>
<ul>
<li><strong>Errore assoluto</strong>: <span class="math inline">\(\vert
x - x^* \vert\)</span><br />
</li>
<li><strong>Errore relativo</strong>: <span
class="math inline">\(\displaystyle \dfrac{\vert x - x^* \vert}{\vert x
\vert}, \quad x \neq 0\)</span></li>
</ul>
<p>Nel caso si abbia a che fare con enti diversi da numeri real
(funzioni, vettori, matrici) le definizioni sono le stesse a patto di
sostituire il valore assoluto con un’opportuna norma.</p>
<h4 id="errore-assoluto-per-troncamento">Errore assoluto per
troncamento</h4>
<p><span class="math inline">\(\vert x - fl(x) \vert = \vert p \cdot B^e
- \bar{p} \cdot B^e \vert = \vert (p - \bar{p}) \cdot B^e \vert \leq
B^{-t} B^e\)</span></p>
<p><span class="math inline">\(p = 0.d_1d_2d_3\ldots d_t \vert d_{t +
1}d_{t + 2}\ldots\)</span><br />
<span class="math inline">\(\bar{p} = 0.d_1d_2d_3\ldots
d_t\)</span><br />
<span class="math inline">\(\vert p - \bar{p} \vert = 0.00\ldots \vert
d_{t + 1}d_{t + 2}\ldots\)</span></p>
<p><span class="math inline">\(\vert p - \bar{p} \vert = \displaystyle
\sum_{k = t + 1}^{\infty} d_k \cdot B^{-k} \leq (B - 1) \cdot \sum_{k =
t + 1}^{\infty} B^{-k} = (B - 1) \cdot \dfrac{B^{}}{} \dots\)</span></p>
<p><strong>Esempio</strong>: <span class="math inline">\(x =
0.745645897, fl(x) = 0.745645, t = 6\)</span><br />
<span class="math inline">\(\vert x - fl(x) \vert = \vert 0.745645897 -
0.745645 \vert = 0.000000897 \leq 10^{-6}\)</span></p>
<h4 id="errore-assoluto-per-arrotondamento">Errore assoluto per
arrotondamento</h4>
<p><span class="math inline">\(\vert x - fl(x) \vert = \vert p \cdot B^e
- \bar{p} \cdot B^e \vert \leq \dfrac{B}{2} \cdot B^{-(t + 1)}
B^{e}\)</span></p>
<p>Fra due numeri macchina consecutivi c’è una distanza di <span
class="math inline">\(\dfrac{B}{2} \cdot B^{-(t + 1)}\)</span>.</p>
<p>Caso A: <span class="math inline">\(d_{t + 1} \geq
\dfrac{B}{2}\)</span></p>
<p><span class="math inline">\(\vert p - \bar{p} \vert = \dfrac{B}{2}
\cdot B^{-(t + 1)}\)</span></p>
<p>Caso B: <span class="math inline">\(d_{t + 1} &lt;
\dfrac{B}{2}\)</span></p>
<p><span class="math inline">\(\vert p - \bar{p} \vert = \left(
\dfrac{B}{2} - 1 \right) \cdot B^{-(t + 1)} + \displaystyle \sum_{k = t
+ 2}^{\infty} d_k \cdot B^{-k} \leq \left( \dfrac{B}{2} - 1 \right)
\cdot B^{-(t + 1)} + (B - 1) \cdot \sum_{k = t + 2}^{\infty}
B^{-k}\)</span> = …</p>
<p><strong>Esempio</strong>: <span class="math inline">\(x =
0.745645897, fl(x) = 0.745646, t = 6\)</span><br />
<span class="math inline">\(\vert x - fl(x) \vert = \vert 0.745645897 -
0.745646 \vert = 0.000000103 \leq 5 \cdot 10^{-7}\)</span></p>
<h3 id="maggiorazione-dellerrore-relativo">Maggiorazione dell’errore
relativo</h3>
<h4 id="errore-relativo-per-troncamento">Errore relativo per
troncamento</h4>
<p><span class="math inline">\(\dfrac{\vert x - fl(x) \vert}{\vert x
\vert} \leq \dfrac{\vert p - \bar{p} \vert \cdot B^e}{p \cdot B^e} =
\dfrac{\vert p - \bar{p} \vert}{p} \leq \dfrac{B^{-t}}{B^{-1}} = B^{1 -
t}\)</span></p>
<h4 id="errore-relativo-per-arrotondamento">Errore relativo per
arrotondamento</h4>
<p><span class="math inline">\(\dfrac{\vert x - fl(x) \vert}{\vert x
\vert} \leq \dfrac{\vert p - \bar{p} \vert \cdot B^e}{p \cdot B^e} =
\dfrac{\vert p - \bar{p} \vert}{p} \leq \dfrac{\dfrac{B}{2} \cdot B^{-(t
+ 1)}}{B^{-1}} = \dfrac{B}{2} \cdot B^{-t} = \dfrac{1}{2} \cdot B^{1 -
t}\)</span></p>
<p>Attualmente, la maggior parte dei sistemi implementa la tecnica di
arrotondamento perché produce mediamente errori più piccoli.</p>
<h2 id="precisione-di-macchina">Precisione di macchina</h2>
<p><strong>Definizione</strong>: Si definisce <strong>precisione di
macchina</strong> (unit roundoff) il più piccolo numero positivo
rappresentabile in <span class="math inline">\(\mathbb{F}(B, t, L,
U)\)</span>, indicato con <span class="math inline">\(u = \dfrac{1}{2}
\cdot B^{1 - t}\)</span>.</p>
<p>La precisione di macchina rappresenta il massimo errore relativo che
si commette nell’approssimare il numero reale <span
class="math inline">\(x\)</span> con il suo corrispondente numero
macchina <span class="math inline">\(fl(x)\)</span> per
arrotondamento.</p>
<p><strong>Esempi</strong>:</p>
<ul>
<li><span class="math inline">\(F(2, 24, -126, 127)\)</span>, <span
class="math inline">\(u = \dfrac{1}{2} \cdot 2^{1 - 24} = 2^{-24}
\approx 6.0 \cdot 10^{-8}\)</span><br />
</li>
<li><span class="math inline">\(F(2, 53, -1022, 1023)\)</span>, <span
class="math inline">\(u = \dfrac{1}{2} \cdot 2^{1 - 53} = 2^{-53}
\approx 1.1 \cdot 10^{-16}\)</span><br />
</li>
<li><span class="math inline">\(F(2, 11, -14, 15)\)</span>, <span
class="math inline">\(u = \dfrac{1}{2} \cdot 2^{1 - 11} = 2^{-11}
\approx 4.9 \cdot 10^{-4}\)</span></li>
</ul>
<h2 id="standard-ansi-ieee-754r">Standard ANSI IEEE-754r</h2>
<p>Scritto nel 1985 e modificato nel 1989 e, più recentemente, nel
2008<br />
costituisce lo standard ufficiale per la rappresentazione binaria dei
numeri all’interno del calcolatore e l’aritmetica di macchina (il nome
dello standard in inglese “Binary floating point arithmetic for
microprocessor systems”).</p>
<p>Secondo lo standard un numero non nullo normalizzato si scrive
come</p>
<p><span class="math inline">\(x = (-1)^s \cdot (1 + f ) \cdot 2^{e^* -
\text{bias}}\)</span></p>
<p>La mantissa si rappresenta dunque come <span
class="math inline">\(1.d_1d_2 \dots d_\tau\)</span> essendo</p>
<p><span class="math inline">\(f = 0.d_1d_2 \dots d_\tau\)</span></p>
<p><span class="math inline">\(\tau\)</span> identifica il numero di bit
usato per codificare la parte frazionaria della mantissa. Il numero di
cifre totali per la mantissa è <span class="math inline">\(t = \tau +
1\)</span>.</p>
<p>Il vero esponente del numero <span class="math inline">\(e\)</span>
si immagazzina in traslazione come<br />
<span class="math inline">\(e^* = e + \text{bias}.\)</span></p>
<p>In questa maniera non serve un bit di segno per l’esponente.</p>
<p>Il bias in singola precisione vale 127 mentre in doppia 1023.</p>
<p>Lo standard riserva due dei possibili valori per l’esponente per
codificare due situazioni speciali:</p>
<ul>
<li><span class="math inline">\(e^* = 0\)</span>, viene riservato per la
codifica dello zero ed eventuali numeri denormalizzati.<br />
</li>
<li><span class="math inline">\(e^* = 255\)</span> (singola precisione)
o <span class="math inline">\(e^* = 2047\)</span> (doppia precisione),
che corrisponde a un esponente vero pari a 128 (singola) o 1024
(doppia), viene riservato per la codifica di:
<ul>
<li><strong>Inf (Overflow)</strong><br />
</li>
<li><strong>NaN (Not a Number)</strong>, ovvero operazioni del
tipo<br />
<span class="math inline">\(\dfrac{0}{0}, \quad \infty - \infty, \quad
\dfrac{\infty}{\infty}\)</span></li>
</ul></li>
</ul>
<p>Inf viene codificato con mantissa nulla, mentre NaN con mantissa
<span class="math inline">\(\neq 0\)</span>.</p>
<h2 id="massimo-e-minimo-numero-rappresentabile">Massimo e minimo numero
rappresentabile</h2>
<p><span class="math inline">\(F\)</span> è limitato inferiormente e
superiormente.</p>
<h3 id="massimo-numero-rappresentabile">Massimo numero
rappresentabile</h3>
<p>Il massimo numero rappresentabile è il numero più grande che si può
rappresentare in <span class="math inline">\(\mathbb{F}(B, t, L,
U)\)</span>, ovvero con tutte le cifre della mantissa uguali a <span
class="math inline">\(B - 1\)</span> e l’esponente massimo:</p>
<p><span class="math inline">\(M = (1 - B^{-t}) \cdot B^{U}\)</span></p>
<h3 id="minimo-numero-rappresentabile">Minimo numero
rappresentabile</h3>
<p>Il minimo numero rappresentabile è il numero più piccolo che si può
rappresentare in <span class="math inline">\(\mathbb{F}(B, t, L,
U)\)</span>, ovvero con le cifre della mantissa tutte uguali a <span
class="math inline">\(0\)</span> tranne la prima, e l’esponente
minimo:</p>
<p><span class="math inline">\(m = B^{L}\)</span> ???</p>
<h2 id="distanza-assoluta-tra-due-numeri-macchina-consecutivi">Distanza
assoluta tra due numeri macchina consecutivi</h2>
<p><span class="math inline">\(x = (-1)^s \cdot (1 + 0.d_1d_2\ldots
d_{\tau}) \cdot B^{e}\)</span><br />
<span class="math inline">\(x_+ = (-1)^s \cdot (1 + 0.d_1d_2\ldots
d_{\tau} + 1) \cdot B^{e}\)</span></p>
<p><span class="math inline">\(\Delta x = \vert x - x_+ \vert =
B^{-\tau} \cdot B^{e} = B^{e - \tau}\)</span></p>
<p>Questa distanza è uguale per tutti i numeri macchina aventi lo stesso
esponente.<br />
L’incremento/decremento di una unità dell’esponente comporta un
incremento/decremento di in fattore pari alla base della distanza
assoluta tra due numeri macchina consecutivi.</p>
<h2 id="distanza-relativa-tra-due-numeri-macchina-consecutivi">Distanza
relativa tra due numeri macchina consecutivi</h2>
<p><span class="math inline">\(\dfrac{\vert x - x_+ \vert}{\vert x
\vert} = \dfrac{B^{e - \tau}}{p \cdot B^{e}} =
\dfrac{B^{-\tau}}{p}\)</span></p>
<p>Si può vedere che la distanza relativa tra due numeri macchina
consecutivi ha un andamento periodico.</p>
<p>La massima distanza relativa tra due numeri macchina consecutivi
è:</p>
<p><span class="math display">\[\varepsilon_{M} = B^{-
\tau}\]</span></p>
<p>che si ha quando <span class="math inline">\(p = 1\)</span>.</p>
<p>Nello standard IEEE-754r in doppia precisione <span
class="math inline">\(\varepsilon_M = 2^{-52}\)</span>.</p>
<h2 id="precisione-di-macchina-2">Precisione di macchina (2)</h2>
<p>La precisione di macchina definita precedentemente come il massimo
errore relativo di arrotondamento, coincide anche con <span
class="math inline">\(\boldsymbol{u} = \dfrac{1}{2}
\varepsilon_M\)</span>.</p>
<p>In un computer che usa doppia precisione secondo lo standard
IEEE-754r il valore della precisione di macchina è pari a <span
class="math inline">\(u = 2^{-53}\)</span>.</p>
<h2 id="errori-nelle-operazioni-macchina">Errori nelle Operazioni
Macchina</h2>
<p>Ricordiamo che <span class="math inline">\(\varepsilon_x =
\dfrac{\vert x - fl(x) \vert}{\vert x \vert} \leq u\)</span>.</p>
<h3 id="errore-relativo-risultante-da-operazioni-macchina">Errore
relativo risultante da operazioni macchina</h3>
<p><span class="math inline">\(x \oplus y\)</span> per definizione è
l’approssimazione di <span class="math inline">\(x + y\)</span> in <span
class="math inline">\(\mathbb{F}(B, t, L, U)\)</span>.</p>
<p><span class="math inline">\(\varepsilon_{x,y}^{\oplus} = \dfrac{\vert
(x + y) - (x \oplus y) \vert}{\vert x + y \vert}\)</span></p>
<h3 id="propagazione-degli-errori">Propagazione degli Errori</h3>
<ul>
<li><p><strong>Somma:</strong><br />
<span class="math display">\[\epsilon_{\oplus}(x,y) \leq
\left|\dfrac{x}{x+y}\right|\epsilon_x +
\left|\dfrac{y}{x+y}\right|\epsilon_y\]</span></p></li>
<li><p><strong>Prodotto:</strong><br />
<span class="math display">\[\epsilon_{\otimes}(x,y) \leq \approx
\epsilon_x + \epsilon_y\]</span></p></li>
<li><p><strong>Divisione (o altre operazioni):</strong><br />
<span class="math display">\[\epsilon_{\oslash}{x,y} \leq |\epsilon_x -
\epsilon_y|\]</span></p></li>
</ul>
<p>con <span class="math inline">\(\epsilon_x\)</span> e <span
class="math inline">\(\epsilon_y\)</span> tali che <span
class="math inline">\(\epsilon_x, \epsilon_y \leq u\)</span>.</p>
<h3 id="errori-nelladdizione-dimostrazione">Errori nell’addizione
dimostrazione</h3>
<p><span class="math inline">\(x \neq 0, y \neq 0, x + y \neq 0,
fl(fl(x) + fl(y)) = fl(x) + fl(y)\)</span>, cioè la somma di due numeri
macchina è un numero macchina.</p>
<p><span class="math display">\[
\begin{aligned}
\epsilon_{\oplus}(x,y) &amp; = \dfrac{|(x + y) - (x \oplus y)|}{|x + y|}
= \dfrac{|(x + y) - (fl(x) + fl(y))|}{|x + y|} \\
&amp; \leq \dfrac{|(x - fl(x))|}{|x + y|} + \dfrac{|(y - fl(y))|}{|x +
y|} \\
&amp; = \dfrac{|(x - fl(x))| \cdot |x|}{|x + y| \cdot |x|} + \dfrac{|(y
- fl(y))| \cdot |y|}{|x + y| \cdot |y|} \\
&amp; = \dfrac{|(x - fl(x))|}{|x|} \cdot \dfrac{|x|}{|x + y|} +
\dfrac{|(y - fl(y))|}{|y|} \cdot \dfrac{|y|}{|x + y|} \\
&amp; \leq \epsilon_x \cdot \dfrac{|x|}{|x + y|} + \epsilon_y \cdot
\dfrac{|y|}{|x + y|} \\
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
<h3 id="errori-nel-prodotto-dimostrazione">Errori nel prodotto
dimostrazione</h3>
<p><span class="math inline">\(x \neq 0, y \neq 0, x \cdot y \neq 0,
fl(fl(x) \cdot fl(y)) = fl(x) \cdot fl(y)\)</span>, cioè il prodotto di
due numeri macchina è un numero macchina.</p>
<p><span class="math display">\[
\begin{aligned}
\epsilon_{\otimes}(x,y) &amp; = \dfrac{|(x \cdot y) - (x \otimes y)|}{|x
\cdot y|} = \dfrac{|(x \cdot y) - (fl(x) \cdot fl(y))|}{|x \cdot y|} \\
&amp; = \dfrac{|x \cdot y - x \cdot fl(y) + x \cdot fl(y) - fl(x) \cdot
fl(y)|}{|x \cdot y|} \\
&amp; = \dfrac{|x \cdot (y - fl(y)) + fl(y) \cdot (x - fl(x))|}{|x \cdot
y|} \\
&amp; \leq \dfrac{|x \cdot (y - fl(y))|}{|x \cdot y|} + \dfrac{|fl(y)
\cdot (x - fl(x))|}{|x \cdot y|} \\
&amp; = \dfrac{|y - fl(y)|}{|y|} + \dfrac{|x - fl(x)|}{|x|} \\
&amp; = \epsilon_y + \epsilon_x \\
\end{aligned}
\]</span></p>
<h3 id="osservazioni">Osservazioni</h3>
<ol type="1">
<li>Le operazioni macchina <strong>prodotto</strong> e
<strong>divisione</strong> introducono un errore dell’ordine della
precisione di macchina.</li>
<li>Con la <strong>somma</strong> (e la sottrazione) non si può
garantire che il risultato dell’operazione sia affetto da un errore
relativo piccolo. In particolare, l’errore per la somma diventa grande
quando <span class="math inline">\(x \approx -y\)</span>. Questo
fenomeno è noto come <strong>cancellazione numerica</strong>.</li>
</ol>
<h2 id="cancellazione-numerica">Cancellazione Numerica</h2>
<p>La cancellazione numerica rappresenta la perdita di cifre
significative nel risultato, dovuta alla sottrazione di due numeri quasi
uguali.</p>
<h3 id="esempio-1">Esempio 1</h3>
<ul>
<li><p>Consideriamo l’aritmetica di macchina <span
class="math inline">\(F(10,5,*,*)\)</span> con:</p>
<ul>
<li><span class="math inline">\(a = 0.73415507\)</span><br />
<span class="math inline">\(fl(a) = 0.73416\)</span></li>
<li><span class="math inline">\(b = 0.73415448\)</span><br />
<span class="math inline">\(fl(b) = 0.73415\)</span></li>
</ul></li>
<li><p>Valore esatto:<br />
<span class="math inline">\(a - b = 0.59\cdot 10^{-6}\)</span></p></li>
<li><p>In aritmetica macchina:<br />
<span class="math inline">\(fl(a - b) = 10^{-5}\)</span></p></li>
<li><p>L’errore relativo diventa: <span
class="math inline">\(\dfrac{|(a-b) - fl(a-b)|}{|a-b|} \approx
1595\%\)</span></p></li>
<li><p>La precisione di macchina in questo caso è: <span
class="math inline">\(u = \dfrac{1}{2} B^{1-t} = \dfrac{1}{2} \cdot
10^{-4} = 5\cdot 10^{-5}\)</span></p></li>
</ul>
<p>L’errore relativo è molto maggiore della precisione di macchina. Noi
vogliamo che l’errore relativo sia dell’ordine della precisione di
macchina!</p>
<h3 id="esempio-2">Esempio 2</h3>
<ul>
<li><p>Consideriamo ora l’aritmetica di macchina <span
class="math inline">\(F(10,6,*,*)\)</span> con:</p>
<ul>
<li><span class="math inline">\(a = 0.147554326\)</span><br />
<span class="math inline">\(fl(a) = 0.147554\)</span></li>
<li><span class="math inline">\(b = 0.147251742\)</span><br />
<span class="math inline">\(fl(b) = 0.147252\)</span></li>
</ul></li>
<li><p>Valore esatto: <span class="math inline">\(a - b =
0.000302584\)</span></p></li>
<li><p>In aritmetica macchina: <span class="math inline">\(fl(a - b) =
0.000302\)</span></p></li>
<li><p>L’errore relativo risulta: <span
class="math inline">\(\dfrac{|0.000302584 - 0.000302|}{0.000302584}
\approx 1.9\cdot 10^{-3} \quad (0.19\%)\)</span></p></li>
<li><p>Precisione di macchina: <span class="math inline">\(u =
\dfrac{1}{2} \cdot 10^{-5} = 5\cdot 10^{-6}\)</span></p></li>
</ul>
<p>Anche in questo caso l’errore relativo è molto maggiore della
precisione di macchina.</p>
<h2 id="proprietà-delle-operazioni-non-associatività">Proprietà delle
Operazioni: Non Associatività</h2>
<p>In aritmetica macchina alcune proprietà delle operazioni sui numeri
reali non sono valide. In particolare, la <strong>proprietà
associativa</strong> non vale:</p>
<p><span class="math display">\[(a \oplus b) \oplus c \neq a \oplus (b
\oplus c)\]</span></p>
<h3 id="esempio-di-non-associatività">Esempio di Non Associatività</h3>
<ul>
<li>Consideriamo:
<ul>
<li><span class="math inline">\((1 \oplus 10^{-15}) \oplus 1 = 1.11\cdot
10^{-15}\)</span></li>
<li><span class="math inline">\((1 \oplus 1) \oplus 10^{-15} =
10^{-15}\)</span></li>
</ul></li>
</ul>
<p>L’ordine in cui vengono effettuate le operazioni influisce sul
risultato.<br />
Il problema dell’esempio è dovuto alla differenza di ordine di grandezza
tra i numeri sommati, che non permette di rappresentare correttamente il
risultato: si ha una perdita di cifre significative.</p>
<h3 id="esempio-con-overﬂowunderﬂow">Esempio con Overﬂow/Underﬂow</h3>
<ul>
<li>Siano:
<ul>
<li><span class="math inline">\(a = 1.0\cdot 10^{308}\)</span></li>
<li><span class="math inline">\(b = 1.1\cdot 10^{308}\)</span></li>
<li><span class="math inline">\(c = -1.001\cdot 10^{308}\)</span></li>
</ul></li>
<li>Calcolando:
<ul>
<li><span class="math inline">\(a \oplus (b \oplus c) = 1.0\cdot
10^{308} \oplus (0.99\cdot 10^{307}) = 1.099\cdot 10^{308}\)</span></li>
<li><span class="math inline">\((a \oplus b) \oplus c = \text{Inf}
\oplus c = \text{Inf}\)</span></li>
</ul></li>
</ul>
<p>Questo esempio evidenzia come la violazione dell’associatività possa
portare a problemi di overflow o underflow.</p>
<h2
id="cancellazione-numerica-e-stabilità-di-un-algoritmo">Cancellazione
numerica e stabilità di un algoritmo</h2>
<p>Un metodo numerico (formula, algoritmo) si dice
<strong>stabile</strong> se non propaga gli errori (inevitabili) dovuti
alla rappresentazione dei numeri nel calcolatore. Altrimenti si dice
<strong>instabile</strong>.</p>
<ul>
<li>La cancellazione numerica genera delle formule instabili.</li>
<li>Per evitare i problemi legati alla cancellazione numerica occorre
trasformare le formule in altre numericamente più stabili.</li>
<li>La stabilità è un concetto legato all’algoritmo usato per risolvere
un determinato problema.</li>
</ul>
<h2 id="esempii-di-algoritmo-instabile">Esempii di Algoritmo
Instabile</h2>
<h3 id="esempio-1-1">Esempio 1</h3>
<p><span class="math inline">\(\sqrt{x + \delta} - \sqrt{x}\)</span> per
<span class="math inline">\(\delta \to 0\)</span></p>
<p>Razionalizzando:</p>
<p><span class="math display">\[
\sqrt{x + \delta} - \sqrt{x} = \dfrac{(\sqrt{x + \delta} -
\sqrt{x})(\sqrt{x + \delta} + \sqrt{x})}{\sqrt{x + \delta} + \sqrt{x}} =
\dfrac{\delta}{\sqrt{x + \delta} + \sqrt{x}}
\]</span></p>
<h3 id="formula-risolutiva-delle-equazioni-di-secondo-grado">Formula
risolutiva delle equazioni di secondo grado</h3>
<p><span class="math inline">\(a x^2 + b x + c = 0, \quad a \neq
0\)</span><br />
<span class="math inline">\(x^2 + \dfrac{b}{a} x + \dfrac{c}{a} =
0\)</span><br />
con <span class="math inline">\(\dfrac{b}{a} = 2p, \quad \dfrac{c}{a} =
-q\)</span><br />
<span class="math inline">\(x^2 + 2px - q = 0\)</span></p>
<p><span class="math inline">\(x_{1,2} = -p \pm \sqrt{p^2 +
q}\)</span></p>
<p>Potenzialmente instabile per <span class="math inline">\(p \gg q &gt;
0\)</span>, a causa della sottrazione di due numeri quasi uguali
(cancellazione numerica).</p>
<p>Soluzione stabile: razionalizzare la formula.</p>
<p><span class="math inline">\(x_1 = -p + \sqrt{p^2 + q} \cdot \dfrac{(p
+ \sqrt{p^2 + q})}{(p + \sqrt{p^2 + q})} = -p + \dfrac{q}{p + \sqrt{p^2
+ q}}\)</span></p>
<h3 id="successione-ricorrente">Successione Ricorrente</h3>
<p>Si vuole calcolare la seguente successione di integrali definiti:</p>
<p><span class="math display">\[
I_n = \dfrac{1}{e} \int_0^1 x^n e^x dx, \quad n \geq 0
\]</span></p>
<p>Dove:</p>
<p><span class="math display">\[
I_0 = \dfrac{1}{e} \int_0^1 e^x dx = 1 - \dfrac{1}{e} \approx 0.632121
\]</span></p>
<p>Per <span class="math inline">\(n \geq 1\)</span>, integrando per
parti:</p>
<p><span class="math display">\[
I_n = \dfrac{1}{e} \left[ x^n e^x \Big|_0^1 - n \int_0^1 x^{n-1} e^x dx
\right]
\]</span></p>
<p>Si ottiene la formula ricorsiva:</p>
<p><span class="math display">\[
I_n = 1 - n I_{n-1}, \quad n \geq 1
\]</span></p>
<p>Si noti che <span class="math inline">\(0 &lt; I_n &lt;
1\)</span>.</p>
<h2 id="instabilità-della-formula-ricorsiva">Instabilità della Formula
Ricorsiva</h2>
<p>Implementando la formula <span class="math inline">\(I_n = 1 - n
I_{n-1}\)</span> per <span class="math inline">\(n = 2, \dots,
25\)</span>, i valori calcolati mostrano un comportamento instabile.</p>
<h3 id="analisi-dellerrore">Analisi dell’Errore</h3>
<p>Nel calcolatore:</p>
<p><span class="math display">\[
(I_n + \epsilon_n) = 1 - n (I_{n-1} + \epsilon_{n-1})
\]</span></p>
<p>Sottraendo la relazione teorica <span class="math inline">\(I_n = 1 -
n I_{n-1}\)</span>:</p>
<p><span class="math display">\[
\epsilon_n = -n \epsilon_{n-1}
\]</span></p>
<p>Per induzione:</p>
<p><span class="math display">\[
|\epsilon_n| = n! |\epsilon_0|
\]</span></p>
<p>Il fattore <span class="math inline">\(n!\)</span> amplifica l’errore
iniziale su <span class="math inline">\(I_0\)</span>. Per esempio, nel
calcolo di <span class="math inline">\(I_{20}\)</span>:</p>
<p><span class="math display">\[
\epsilon_{20} = 20! \epsilon_0 \approx 2.7 \times 10^2 \epsilon_0
\]</span></p>
<h2 id="formula-alternativa-stabile">Formula Alternativa Stabile</h2>
<p>Una successione ricorrente alternativa può essere calcolata
all’indietro, partendo da un’approssimazione di <span
class="math inline">\(I_m\)</span>:</p>
<p><span class="math display">\[
I_{n-1} = \dfrac{1}{n} (1 - I_n), \quad n = m, m-1, \dots, m-k+1
\]</span></p>
<p>Questa formula è <strong>stabile</strong>, poiché l’errore diminuisce
a ogni passo.</p>
<h3 id="smorzamento-dellerrore">Smorzamento dell’Errore</h3>
<p>L’errore al passo <span class="math inline">\(n-1\)</span> è:</p>
<p><span class="math display">\[
\epsilon_{n-1} = \dfrac{-1}{n} \epsilon_n
\]</span></p>
<p>Iterando:</p>
<p><span class="math display">\[
|\epsilon_{m-1}| = \dfrac{|\epsilon_m|}{m}, \quad |\epsilon_{m-2}| =
\dfrac{|\epsilon_m|}{m(m-1)}, \quad \dots, \quad |\epsilon_{m-k}| =
\dfrac{|\epsilon_m|}{m(m-1) \dots (m-k+1)}
\]</span></p>
<p>La produttoria al denominatore <strong>riduce rapidamente</strong>
l’errore iniziale!</p>
<p>Per esempio, calcolando <span class="math inline">\(I_{25}\)</span>
partendo da <span class="math inline">\(I_{40} = 0.5\)</span>, l’errore
iniziale <span class="math inline">\(|\epsilon_{40}| &lt; 0.5\)</span>
viene abbattuto di un fattore:</p>
<p><span class="math display">\[
40 \cdot 39 \cdots 27 \cdot 26 \approx 5.26 \times 10^{22}
\]</span></p>
<h2 id="condizionamento-di-un-problema">Condizionamento di un
Problema</h2>
<h3 id="definizione">Definizione</h3>
<p>Un problema è <strong>mal condizionato</strong> se piccole variazioni
nei dati producono grandi variazioni nei risultati. Se invece il
problema è <strong>ben condizionato</strong>, gli errori nei dati
iniziali restano contenuti nei risultati.</p>
<p>Il malcondizionamento è <strong>indipendente</strong> dall’algoritmo
scelto: se un problema è mal condizionato, nessun algoritmo potrà
fornire una soluzione accurata.</p>
<h3 id="esempio">Esempio</h3>
<p>Consideriamo il sistema lineare:</p>
<p><span class="math display">\[
\begin{cases}
x + y = 2 \\
1001x + 1000y = 2001
\end{cases}
\]</span></p>
<p>La soluzione esatta è <span class="math inline">\(x = 1, y =
1\)</span>.</p>
<p>Se perturbiamo il coefficiente della <span
class="math inline">\(x\)</span> nella prima equazione di <span
class="math inline">\(0.01\)</span>:</p>
<p><span class="math display">\[
\begin{cases}
1.01x + y = 2 \\
1001x + 1000y = 2001
\end{cases}
\]</span></p>
<p>La nuova soluzione diventa <span class="math inline">\(x \approx
-0.1111\)</span>, <span class="math inline">\(y \approx
2.1122\)</span>.</p>
<p>L’errore relativo:</p>
<p><span class="math display">\[
err_x = \dfrac{|1 + 0.1111|}{1} \approx 1.1111, \quad err_y = \dfrac{|1
- 2.1122|}{1} \approx 1.1122
\]</span></p>
<p>Entrambi superiori al <strong>100%</strong>, dimostrando il
malcondizionamento del problema.</p>
<h2 id="numero-di-condizionamento">Numero di Condizionamento</h2>
<p>Il <strong>numero di condizionamento</strong> misura la sensibilità
di un problema rispetto a variazioni nei dati iniziali.</p>
<p>Per la valutazione di una funzione <span
class="math inline">\(f(x)\)</span> in un punto:</p>
<p><span class="math display">\[
y = f(x)
\]</span></p>
<p>Se <span class="math inline">\(x\)</span> è perturbato di <span
class="math inline">\(\Delta x\)</span>, per il teorema di Lagrange:</p>
<p><span class="math display">\[
f(x + \Delta x) - f(x) = \Delta x f&#39;(\xi)
\]</span></p>
<p>L’errore relativo:</p>
<p><span class="math display">\[
\left| \dfrac{\Delta y}{y} \right| = \left| \dfrac{\Delta x
f&#39;(\xi)}{y} \right| = \left| \dfrac{\Delta x}{x} \right| \cdot
\left| \dfrac{x f&#39;(x)}{y} \right|
\]</span></p>
<p>Definiamo quindi il <strong>numero di condizionamento</strong>:</p>
<p><span class="math display">\[
K(f, x) = \left| \dfrac{x f&#39;(x)}{y} \right|
\]</span></p>
<h3 id="esempio-3">Esempio</h3>
<p>Per la funzione:</p>
<p><span class="math display">\[
f(x) = \sqrt{1 - x^2}
\]</span></p>
<p>Il numero di condizionamento è:</p>
<p><span class="math display">\[
K(f, x) = \dfrac{x^2}{1 - x^2}
\]</span></p>
<p>Il problema peggiora quanto più <span
class="math inline">\(x\)</span> si avvicina a <span
class="math inline">\(1\)</span>:</p>
<p><span class="math display">\[
\begin{array}{c|c}
x &amp; K(f, x) \\
\hline
1 - 10^{-6} &amp; 4.99999 \times 10^5 \\
1 - 10^{-12} &amp; 5.00011 \times 10^{11} \\
1 - 10^{-15} &amp; 5.00399 \times 10^{14}
\end{array}
\]</span></p>
<p>Più <span class="math inline">\(x\)</span> è vicino a <span
class="math inline">\(1\)</span>, maggiore è il numero di
condizionamento, indicando un <strong>problema mal
condizionato</strong>.</p>
<h1 id="calcolo-degli-zeri-di-una-funzione">Calcolo degli Zeri di una
Funzione</h1>
<h2 id="introduzione-1">Introduzione</h2>
<p><strong>Problema</strong>: trovare gli zeri di una funzione <span
class="math inline">\(f(x)\)</span>, ovvero i punti in cui <span
class="math inline">\(f(x) = 0\)</span>, con <span
class="math inline">\(f: [a, b] \to \mathbb{R}\)</span> continua.</p>
<p><strong>Zeri</strong>: <span class="math inline">\(\alpha \in [a,
b]\)</span> tale che <span class="math inline">\(f(\alpha) =
0\)</span>.</p>
<p>Esempi:<br />
<span class="math inline">\(f(x) = (x - 1)^2 = 0\)</span><br />
<span class="math inline">\(f(x) = \cos(\log(\dfrac{1}{x})) =
0\)</span></p>
<p><strong>Molteplicità di uno zero</strong>: <span
class="math inline">\(\alpha\)</span> è <em>zero semplice</em> se <span
class="math inline">\(f(\alpha) = 0\)</span> e <span
class="math inline">\(f&#39;(\alpha) \neq 0\)</span>. La molteplicità di
uno zero è l’indice della prima derivata non nulla in <span
class="math inline">\(\alpha\)</span>.</p>
<p>Esempi:<br />
<span class="math inline">\(f(x) = \cos(x) - 1 + \dfrac{x^2}{2} +
\dfrac{x^5}{5} = 0\)</span><br />
<span class="math inline">\(f&#39;(x) = -\sin(x) + x +
x^4\)</span><br />
<span class="math inline">\(f&#39;(0) = 0\)</span> <span
class="math inline">\(f&#39;&#39;(x) = -\cos(x) + 1 +
4x^3\)</span><br />
<span class="math inline">\(f&#39;&#39;(0) = 0\)</span><br />
<span class="math inline">\(f&#39;&#39;&#39;(x) = \sin(x) +
12x^2\)</span><br />
<span class="math inline">\(f&#39;&#39;&#39;(0) = 0\)</span><br />
<span class="math inline">\(f^{(4)}(x) = \cos(x) + 24x\)</span><br />
<span class="math inline">\(f^{(4)}(0) = 1 \neq 0 \Rightarrow\)</span>
zero di molteplicità 4</p>
<h2 id="esistenza-dello-zero---teorema-di-bolzano">Esistenza dello zero
- Teorema di Bolzano</h2>
<p><strong>Teorema di Bolzano</strong>:</p>
<p><span class="math inline">\(f: [a, b] \to \mathbb{R}\)</span>
continua. Se <span class="math inline">\(f(a) \cdot f(b) &lt;
0\)</span>, allora <span class="math inline">\(\exists \alpha \in [a,
b]\)</span> tale che <span class="math inline">\(f(\alpha) =
0\)</span>.</p>
<h2 id="unicità-dello-zero">Unicità dello zero</h2>
<p>Se <span class="math inline">\(f(x)\)</span> è <strong>monotona in
senso stretto</strong> su <span class="math inline">\([a, b]\)</span> e
<span class="math inline">\(\exists \alpha \in [a, b]\)</span> tale che
<span class="math inline">\(f(\alpha) = 0\)</span>, allora <span
class="math inline">\(\alpha\)</span> è unico (condizione
sufficiente).</p>
<p><strong>Nota</strong>: la monotonia in senso stretto è definita come
<span class="math inline">\(f&#39;(x) &gt; 0\)</span> o <span
class="math inline">\(f&#39;(x) &lt; 0\)</span> per ogni <span
class="math inline">\(x \in [a, b]\)</span>.</p>
<p><strong>Esempio</strong>:</p>
<p><span class="math inline">\(f(x) = 2x^2 + \log(x) -
\dfrac{1}{x}\)</span></p>
<p>Esistenza:</p>
<p>So che fra <span class="math inline">\(0.5\)</span> e <span
class="math inline">\(1\)</span> c’è uno zero. Per il teorema degli
zeri, se <span class="math inline">\(f(0.5) \cdot f(1) &lt; 0\)</span>
allora c’è uno zero.</p>
<p><span class="math inline">\(f(0.5) = 2 \cdot 0.5^2 + \log(0.5) - 2 =
-2.1931\)</span><br />
<span class="math inline">\(f(1) = 2 \cdot 1^2 + \log(1) - 1 =
1\)</span></p>
<p>Quindi <span class="math inline">\(\exists \alpha \in [0.5,
1]\)</span> tale che <span class="math inline">\(f(\alpha) =
0\)</span>.</p>
<p>Unicità:</p>
<p><span class="math inline">\(f&#39;(x) = 4x + \dfrac{1}{x} -
\dfrac{1}{x^2} &gt; 0\)</span> per <span class="math inline">\(x \in
[0.5, 1]\)</span>. Quindi lo zero è unico, poiché la funzione è monotona
in senso stretto.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;</span> <span class="va">f</span> <span class="op">=</span> <span class="op">@</span>(<span class="va">x</span>) <span class="fl">2</span><span class="op">*</span><span class="va">x</span><span class="op">.^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">log</span>(<span class="va">x</span>) <span class="op">-</span> <span class="fl">1</span><span class="op">./</span><span class="va">x</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="va">f</span> <span class="op">=</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">@</span>(<span class="va">x</span>) <span class="fl">2</span> <span class="op">*</span> <span class="va">x</span> <span class="op">.^</span> <span class="fl">2</span> <span class="op">+</span> <span class="va">log</span> (<span class="va">x</span>) <span class="op">-</span> <span class="fl">1</span> <span class="op">./</span> <span class="va">x</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;</span> <span class="va">f</span>(<span class="fl">0.5</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="va">ans</span> <span class="op">=</span> <span class="op">-</span><span class="fl">2.1931</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;</span> <span class="va">f</span>(<span class="fl">1</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="va">ans</span> <span class="op">=</span> <span class="fl">1</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;</span> <span class="va">fplot</span>(<span class="va">f</span><span class="op">,</span> [<span class="fl">0.5</span><span class="op">,</span> <span class="fl">1</span>])<span class="op">;</span> <span class="va">hold</span> <span class="va">on</span><span class="op">;</span> <span class="va">plot</span>([<span class="fl">0.1</span> <span class="fl">1</span>]<span class="op">,</span> [<span class="fl">0</span> <span class="fl">0</span>]<span class="op">,</span> <span class="ss">&#39;k-&#39;</span>)<span class="op">;</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;</span> <span class="va">format</span> <span class="va">long</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;</span> <span class="va">fzero</span>(<span class="va">f</span><span class="op">,</span> <span class="fl">0.8</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="va">ans</span> <span class="op">=</span> <span class="fl">0.832233982809322</span></span></code></pre></div>
<h2 id="metodi-per-il-calcolo-degli-zeri">Metodi per il Calcolo degli
Zeri</h2>
<p>I metodi numerici per il calcolo degli zeri di una funzione sono
<strong>iterativi</strong>.</p>
<p><strong>Definizione</strong>: Un <strong>metodo iterativo</strong> è
una procedura che genera una successione <span
class="math inline">\(\{x_k\}_{k \geq 0}\)</span> a partire da uno o più
valori iniziali e quindi a partire da uno o più termini precedenti.</p>
<p><strong>Definizione</strong>: Un metodo iterativo è
<strong>convergente</strong> ad <span
class="math inline">\(\alpha\)</span> se <span
class="math inline">\(\displaystyle \lim_{k \to \infty} x_k =
\alpha\)</span> o equivalentemente <span
class="math inline">\(\displaystyle \lim_{k \to \infty} |x_k - \alpha| =
0\)</span>.<br />
<span class="math inline">\(x_k - \alpha = \varepsilon_k\)</span> è
l’errore al passo <span class="math inline">\(k\)</span>.</p>
<p><strong>Definizione convergenza locale</strong>: Un metodo iterativo
è <strong>localmente convergente</strong> ad <span
class="math inline">\(\alpha\)</span> se <span
class="math inline">\(\exists \delta &gt; 0\)</span> raggio di
convergenza tale che <span class="math inline">\(\forall x_0 \in
B(\alpha, \delta) = [\alpha - \delta, \alpha + \delta]\)</span> si ha
che <span class="math inline">\(\displaystyle \lim_{k \to \infty} x_k =
\alpha\)</span>.</p>
<p><strong>Definizione</strong>: si dice <strong>ordine di
convergenza</strong> <span class="math inline">\(p\)</span> di un metodo
iterativo convergente (tale che <span class="math inline">\(x_k \to
\alpha\)</span>) se esiste una costante <span class="math inline">\(C
&gt; 0\)</span> tale che <span class="math inline">\(\displaystyle
\lim_{k \to \infty} \dfrac{|\varepsilon_{k+1}|}{|\varepsilon_k|^p} =
C\)</span> (con <span class="math inline">\(p \geq 1, p \in
\mathbb{R}\)</span>).</p>
<p>Se <span class="math inline">\(p = 1\)</span> il metodo è detto
<strong>lineare</strong>, se <span class="math inline">\(p = 2\)</span>
è detto <strong>quadratico</strong>. Per <span class="math inline">\(p
&gt; 1\)</span> il metodo è detto <strong>superlineare</strong>.</p>
<p>Se <span class="math inline">\(\displaystyle \lim_{k \to \infty}
\dfrac{|\varepsilon_{k+1}|}{|\varepsilon_k|} = 0\)</span> avremo
convergenza <strong>superlineare</strong>.</p>
<p>Per <span class="math inline">\(p = 1\)</span>, togliendo il limite
ho che:</p>
<p><span
class="math inline">\(\dfrac{|\varepsilon_{k+1}|}{|\varepsilon_k|} \sim
C \Rightarrow |\varepsilon_{k+1}| \sim C \cdot
|\varepsilon_k|\)</span></p>
<p><span class="math inline">\(C\)</span> è la <strong>costante
asintotica di riduzione dell’errore</strong>. Per <span
class="math inline">\(p = 1\)</span>, <span
class="math inline">\(C\)</span> deve essere minore di <span
class="math inline">\(1\)</span> affinché il metodo sia convergente.</p>
<p>Per <span class="math inline">\(p = 2\)</span>, togliendo il limite
ho che:</p>
<p><span
class="math inline">\(\dfrac{|\varepsilon_{k+1}|}{|\varepsilon_k|^2}
\sim C \Rightarrow |\varepsilon_{k+1}| \sim C \cdot
|\varepsilon_k|^2\)</span></p>
<p>In questo caso, <span class="math inline">\(C\)</span> non determina
la convergenza del metodo, poiché <span
class="math inline">\(\varepsilon_k\)</span> tende a <span
class="math inline">\(0\)</span> più velocemente. ???</p>
<h2 id="metodo-di-bisezione">Metodo di Bisezione</h2>
<p>Il <strong>metodo di bisezione</strong> è un metodo iterativo per il
calcolo degli zeri di una funzione <span
class="math inline">\(f(x)\)</span>.</p>
<p><span class="math inline">\(f: [a, b] \to \mathbb{R}\)</span>
continua, <span class="math inline">\(\exists \alpha \in [a, b]\)</span>
tale che <span class="math inline">\(f(\alpha) = 0\)</span>. Lo zero è
unico in <span class="math inline">\([a, b]\)</span>.</p>
<p><em>Idea del metodo</em>: Genera a partire da un intervallo iniziale
<span class="math inline">\(I_0 = [a, b]\)</span> che contiene lo zero,
una successione di intervalli <span class="math inline">\(I_k = [a_k,
b_k]\)</span>, ciascuno contenente lo zero (<span
class="math inline">\(f(a_k) \cdot f(b_k) &lt; 0\)</span>) e di
dimensione dimezzata rispetto a <span
class="math inline">\(I_{k-1}\)</span>.</p>
<h3 id="convergenza-globale-del-metodo-di-bisezione">Convergenza globale
del metodo di bisezione</h3>
<p><strong>Teorema</strong>: Il metodo di bisezione converge globalmente
ad uno zero di <span class="math inline">\(f(x)\)</span>
(indipendentemente da <span class="math inline">\(x_0\)</span>), cioè
<span class="math inline">\(\forall x_0 \in [a, b]\)</span>.</p>
<p><span class="math inline">\(|x_k - \alpha| = | \varepsilon_k | \leq
\dfrac{b_k - a_k}{2} \leq \dfrac{b - a}{2^{k+1}}\)</span></p>
<p><span class="math inline">\(\displaystyle \lim_{k \to \infty}
\dfrac{b - a}{2^{k+1}} = 0\)</span></p>
<p>Per il teorema dei due carabinieri:</p>
<p><span class="math inline">\(\displaystyle \lim_{k \to \infty} 0 =
0\)</span><br />
<span class="math inline">\(\displaystyle \lim_{k \to \infty} \dfrac{b -
a}{2^{k+1}} = 0\)</span></p>
<p><span class="math inline">\(0 \leq |x_k - \alpha| \leq \dfrac{b -
a}{2^{k+1}}\)</span></p>
<p>Quindi <span class="math inline">\(\displaystyle \lim_{k \to \infty}
|x_k - \alpha| = \lim_{k \to \infty} | \varepsilon_k | = 0\)</span>.</p>
<p><strong>Nota</strong>: con la minorazione <span
class="math inline">\(| \varepsilon_k | \leq \dfrac{b - a}{2^{k+1}} &lt;
\text{toll}\)</span>, possiamo determinare il numero di iterazioni
necessarie per raggiungere la tolleranza desiderata, cioè ridurre
l’errore al di sotto della tolleranza. <span class="math display">\[
\log (b - a) - (k + 1) \log 2 &lt; \log \text{toll} \Rightarrow
\]</span></p>
<p><span class="math display">\[
\log (b - a) - \log \text{toll} &lt; (k + 1) \log 2 \Rightarrow k &gt;
\Big\lceil \dfrac{\log (b - a) - \log \text{toll}}{\log 2} \Big \rceil \
- 1
\]</span></p>
<h3
id="ordine-di-convergenza-del-metodo-di-bisezione-e-costante-asintotica">Ordine
di convergenza del metodo di bisezione e costante asintotica</h3>
<p><strong>Teorema</strong>: Il metodo di bisezione è
<strong>lineare</strong> (<span class="math inline">\(p = 1\)</span>)
con costante asintotica di riduzione dell’errore <span
class="math inline">\(C = \dfrac{1}{2}\)</span> (il limite non
esiste).</p>
<h2 id="criterio-di-arresto">Criterio di Arresto</h2>
<p>Il <strong>criterio di arresto</strong> è una condizione che permette
di interrompere il calcolo quando si raggiunge una tolleranza
desiderata, cioè quando l’errore è inferiore ad una certa soglia.</p>
<h3 id="test-di-arresto-sul-residuo">Test di arresto sul residuo</h3>
<p>Il <strong>test di arresto sul residuo</strong> è:</p>
<p><span class="math display">\[
|f(\alpha)| + |f(x_k)| = |f(x_k)| \leq \text{toll}
\]</span></p>
<p><strong>Esempio:</strong></p>
<p><span class="math inline">\(f(x) = 10^{-50} x\)</span><br />
<span class="math inline">\(x_k = 1, \quad f(x_k) =
10^{-50}\)</span></p>
<p>oppure</p>
<p><span class="math inline">\(f(x) = 10^{50} x\)</span><br />
<span class="math inline">\(x_k = 10^{-50}, \quad f(x_k) =
1\)</span></p>
<p>sono inaffidabili, poiché il residuo è molto piccolo o molto
grande.</p>
<h3 id="test-sul-residuo-pesato">Test sul residuo pesato</h3>
<p>Per evitare questo problema, si può usare il <strong>test di arresto
sul residuo pesato</strong>:</p>
<p><span class="math display">\[
\dfrac{|f(x_k)|}{|f&#39;(x_k)|} = \varepsilon_k + O(\varepsilon_k^2)
\leq \text{toll}
\]</span></p>
<p><span class="math inline">\(O(\varepsilon_k^2)\)</span> è
trascurabile rispetto a <span
class="math inline">\(\varepsilon_k\)</span>.</p>
<p>Solo se <span class="math inline">\(f&#39;(\alpha) \neq 0\)</span>,
quindi se <span class="math inline">\(\alpha\)</span> è uno zero
semplice, il test di arresto sul residuo pesato è affidabile.</p>
<p><strong>Dimostrazione</strong>:</p>
<p><span class="math inline">\(\varepsilon_k = \alpha - x_k, \quad
\alpha = x_k + \varepsilon_k\)</span></p>
<p><span class="math inline">\(0 = f(\alpha) = f(x_k + \varepsilon_k) =
f(x_k) + f&#39;(x_k) \varepsilon_k + O(\varepsilon_k^2)\)</span></p>
<p><span class="math inline">\(\varepsilon_k \sim -
\dfrac{f(x_k)}{f&#39;(x_k)}\)</span></p>
<h2 id="metodo-di-newton---raphson">Metodo di Newton - Raphson</h2>
<p>Il <strong>metodo di Newton</strong> è un metodo iterativo per il
calcolo degli zeri di una funzione <span
class="math inline">\(f(x)\)</span>.</p>
<p><em>Applicabilità</em>: <span class="math inline">\(f \in C^1[a,
b]\)</span>, <span class="math inline">\(\alpha \in [a, b] | f(\alpha) =
0\)</span>.</p>
<p><em>Idea del metodo</em>: Approssimare la funzione <span
class="math inline">\(f(x)\)</span> con la sua retta tangente in <span
class="math inline">\(x_0\)</span>, cioè <span
class="math inline">\(f(x) \approx f(x_0) + f&#39;(x_0)(x -
x_0)\)</span>. In questo modo si approssima lo zero di <span
class="math inline">\(f(x)\)</span> con l’intersezione della retta
tangente con l’asse delle <span class="math inline">\(x\)</span>.</p>
<p><strong>Schema iterativo</strong>:</p>
<p><span class="math display">\[
x_k = \begin{cases}
y = f(x_k) + f&#39;(x_k)(x - x_k) \\
y = 0
\end{cases}
\]</span></p>
<p><span class="math display">\[
0 = f(x_k) + f&#39;(x_k)(x - x_k) \Rightarrow -
\dfrac{f(x_k)}{f&#39;(x_k)} = x - x_k
\]</span></p>
<p><span class="math display">\[
\Rightarrow \boxed{x_{k + 1} = x_k - \dfrac{f(x_k)}{f&#39;(x_k)}} \quad
k \geq 0
\]</span></p>
<p>oppure</p>
<p><span class="math display">\[
\begin{aligned}
x_{k + 1} &amp;= x_k + S \\
0 &amp;= f(x_{k + 1}) = f(x_k + S) = f(x_k) + f&#39;(x_k) S \\
S &amp;= - \dfrac{f(x_k)}{f&#39;(x_k)} \Rightarrow \boxed{x_{k + 1} =
x_k - \dfrac{f(x_k)}{f&#39;(x_k)}} \quad k \geq 0
\end{aligned}
\]</span></p>
<h3
id="teorema-di-convergenza-locale-del-metodo-di-newton---raphson">Teorema
di convergenza locale del metodo di Newton - Raphson</h3>
<p>Sia <span class="math inline">\(f \in C^2[a, b]\)</span>, <span
class="math inline">\(\alpha \in [a, b] | f(\alpha) = 0\)</span> e <span
class="math inline">\(f&#39;(x) \neq 0 \quad \forall x \in [a,
b]\)</span> (zero semplice).<br />
Allora <span class="math inline">\(\exists \delta &gt; 0\)</span> tale
che:</p>
<ul>
<li>il metodo di Newton è ben definito (non si divide per zero), <span
class="math inline">\(\{x_k\}_{k \geq 0} \subset B(\alpha,
\delta)\)</span></li>
<li><span
class="math inline">\(\dfrac{|\varepsilon_{k+1}|}{|\varepsilon_k|^2}
\leq M\)</span> quindi <span class="math inline">\(\displaystyle\lim_{k
\to \infty} x_k = \alpha\)</span> e il metodo è quadraticamente
convergente.</li>
</ul>
<p>Inoltre si ha che <span class="math inline">\(\displaystyle\lim_{k
\to \infty} \dfrac{|\varepsilon_{k+1}|}{|\varepsilon_k|^2} =
\dfrac{1}{2} \Big \vert \dfrac{f&#39;&#39;(\alpha)}{f&#39;(\alpha)} \Big
\vert\)</span></p>
<p>Nel caso in cui <span class="math inline">\(f&#39;&#39;(\alpha) =
0\)</span>, il metodo è più veloce (<span class="math inline">\(p \geq
3\)</span>).</p>
<h4 id="dimostrazione">Dimostrazione</h4>
<p><span class="math inline">\(\alpha = x_k + \varepsilon_k\)</span></p>
<p>Applico Taylor a <span class="math inline">\(f(x)\)</span> centrato
in <span class="math inline">\(x_k\)</span>:</p>
<p><span class="math inline">\(0 = f(\alpha) = f(x_k + \varepsilon_k) =
f(x_k) + f&#39;(x_k) \varepsilon_k + \dfrac{f&#39;&#39;(\xi_k)}{2}
\varepsilon_k^2 \quad \xi_k \in (x_k, \alpha)\)</span></p>
<p><span class="math inline">\(f&#39;(x_k) \neq 0 \Rightarrow -
\dfrac{f(x_k)}{f&#39;(x_k)} = \varepsilon_k +
\dfrac{f&#39;&#39;(\xi_k)}{2 f&#39;(x_k)} \varepsilon_k^2\)</span></p>
<p><span class="math inline">\(- \dfrac{f(x_k)}{f&#39;(x_k)} = x_{k+1} -
x_k = x_{k+1} - \alpha + \alpha - x_k = - \varepsilon_{k+1} +
\varepsilon_k = \varepsilon_k + \dfrac{f&#39;&#39;(\xi_k)}{2
f&#39;(x_k)} \varepsilon_k^2\)</span></p>
<p><span class="math inline">\(-
\dfrac{\varepsilon_{k+1}}{\varepsilon_k^2} = \dfrac{1}{2}
\dfrac{f&#39;&#39;(\xi_k)}{f&#39;(x_k)}\)</span></p>
<p><span class="math inline">\(\displaystyle
\dfrac{|\varepsilon_{k+1}|}{|\varepsilon_k|^2} \leq \dfrac{1}{2}
\dfrac{\max_{x \in [a, b]} |f&#39;&#39;(x)|}{\min_{x \in [a, b]}
|f&#39;(x)|} = M\)</span></p>
<p>Questo risultato ci permette di dimostrare la convergenza di <span
class="math inline">\(\{x_k\}\)</span> ad <span
class="math inline">\(\alpha\)</span> (ovvero <span
class="math inline">\(\displaystyle\lim_{k \to \infty} \varepsilon_k =
0\)</span>).</p>
<p><span class="math inline">\(| \varepsilon_{k+1} | \leq M |
\varepsilon_k |^2 \quad | \varepsilon_k | \leq M | \varepsilon_{k-1} |^2
\leq \dots \leq (M | \varepsilon_0 |^2)^2 \leq \dots \leq (M |
\varepsilon_0 )^{2^k}\)</span></p>
<p><span class="math inline">\(M | \varepsilon_k | \leq (M |
\varepsilon_0 |)^{2^k} \Rightarrow 0 \leq | \varepsilon_k | \leq
\dfrac{1}{M} (M | \varepsilon_0 |)^{2^k}\)</span></p>
<p><span class="math inline">\(\displaystyle \lim_{k \to \infty}
\dfrac{1}{M} (M | \varepsilon_0 |)^{2^k} = 0\)</span> se <span
class="math inline">\(| \varepsilon_0 | &lt; \dfrac{1}{M}\)</span></p>
<p><span class="math inline">\(| \varepsilon_0 | &lt; \dfrac{1}{M}
\Rightarrow | \alpha - x_0 | &lt; \dfrac{1}{M} \Rightarrow x_0 \in
[\alpha - \dfrac{1}{M}, \alpha + \dfrac{1}{M}] = B(\alpha,
\delta)\)</span></p>
<hr />
<p><strong>Osservazione</strong>:</p>
<p><span class="math inline">\(\forall x_0 \in B(\alpha, \dfrac{1}{M})
\Rightarrow \{x_k\} \subset B(\alpha, \dfrac{1}{M})\)</span></p>
<p>Dimostrazione: <span class="math inline">\(| \alpha - x_0 | &lt;
\dfrac{1}{M} \Rightarrow ???\)</span></p>
<hr />
<p>Da <span
class="math inline">\(\dfrac{|\varepsilon_{k+1}|}{|\varepsilon_k|^2} =
\dfrac{1}{2} \Big \vert \dfrac{f&#39;&#39;(\xi_k)}{f&#39;(\xi_k)} \Big
\vert\)</span> si ha che <span class="math inline">\(\displaystyle
\lim_{k \to \infty} \dfrac{|\varepsilon_{k+1}|}{|\varepsilon_k|^2} =
\dfrac{1}{2} \Big \vert \dfrac{f&#39;&#39;(\alpha)}{f&#39;(\alpha)} \Big
\vert\)</span></p>
<p>questo perché <span class="math inline">\(\xi_k \to \alpha\)</span>
per <span class="math inline">\(k \to \infty\)</span>, poiché <span
class="math inline">\(\xi_k \in (x_k, \alpha)\)</span> e <span
class="math inline">\(x_k \to \alpha\)</span>.</p>
<h3
id="ordine-di-convergenza-del-metodo-di-newton---raphson-e-costante-asintotica">Ordine
di convergenza del metodo di Newton - Raphson e costante asintotica</h3>
<p>Se <span class="math inline">\(f&#39;&#39;(\alpha) \neq 0\)</span>,
allora il metodo di Newton è <strong>quadratico</strong> (<span
class="math inline">\(p = 2\)</span>) con costante asintotica di
riduzione dell’errore <span class="math inline">\(C = \dfrac{1}{2} \Big
\vert \dfrac{f&#39;&#39;(\alpha)}{f&#39;(\alpha)} \Big \vert \neq
0\)</span>.</p>
<p>Se <span class="math inline">\(f&#39;&#39;(\alpha) = 0\)</span>,
allora <span class="math inline">\(p \geq 3\)</span>. Costante
asintotica di riduzione dell’errore <span class="math inline">\(C =
???\)</span>.</p>
<p>Se <span class="math inline">\(f&#39;(\alpha) = 0\)</span> (zero
multiplo), allora il metodo è <strong>lineare</strong> (<span
class="math inline">\(p = 1\)</span>), con costante asintotica di
riduzione dell’errore <span class="math inline">\(C = 1 -
\dfrac{1}{\tau}\)</span>, dove <span class="math inline">\(\tau\)</span>
è la molteplicità dello zero. Quindi so anche che maggiore è la
molteplicità dello zero, minore è la costante asintotica di riduzione
dell’errore, e quindi più lento è il metodo.</p>
<h3 id="criterio-di-arresto-per-il-metodo-di-newton---raphson">Criterio
di arresto per il metodo di Newton - Raphson</h3>
<p>Il <strong>criterio di arresto</strong> per il metodo di Newton -
Raphson è il residuo pesato:</p>
<p><span class="math display">\[
S_{k+1} = x_{k+1} - x_k
\]</span></p>
<p>che è lo scarto al passo <span class="math inline">\(k +
1\)</span>.</p>
<h3 id="idea-implementazione">Idea implementazione</h3>
<ol type="1">
<li><span class="math inline">\(f(x_k)\)</span> e <span
class="math inline">\(f&#39;(x_k)\)</span></li>
<li><span class="math inline">\(x_{k+1} = x_k -
\dfrac{f(x_k)}{f&#39;(x_k)}\)</span></li>
<li><span class="math inline">\(f(x_k), f&#39;(x_k) \neq 0\)</span></li>
<li><span class="math inline">\(S = - \dfrac{f(x_k)}{f&#39;(x_k)} \sim
\varepsilon_k\)</span></li>
<li><span class="math inline">\(x_{k+1} = x_k + S\)</span></li>
<li><span class="math inline">\(S_{k + 1} = x_{k+1} - x_k\)</span></li>
<li>if <span class="math inline">\(|S_{k+1}| \leq \text{toll}\)</span>
STOP<br />
</li>
<li><span class="math inline">\(S_{k+1} \sim \varepsilon_k\)</span></li>
</ol>
<h3 id="esercizio">Esercizio</h3>
<p>Dimostrare esistenza ed unicità dello zero <span
class="math inline">\(\alpha \in [0, 1]\)</span> della funzione <span
class="math inline">\(e^{-2x} - x^2 = 0\)</span>.</p>
<p><span class="math inline">\(f(0) = 1, \quad f(1) = e^{-2} - 1 = -
0.86466 &lt; 0\)</span></p>
<p>Quindi per il test di Bolzano esiste uno zero in <span
class="math inline">\([0, 1]\)</span>.</p>
<p><span class="math inline">\(f&#39;(x) = -2 e^{-2x} - 2x &lt;
0\)</span> per <span class="math inline">\(x \in [0, 1] \Rightarrow
f(x)\)</span> è monotona decrescente e lo zero è unico.</p>
<p>Se <span class="math inline">\(toll = 10^{-7}\)</span>:</p>
<p><span class="math inline">\(| \varepsilon_k | \leq \dfrac{b -
a}{2^{k+1}} = \dfrac{1 - 0}{2^{k+1}} &lt; 10^{-7} \Rightarrow k &gt;
\log_2(10^7) - 1 \approx 23.25 -1 \Rightarrow k = 23\)</span></p>
<h2 id="metodo-di-newton-in-più-variabili">Metodo di Newton in più
variabili</h2>
<p>Sia <span class="math inline">\(F: \mathbb{R}^n \to
\mathbb{R}^n\)</span> con <span class="math display">\[F(X) = (f_1(x_1,
x_2, \dots, x_n), f_2(x_1, x_2, \dots, x_n), \dots, f_n(x_1, x_2, \dots,
x_n))\]</span></p>
<p>La condizione <span class="math inline">\(F(X) = 0\)</span> equivale
a risolvere il sistema: <span class="math display">\[\begin{cases}
f_1(x_1, \dots, x_n) = 0 \\
f_2(x_1, \dots, x_n) = 0 \\
\dots \\
f_n(x_1, \dots, x_n) = 0
\end{cases}\]</span></p>
<p>Nel caso scalare, lo schema iterativo del metodo di Newton è: <span
class="math display">\[x_{k+1} = x_k -
\dfrac{f(x_k)}{f&#39;(x_k)}\]</span></p>
<p>Con scarto: <span class="math display">\[S_{k+1} = -
\dfrac{f(x_k)}{f&#39;(x_k)} = - f&#39;(x_k)^{-1} f(x_k)\]</span></p>
<p>dove <span class="math inline">\(f&#39;(x_k)\)</span> è scalare.</p>
<p>Nel caso di più variabili: <span class="math display">\[S_{k+1} = -
J_F(x_k)^{-1} F(x_k)\]</span> <span class="math display">\[x_{k+1} = x_k
+ S_{k+1}\]</span></p>
<p>dove <span class="math inline">\(J_F(x_k)\)</span> è la matrice
Jacobiana.</p>
<p>A livello computazionale, ad ogni iterazione si deve calcolare una
matrice <span class="math inline">\(n \times n\)</span> contenente le
derivate parziali. Poiché l’inversione di una matrice è costosa, spesso
si riscrive la formula come <span class="math inline">\(J_F(x_k) S_k =
-F(x_k)\)</span> e si risolve il sistema lineare ad ogni
iterazione.<br />
<span class="math inline">\(Ax = b\)</span> con <span
class="math inline">\(A = J_F(x_k)\)</span>, <span
class="math inline">\(x = S_k\)</span> e <span class="math inline">\(b =
-F(x_k)\)</span>.</p>
<h2 id="condizioni-di-convergenza">Condizioni di convergenza</h2>
<p>Per la convergenza locale del metodo di Newton, si richiede che: -
<span class="math inline">\(f \in C^2([a,b])\)</span>
<strong>diventa</strong> che <span class="math inline">\(f&#39;\)</span>
sia Lipschitziana (cioè esista una costante <span
class="math inline">\(L\)</span> tale che <span
class="math inline">\(|f&#39;(x) - f&#39;(y)| \leq L |x - y|, \forall x,
y \in [a, b]\)</span>) - <span class="math inline">\(f&#39;(x) \neq
0\)</span> per ogni <span class="math inline">\(x \in [a, b]\)</span>
<strong>diventa</strong> <span class="math inline">\(F(X)\)</span> sia
invertibile, quindi il determinante della Jacobiana sia diverso da
zero.</p>
<h2 id="varianti-del-metodo-di-newton">Varianti del Metodo di
Newton</h2>
<h3 id="metodo-della-tangente-fissa">Metodo della Tangente Fissa</h3>
<p><strong>Schema:</strong> <span class="math display">\[x_{k+1} = x_k -
\dfrac{f(x_k)}{f&#39;(x_0)}\]</span></p>
<p>il coefficiente angolare di ogni iterazione è quello della tangente
trovata in <span class="math inline">\(x_0\)</span>, cioè nella prima
iterazione.</p>
<p>Questo metodo ha un costo computazionale minore rispetto al metodo di
Newton normale, poiché <span class="math inline">\(f&#39;(x)\)</span> è
calcolato solo una volta per <span class="math inline">\(x =
x_0\)</span>, ma è più lento.</p>
<p>Ha convergenza lineare con coefficiente di convergenza <span
class="math inline">\(p = 1\)</span> e costante <span
class="math inline">\(C\)</span>: <span class="math display">\[C =
\left|1 - \dfrac{f&#39;(\alpha)}{f&#39;(x_0)}\right|\]</span> Se <span
class="math inline">\(f&#39;(\alpha) = 0\)</span>, allora <span
class="math inline">\(C = 1\)</span> e quindi non c’è convergenza.<br />
<strong>Il metodo della tangente fissa non converge per zeri
multipli</strong>.</p>
<h3 id="metodo-della-secante-variabile-o-delle-secanti">Metodo della
Secante Variabile (o delle Secanti)</h3>
<p><strong>Schema iterativo:</strong><br />
Per <span class="math inline">\(k \geq 1\)</span> (per iniziare il
metodo, servono due valori iniziali <span
class="math inline">\(x_0\)</span> e <span
class="math inline">\(x_1\)</span>):</p>
<p><span class="math display">\[x_{k+1} = x_k - \dfrac{f(x_k)(x_k -
x_{k-1})}{f(x_k) - f(x_{k-1})}\]</span></p>
<p>oppure</p>
<p><span class="math display">\[x_{k+1} = x_k -
\dfrac{f(x_k)}{a_k}\]</span></p>
<p>con</p>
<p><span class="math display">\[a_k = \dfrac{f(x_k) - f(x_{k-1})}{x_k -
x_{k-1}}\]</span></p>
<p>Metodo più lento del metodo di Newton ma comunque superlineare con
ordine di convergenza <span class="math inline">\(p = \dfrac{1 +
\sqrt{5}}{2} \approx 1.618\)</span>, se <span class="math inline">\(f
\in C^2([a, b])\)</span> e <span class="math inline">\(f&#39;&#39;(x)
\neq 0\)</span>. Con costante asintotica <span class="math inline">\(C =
\dfrac{1}{2}(1 + \sqrt{5})\)</span>.</p>
<p>Il metodo ha una forte riduzione dell’errore: <span
class="math display">\[\dfrac{|e_{k+1}|}{|e_k|} \approx C
|e_k|^{1.618}\]</span></p>
<p>dove <span class="math inline">\(C = \dfrac{1}{2}(1 +
\sqrt{5})\)</span>.</p>
<h3 id="metodo-di-punto-fisso-o-iterazione-funzionale">Metodo di Punto
Fisso (o Iterazione Funzionale)</h3>
<p>Si definisce <span class="math inline">\(\alpha\)</span> un punto
fisso di <span class="math inline">\(g: [a, b] \to \mathbb{R}\)</span>
se <span class="math inline">\(g(\alpha) = \alpha\)</span>.</p>
<p>Il problema <span class="math inline">\(f(x) = 0\)</span> viene
riformulato come <span class="math inline">\(x = g(x)\)</span>.</p>
<p>Esempio: <span class="math display">\[f(x) = e^{-x} - x\]</span>
Soluzione esplicitata: <span class="math display">\[x =
e^{-x}\]</span></p>
<p><strong>Schema iterativo:</strong> <span
class="math display">\[x_{k+1} = g(x_k)\]</span></p>
<p>Il metodo converge se la derivata di <span
class="math inline">\(g\)</span> soddisfa: <span
class="math display">\[|g&#39;(\alpha)| &lt; 1 \quad | g&#39;(x) | \leq
m &lt; 1 \quad \forall x \in I_{\alpha}\]</span></p>
<h3
id="teorema-di-convergenza-globale-del-metodo-di-punto-fisso">Teorema di
Convergenza Globale del Metodo di Punto Fisso</h3>
<p><strong>Teorema</strong>: Sia <span class="math inline">\(g\)</span>
continua in <span class="math inline">\([a, b]\)</span> e derivabile in
<span class="math inline">\((a, b)\)</span> e tale che <span
class="math inline">\(g(x) \in [a, b]\)</span> per ogni <span
class="math inline">\(x \in [a, b]\)</span>, cioè <span
class="math inline">\(g([a, b]) \subseteq [a, b]\)</span>. Allora <span
class="math inline">\(\exists \alpha \in [a, b]\)</span> tale che <span
class="math inline">\(g(\alpha) = \alpha\)</span>.<br />
Se inoltre si ha <span class="math inline">\(|g&#39;(x)| \leq m &lt;
1\)</span> per ogni <span class="math inline">\(x \in [a, b]\)</span>,
allora il punto fisso <span class="math inline">\(\alpha\)</span> è
unico e <span class="math inline">\(\displaystyle\lim_{k \to \infty}
|\varepsilon_k| = 0, \quad \forall x_0 \in [a, b]\)</span>.</p>
<p><strong>Dimostrazione</strong>: … (Note sul metodo di punto fisso
5/17)</p>
<h3 id="teorema-di-convergenza-locale-del-metodo-di-punto-fisso">Teorema
di Convergenza Locale del Metodo di Punto Fisso</h3>
<p><strong>Teorema</strong>: Sia <span class="math inline">\(g \in
C^1([a, b])\)</span> e sia <span class="math inline">\(\alpha\)</span>
un punto fisso di <span class="math inline">\(g\)</span> in <span
class="math inline">\([a, b]\)</span>. Se <span
class="math inline">\(|g&#39;(\alpha)| &lt; 1\)</span>, allora <span
class="math inline">\(\exists \delta &gt; 0\)</span> tale che <span
class="math inline">\(\forall x_0 \in B(\alpha, \delta) = (\alpha -
\delta, \alpha + \delta)\)</span>, lo schema iterativo di punto fisso
converge ad <span class="math inline">\(\alpha\)</span>.<br />
Si ha anche</p>
<p><span class="math display">\[
\lim_{k \to \infty} \dfrac{|\varepsilon_{k+1}|}{|\varepsilon_k|} =
g&#39;(\alpha)
\]</span></p>
<p><strong>Dimostrazione</strong>: … (Note sul metodo di punto fisso
6-8/17)</p>
<h3 id="relazione-errore---scarto-nel-metodo-di-punto-fisso">Relazione
Errore - Scarto nel metodo di punto fisso</h3>
<p>… (Note sul metodo di punto fisso 9/17)</p>
<p>… ?</p>
<h1 id="approssimazione-di-funzioni-e-dati">Approssimazione di funzioni
e dati</h1>
<p><span class="math inline">\(f\)</span> e <span
class="math inline">\(\tilde{f}\)</span> sono due funzioni definite su
un intervallo <span class="math inline">\(I\)</span>.</p>
<p><span class="math inline">\(x_0, x_1, \dots, x_n\)</span> sono i
<strong>nodi di interpolazione</strong>.</p>
<p><span class="math inline">\(y_0 = f(x_0), y_1 = f(x_1), \dots, y_n =
f(x_n)\)</span> sono i valori della funzione nei punti di
interpolazione.</p>
<p><span class="math inline">\(\tilde{f}(x)\)</span> è l’approssimazione
della funzione <span class="math inline">\(f(x)\)</span>.</p>
<p>Selezionato uno spazio funzionale (es. polinomiale), si scrive</p>
<p><span class="math display">\[
\tilde{f}(x) = \sum_{k=0}^n a_k \phi_k(x)
\]</span></p>
<p>combinazione lineare delle funzioni di una base dello spazio
funzionale scelto.</p>
<h2 id="interpolazione-polinomiale">Interpolazione polinomiale</h2>
<p><span class="math display">\[
\mathbb{P}_n (x) = \prod {}_n (x) = \{p(x) = a_0 + a_1 x + a_2 x^2 +
\dots + a_n x^n | a_k \in \mathbb{R} \}
\]</span></p>
<p><span class="math inline">\(\mathbb{P}_n\)</span> è lo spazio dei
polinomi di grado al più <span class="math inline">\(n\)</span>.</p>
<p><strong>Problema</strong>: Data una serie di punti della forma <span
class="math inline">\(\{(x_i, y_i)\}_{i=0}^n\)</span>, quindi <span
class="math inline">\(n + 1\)</span> coppie di punti, con <span
class="math inline">\(x_i \neq x_j\)</span> per <span
class="math inline">\(i \neq j\)</span>, il problema dell’interpolazione
polinomiale consiste nel trovare un polinomio <span
class="math inline">\(p_n(x)\)</span> tale che <span
class="math inline">\(p_n(x_i) = y_i\)</span> per ogni <span
class="math inline">\(i = 0, 1, \dots, n\)</span>, dette
<strong>condizioni di interpolazione</strong>. Gli <span
class="math inline">\(x_i\)</span> sono i nodi di interpolazione.</p>
<h3 id="teorema">Teorema</h3>
<p>Date <span class="math inline">\(n + 1\)</span> coppie di punti <span
class="math inline">\(\{(x_i, y_i)\}_{i=0}^n\)</span> con <span
class="math inline">\(x_i \neq x_j\)</span> per <span
class="math inline">\(i \neq j\)</span>, esiste un unico polinomio di
grado al più <span class="math inline">\(n\)</span> che soddisfa le
condizioni di interpolazione, cioè <span class="math inline">\(p_n(x_i)
= y_i\)</span> per ogni <span class="math inline">\(i = 0, 1, \dots,
n\)</span>.</p>
<p><strong>Dimostrazione</strong>: Dimostriamo l’esistenza e l’unicità
unitamente.</p>
<p><span class="math display">\[
\begin{aligned}
p_n(x_0) &amp;= a_0 + a_1 x_0 + a_2 x_0^2 + \dots + a_n x_0^n &amp;= y_0
\\
p_n(x_1) &amp;= a_0 + a_1 x_1 + a_2 x_1^2 + \dots + a_n x_1^n &amp;= y_1
\\
p_n(x_2) &amp;= a_0 + a_1 x_2 + a_2 x_2^2 + \dots + a_n x_2^n &amp;= y_2
\\
&amp;&amp;\vdots \\
p_n(x_n) &amp;= a_0 + a_1 x_n + a_2 x_n^2 + \dots + a_n x_n^n &amp;= y_n
\end{aligned}
\]</span></p>
<p>Ho quindi un sistema di <span class="math inline">\(n + 1\)</span>
equazioni in <span class="math inline">\(n + 1\)</span> incognite.</p>
<p>La matrice dei coefficienti è una matrice di Vandermonde, che è
sempre invertibile se <span class="math inline">\(x_i \neq x_j\)</span>
per <span class="math inline">\(i \neq j\)</span>. Quindi il sistema ha
una e una sola soluzione.</p>
<p><span class="math display">\[
\underbrace{\begin{pmatrix}
1 &amp; x_0 &amp; x_0^2 &amp; \dots &amp; x_0^n \\
1 &amp; x_1 &amp; x_1^2 &amp; \dots &amp; x_1^n \\
1 &amp; x_2 &amp; x_2^2 &amp; \dots &amp; x_2^n \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_n &amp; x_n^2 &amp; \dots &amp; x_n^n
\end{pmatrix}}_{V \in \mathbb{R}^{n + 1 \times n + 1}}
\begin{pmatrix}
a_0 \\
a_1 \\
a_2 \\
\vdots \\
a_n
\end{pmatrix}
=
\begin{pmatrix}
y_0 \\
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}
\]</span></p>
<h3 id="matrice-di-vandermonde">Matrice di Vandermonde</h3>
<p><span class="math display">\[det(V) = \prod_{0 \leq i &lt; j \leq n}
(x_j - x_i)\]</span></p>
<p>Esempio: <span class="math inline">\(n = 2\)</span>, <span
class="math inline">\(x_0, x_1, x_2\)</span> distinti.</p>
<p><span class="math display">\[
det(V) = \begin{vmatrix}
1 &amp; x_0 &amp; x_0^2 \\
1 &amp; x_1 &amp; x_1^2 \\
1 &amp; x_2 &amp; x_2^2
\end{vmatrix}
= (x_2 - x_1)(x_2 - x_0)(x_1 - x_0)
\]</span></p>
<p>Non sarà mai nullo poiché <span class="math inline">\(x_0, x_1,
x_2\)</span> sono distinti (nodi, cioè solo ascisse).</p>
<p><strong>Calcolo del determinante con Laplace</strong>:</p>
<p><span class="math display">\[
det(V) = \sum_{j=0}^n (-1)^{i + j} a_{ij} det(M_{ij})\]</span> dove
<span class="math inline">\(M_{ij}\)</span> è la matrice che si ottiene
eliminando la riga <span class="math inline">\(i\)</span> e la colonna
<span class="math inline">\(j\)</span> dalla matrice <span
class="math inline">\(V\)</span>.</p>
<p><strong>Esempio</strong>: <span class="math inline">\(n = 2\)</span>,
<span class="math inline">\(x_0, x_1, x_2\)</span> distinti.</p>
<p>…</p>
<h3
id="dimostrazione-alternativa-dellunicità-del-teorema-del-polinomio-di-interpolazione">Dimostrazione
alternativa dell’unicità del teorema del polinomio di
interpolazione</h3>
<p>Supponiamo che esistano due polinomi di grado al più <span
class="math inline">\(n\)</span> che soddisfano le condizioni di
interpolazione, chiamati <span class="math inline">\(p_n(x)\)</span> e
<span class="math inline">\(q_n(x)\)</span>, con <span
class="math inline">\(p_n(x_i) = q_n(x_i) = y_i\)</span> per ogni <span
class="math inline">\(i = 0, 1, \dots, n\)</span>.</p>
<p>Allora <span class="math inline">\(p_n(x) - q_n(x)\)</span> è un
polinomio di grado al più <span class="math inline">\(n\)</span> e <span
class="math inline">\(p_n(x_i) - q_n(x_i) = 0\)</span> per ogni <span
class="math inline">\(i = 0, 1, \dots, n\)</span>, perché <span
class="math inline">\(p_n(x_i) = y_i\)</span> e <span
class="math inline">\(q_n(x_i) = y_i\)</span>.</p>
<p>Contraddizione per il teorema fondamentale dell’algebra, che afferma
che un polinomio di grado al più <span class="math inline">\(n\)</span>
ha al massimo <span class="math inline">\(n\)</span> zeri distinti.
Quindi <span class="math inline">\(p_n(x) - q_n(x) = 0\)</span> per ogni
<span class="math inline">\(x \in [a, b]\)</span>, quindi <span
class="math inline">\(p_n(x) = q_n(x)\)</span> per ogni <span
class="math inline">\(x \in [a, b]\)</span>.</p>
<h3
id="dimostrazione-alternativa-dellesistenza-del-teorema-del-polinomio-di-interpolazione">Dimostrazione
alternativa dell’esistenza del teorema del polinomio di
interpolazione</h3>
<p>Lagrange osservò che il polinomio di interpolazione può essere
scritto come una combinazione lineare di polinomi di Lagrange, definiti
come:</p>
<p><span class="math display">\[
L_i(x) = \prod_{j=0, j \neq i}^n \dfrac{x - x_j}{x_i - x_j} =
\dfrac{(x - x_0)(x - x_1) \cdots (x - x_{i-1})(x - x_{i+1}) \cdots (x -
x_n)}{(x_i - x_0)(x_i - x_1) \cdots (x_i - x_{i-1})(x_i - x_{i+1})
\cdots (x_i - x_n)}
\]</span></p>
<p><span class="math inline">\(L_i(x_j) = \delta_{ij}\)</span>, cioè
<span class="math inline">\(L_i(x_j) = 1\)</span> se <span
class="math inline">\(i = j\)</span> e <span
class="math inline">\(L_i(x_j) = 0\)</span> se <span
class="math inline">\(i \neq j\)</span>.</p>
<p><strong>Teorema (forma di Lagrange dell’interpolante)</strong>:<br />
Dati <span class="math inline">\(n + 1\)</span> punti distinti <span
class="math inline">\(x_0, x_1, \dots, x_n\)</span> e i corrispondenti
valori <span class="math inline">\(y_0 = f(x_0), y_1 = f(x_1), \dots,
y_n = f(x_n)\)</span>, il polinomio di interpolazione è dato da:</p>
<p><span class="math display">\[
p_n(x) = \sum_{i=0}^n y_i L_i(x)
\]</span></p>
<p><span class="math inline">\(\{L_0(x), L_1(x), \dots,
L_n(x)\}\)</span> è la base dei polinomi di Lagrange.<br />
<span class="math inline">\(\{1, x, x^2, \dots, x^n\}\)</span> è la base
polinomiale.</p>
<p>Esempio: Calcolare il polinomio di grado 2 che assume nei nodi <span
class="math inline">\(x_0 = -2, x_1 = -1, x_2 = 3\)</span>
rispettivamente i valori <span class="math inline">\(y_0 = -2, y_1 = 11,
y_2 = 17\)</span>.</p>
<p><span class="math inline">\((-2, -2), (-1, 11), (3, 17)\)</span></p>
<p><span class="math inline">\(L_0 (x) = \dfrac{(x + 1)(x - 3)}{(-2 +
1)(-2 - 3)} = \dfrac{(x + 1)(x - 3)}{5} \\ L_1 (x) =\)</span><br />
<span class="math inline">\(\dfrac{(x + 2)(x - 3)}{(-1 + 2)(-1 - 3)} =
\dfrac{(x + 2)(x - 3)}{-4} \\ L_2 (x) =\)</span><br />
<span class="math inline">\(\dfrac{(x + 2)(x + 1)}{(3 + 2)(3 + 1)} =
\dfrac{(x + 2)(x + 1)}{20}\)</span></p>
<p><span class="math inline">\(p_2 (x) = -2 L_0 (x) + 11 L_1 (x) + 17
L_2 (x)\)</span></p>
<h3 id="errore-di-interpolazione">Errore di interpolazione</h3>
<p>l’errore o resto di interpolazione verrà denotato come: <span
class="math inline">\(\mathcal{C}_n (x) = f(x) - p_n(x)\)</span>.</p>
<p><span class="math inline">\(f\)</span> sufficientemente regolare
(continua e derivabile) in un intervallo <span class="math inline">\([a,
b] \to \mathbb{R}\)</span>.</p>
<p><strong>Teorema (dell’errore o resto di
interpolazione)</strong>:<br />
Sia <span class="math inline">\(f \in C^{n + 1}([a, b])\)</span> e sia
<span class="math inline">\(p_n(x)\)</span> il polinomio di
interpolazione di <span class="math inline">\(f\)</span> in <span
class="math inline">\(n + 1\)</span> nodi <span class="math inline">\(a
\leq x_0 &lt; x_1 &lt; \dots &lt; x_n \leq b\)</span>.<br />
Per ogni <span class="math inline">\(x \in [a, b]\)</span> esiste un
<span class="math inline">\(\xi_x\)</span> tale che:</p>
<p><span class="math display">\[
\mathcal{C}_n (x) = \dfrac{f^{(n + 1)}(\xi_x)}{(n + 1)!} \omega_{n + 1}
(x)
\]</span></p>
<p>dove <span class="math inline">\(\displaystyle \omega_{n + 1} (x) =
\prod_{i=0}^n (x - x_i)\)</span> è il polinomio di Lagrange di grado
<span class="math inline">\(n + 1\)</span> che ha gli zeri nei nodi di
interpolazione, detto <strong>polinomio nodale</strong>.</p>
<p><strong>Dimostrazione</strong>:</p>
<p><span class="math inline">\(x = x_i\)</span> per <span
class="math inline">\(i = 0, 1, \dots, n\)</span><br />
<span class="math inline">\(\mathcal{C}_n (x_i) = f(x_i) - p_n(x_i) =
0\)</span> per ogni <span class="math inline">\(i = 0, 1, \dots,
n\)</span><br />
<span class="math inline">\(\omega_{n + 1} (x_i) = 0\)</span> per ogni
<span class="math inline">\(i = 0, 1, \dots, n\)</span></p>
<p>Sia <span class="math inline">\(x \in [a, b]\)</span> e <span
class="math inline">\(x \neq x_i\)</span> per ogni <span
class="math inline">\(i = 0, 1, \dots, n\)</span> (quindi <span
class="math inline">\(\omega_{n + 1} (x) \neq 0\)</span>).<br />
<span class="math inline">\(g(t) = \mathcal{C}_n (t) - \omega_{n + 1}
(t) \cdot \dfrac{\mathcal{C}_n (x)}{\omega_{n + 1} (x)}\)</span>
funzione ausiliaria</p>
<p>…</p>
<h2 id="fenomeno-di-runge">Fenomeno di Runge</h2>
<p>… (lezione 14 aprile)</p>
<p>… (lezione 15 aprile)</p>
<p>… (Dispensa sull’interpolazione polinomiale)</p>
</body>
</html>
