<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Giovanni Norbedo" />
  <title>Algebra Lineare ed Elementi di Geometria</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  
  
  
  
  
  
  
  
  
  
  
  
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Algebra Lineare ed Elementi di Geometria</h1>
<p class="subtitle">Intelligenza Artificiale &amp; Data Analytics</p>
<p class="author">Giovanni Norbedo</p>
<p class="date">2023-2024</p>
</header>
<nav id="TOC" role="doc-toc">

</nav>
<h1 id="introduzione">Introduzione</h1>
<h2 id="equazioni-e-soluzioni">Equazioni e Soluzioni</h2>
<p>Che cos’è un’equazione? Un’<em>equazione</em> è una
<em>domanda</em>.</p>
<p><span class="math inline">\(x^2+2x+1=0\)</span></p>
<p>Questo è un modo di formalizzare la domanda “Qual è quel numero, che
indichiamo con <span class="math inline">\(x\)</span>, tale che se
calcolo il numero <span class="math inline">\(x^2 + 2x +1\)</span>, esso
è <span class="math inline">\(0\)</span>?</p>
<p>Una <em>soluzione</em> è una <em>risposta</em> (corretta) alla
domanda.</p>
<p>Per esempio nel nostro caso abbiamo che il numero <span
class="math inline">\(-1\)</span> è soluzione, perché:</p>
<p><span class="math inline">\((-1)^2 + 2(-1) + 1 = 1 -2 + 1 =
0\)</span></p>
<p>Dato che la quantità che otteniamo al membro sinistro dell’uguale è
la medesima di quella che otteniamo a destra possiamo affermare che
<span class="math inline">\(-1\)</span> è soluzione.</p>
<p>La teoria delle equazioni di secondo grado si dice che non ci sono
altre soluzioni e che <span class="math inline">\(x^2 +2x + 1 =
(x+1)^2\)</span>.</p>
<p>Passiamo, invece, a considerare:</p>
<p><span class="math inline">\(3x + y -2z = 0\)</span></p>
<p><strong>Attenzione</strong>: Soluzione banale con <span
class="math inline">\((0,0,0)\)</span></p>
<p>Risolvere questa equazione significa determinare una (o tutte) le
terne di numeri <span class="math inline">\((x,y,z)\)</span> tali che,
se sostituiamo tali numeri alle variabili nel membro sinistro, otteniamo
zero.</p>
<p><strong>Osservazione</strong><br />
Nella prima equazione ci veniva chiesto di determinare <em>un
numero</em>, nella seconda ogni sostituzione è costituita da <em>tre
numeri</em>; diciamo quindi che la prima equazione è in <em>una
variabile</em>, la seconda in <em>tre variabili</em>.</p>
<p>Una prima soluzione è data dalla scelta:</p>
<p><span class="math inline">\(x=0, y=0, z=0\)</span> ovvero dalla terna
<span class="math inline">\((0,0,0)\)</span></p>
<p>infatti</p>
<p><span class="math inline">\(3 \cdot 0 + 1 \cdot 0 - 2 \cdot 0 =
0\)</span></p>
<p>anche</p>
<p><span class="math inline">\(x=1,y=1,z=2\)</span> ovvero la terna
<span class="math inline">\((1,1,2)\)</span></p>
<p>è soluzione, perché</p>
<p><span class="math inline">\(3 \cdot 1 + 1 \cdot 1 - 2 \cdot 2 = 3 + 1
-4 = 0\)</span></p>
<p>Similmente, anche</p>
<p><span class="math inline">\((0, 2, 1)\)</span></p>
<p>è soluzione, perché</p>
<p><span class="math inline">\(3 \cdot 0 + 1 \cdot 2 - 2 \cdot 1 = 0 + 2
-2 = 0\)</span></p>
<p>Ora accade che da queste ultime due soluzioni che abbiamo esibito
possiamo costruirne altre, sfruttando le proprietà di base delle
operazioni tra numeri, in particolare la proprietà associativa,
commutativa e distributiva.</p>
<p>Più concretamente, vorrei mostrare che</p>
<p><span class="math inline">\(x=2,y=2,z=4\)</span> ovvero la terna
<span class="math inline">\((2,2,4)\)</span></p>
<p>è anch’essa soluzione. Infatti</p>
<p><span class="math inline">\(3 \cdot 2 + 1 \cdot 2 - 2 \cdot 4 = 6 + 2
-8 = 0\)</span></p>
<p>Però possiamo vedere la terna <span
class="math inline">\((2,2,4)\)</span> anche nel modo seguente, partiamo
dalla terna <span class="math inline">\((1,1,2)\)</span> e
moltiplichiamo ogni sua entrata per 2, ottenendo appunto <span
class="math inline">\((2,2,4)\)</span> [sappiamo già che <span
class="math inline">\((1,1,2)\)</span> è soluzione].</p>
<p>In maniera più compatta, introduciamo la notazione</p>
<p><span class="math inline">\((2,2,4) = 2 \cdot (1,1,2)\)</span></p>
<p>Riprendiamo la quantità che abbiamo calcolato prima</p>
<p><span class="math inline">\(3 \cdot 2 + 1 \cdot 2 - 2 \cdot 4 = 3
\cdot (2 \cdot 1) + 1 \cdot (2 \cdot 1) -2 \cdot (2 \cdot 2)
=\)</span></p>
<p><span class="math inline">\(= (3 \cdot 2) \cdot 1 + (1 \cdot 2) \cdot
1 -(2 \cdot 2) \cdot 2\)</span><br />
per la proprietà associativa</p>
<p><span class="math inline">\(= 2 \cdot (3 \cdot 1) + 2(1 \cdot 1) - 2
\cdot (2 \cdot 2)\)</span><br />
per la proprietà commutativa</p>
<p><span class="math inline">\(= 2 \cdot (3 \cdot 1 + 1 \cdot 1 -2 \cdot
2)\)</span><br />
per la proprietà distributiva</p>
<p><span class="math inline">\(= 2 \cdot 0 = 0\)</span></p>
<p><span class="math inline">\((3 \cdot 1 + 1 \cdot 1 -2 \cdot
2)\)</span><br />
questa quantita è zero perché <span
class="math inline">\((1,1,2)\)</span> è soluzione.</p>
<p>Lo stesso ragionamento ci mostra che la terna</p>
<p><span class="math inline">\((37,37,74)\)</span></p>
<p>è soluzione, perché</p>
<p><span class="math inline">\(3 \cdot 37 + 1 \cdot 37 -2 \cdot 74 = 37
\cdot (3 \cdot 1 + 1 \cdot 1 - 2 \cdot 2)\)</span></p>
<p>Usando la notazione di prima, possiamo dire che <span
class="math inline">\((37,37,74)\)</span> è soluzione perché</p>
<p><span class="math inline">\((37,37,74) = 37 \cdot
(1,1,2)\)</span></p>
<p>Generalizzando, vediamo che per ogni <span
class="math inline">\(\alpha \in \mathbb{R}\)</span>,<br />
la terna <span class="math inline">\((\alpha, \alpha, 2 \alpha) = \alpha
\cdot (1,1,2)\)</span> è soluzione.</p>
<p>Analizziamo ora un secondo fenomeno.<br />
Vorrei mostrare che la terna <span
class="math inline">\((1,3,3)\)</span> è soluzione.</p>
<p>Osserviamo che</p>
<p><span class="math inline">\(1 = 1 + 0\)</span><br />
<span class="math inline">\(3 = 1 + 2\)</span><br />
<span class="math inline">\(3 = 2 + 1\)</span></p>
<p>In una notazione più compatta scriviamo</p>
<p><span class="math inline">\((1,3,3) = (1,1,2) + (0,2,1)\)</span></p>
<p>Ora calcoliamo</p>
<p><span class="math inline">\(3 \cdot 1 + 1 \cdot 3 - 2 \cdot 3 = 3
\cdot (1+0) + 1 \cdot (1+2) -2 \cdot (2+1) =\)</span><br />
<span class="math inline">\(= (3 \cdot 1 + 1 \cdot 1 -2 \cdot 2) + (3
\cdot 0 + 1 \cdot 2 -2 \cdot 1) =\)</span><br />
<span class="math inline">\(= 0\)</span><br />
perché <span class="math inline">\((1,1,2)\)</span> è soluzione</p>
<p>Considerando quanto abbiamo imparato, troviamo che</p>
<ul>
<li><p>A. La terna <span class="math inline">\((0,0,0)\)</span> è
soluzione.</p></li>
<li><p>B. Che <span class="math inline">\((\overline{x}, \overline{y},
\overline{z})\)</span> è un particolare soluzione, allora per ogni <span
class="math inline">\(\alpha \in \mathbb{R}\)</span> anche <span
class="math inline">\(\alpha \cdot (\overline{x}, \overline{y},
\overline{z})\)</span> è soluzione.</p></li>
<li><p>C. Che <span class="math inline">\((\overline{x}, \overline{y},
\overline{z})\)</span> e <span class="math inline">\((\hat{x}, \hat{y},
\hat{z})\)</span> sono due soluzioni, allora anche <span
class="math inline">\((\overline{x}, \overline{y}, \overline{z}) +
(\hat{x}, \hat{y}, \hat{z}) = (\overline{x} + \hat{x}, \overline{y} +
\hat{y}, \overline{z} + \hat{z})\)</span> è soluzione.</p></li>
</ul>
<p>Consideriamo ora il sistema</p>
<p><span class="math display">\[\begin{cases}
    \begin{aligned}
        3x + 1y - 2z &amp; = 0 \\
      - 2x - 2y + 2z &amp; = 0 \\  
        2x + 0y - 1z &amp; = 0
    \end{aligned}
\end{cases}\]</span></p>
<p>Le proprietà A, B e C valgono anche in questo caso.</p>
<p>Ora vorrei calcolare le soluzioni di questo sistema. Per farlo usiamo
un teorema chiamato “<em>eliminazione di Gauss</em>” o “<em>eliminazione
gaussiana</em>”.</p>
<p><strong>Definizione</strong><br />
Due sistemi si dicono <strong>equivalenti</strong> se hanno le stesse
soluzioni.</p>
<p>Andremo a manipolare il sistema al fine di trovarne uno equivalente
(ovvero con le stesse soluzioni) più semplicemente da risolvere, ovvero
nella forma:</p>
<ul>
<li>un’equazione di cui compaiono tre variabili</li>
<li>un’equazione di cui compaiono due variabili</li>
<li>un’equazione di cui compaia una variabile</li>
</ul>
<p>Partiamo mostrando che</p>
<p><span class="math inline">\(-2x -2y + 2z = 0\)</span></p>
<p>è equivalente a</p>
<p><span class="math inline">\(x + y -z = 0\)</span></p>
<p>Il sistema diviene</p>
<p><span class="math display">\[
\begin{cases}
    \begin{aligned}
        1x + 1y - 1z &amp;= 0 \\
        3x - 1y + 2z &amp;= 0 \\  
        2x + 0y - 1z &amp;= 0
    \end{aligned}
\end{cases}
\]</span></p>
<p>Manipolo la seconda equazione per “eliminare la x”, sottraendo tre
volte la prima equazione</p>
<p><span class="math inline">\((3x +y -2z) -3 \cdot (x + y -z) = 0 -3
\cdot 0\)</span></p>
<p><span class="math inline">\(3x +y -2z -3x -3y +3x = +y -2z - 3y + 3z
= 0\)</span></p>
<p><span class="math inline">\(-2y + z = 0\)</span></p>
<p>Facciamo la stessa cosa con la terza equazione</p>
<p><span class="math inline">\((2x - z) -2 \cdot (x+y-z) = 0 - 2 \cdot
0\)</span></p>
<p><span class="math inline">\(2x -z -2x -2y +2z = 0\)</span></p>
<p><span class="math inline">\(-2y + z = 0\)</span></p>
<p>In definitiva il sistema è equivalente a</p>
<p><span class="math display">\[
\begin{cases}
    \begin{aligned}
        x + y - z &amp;= 0 \\
        -2y + z &amp;= 0
    \end{aligned}
\end{cases}
\]</span></p>
<p>Questo sistema ci dice che se <span class="math inline">\(z\)</span>
assume un valore <span class="math inline">\(\alpha \in
\mathbb{R}\)</span><br />
allora <span class="math inline">\(y\)</span> deve essere <span
class="math inline">\(\frac{1}{2} \alpha\)</span> e <span
class="math inline">\(x\)</span> deve essere <span
class="math inline">\(\frac{1}{2} \alpha\)</span>.</p>
<p>Quindi le soluzioni sono della forma</p>
<p><span class="math inline">\((\frac{1}{2} \alpha, \frac{1}{2} \alpha,
\alpha) = \alpha \cdot (\frac{1}{2}, \frac{1}{2}, 1)\)</span></p>
<p>Se ora, a partite dal sistema iniziale, estraessimo i coefficienti e
li ponessimo in una tabella, otterremmo</p>
<p><span class="math display">\[
\begin{cases}
    \begin{aligned}
        3x + 1y - 2z &amp;= 0 \\
    - 2x - 2y + 2z &amp;= 0 \\  
        2x + 0y - 1z &amp;= 0
    \end{aligned}
\end{cases}
\]</span></p>
<p><span class="math display">\[
\begin{pmatrix}
    3  &amp;  1 &amp; -2 \\
    -2 &amp; -2 &amp;  2 \\
    2  &amp;  0 &amp; -1
\end{pmatrix}
\]</span></p>
<p>Moltiplico per <span class="math inline">\(- \frac{1}{2}\)</span> la
seconda equazione</p>
<p><span class="math display">\[
\begin{pmatrix}
    3  &amp;  1 &amp; -2 \\
    1  &amp;  1 &amp; -1 \\
    2  &amp;  0 &amp; -1
\end{pmatrix}
\]</span></p>
<p>Inverto la prima equazione con la seconda</p>
<p><span class="math display">\[
\begin{pmatrix}
    1  &amp;  1 &amp; -1 \\
    3  &amp;  1 &amp; -2 \\
    2  &amp;  0 &amp; -1
\end{pmatrix}
\]</span></p>
<p>Sottraggo dall’attuale seconda equazione la prima equazione
moltiplicata per <span class="math inline">\(3\)</span></p>
<p><span class="math display">\[
\begin{pmatrix}
    1  &amp;  1 &amp; -1 \\
    0  &amp; -2 &amp;  1 \\
    2  &amp;  0 &amp; -1
\end{pmatrix}
\]</span></p>
<p>Sottraggo dalla terza equazione la prima moltiplicata per <span
class="math inline">\(2\)</span></p>
<p><span class="math display">\[
\begin{pmatrix}
    1  &amp;  1 &amp; -1 \\
    0  &amp; -2 &amp;  1 \\
    0  &amp; -2 &amp;  1
\end{pmatrix}
\]</span></p>
<p>Sottraggo dalla terza equazione la seconda</p>
<p><span class="math display">\[
\begin{pmatrix}
    1  &amp;  1 &amp; -1 \\
    0  &amp; -2 &amp;  1 \\
    0  &amp;  0 &amp;  0
\end{pmatrix}
\]</span></p>
<p>Sono riuscito a risolvere il sistema.<br />
Ho utilizzato una procedura algoritmica (un algoritmo può essere
eseguito da un calcolatore).</p>
<hr />
<h1 id="vettori-applicati-e-vettori-liberi">Vettori applicati e vettori
liberi</h1>
<p>Ci mettiamo nel contesto della geometria euclidea.<br />
Un <strong>vettore applicato</strong> è un segmento orientato,
caratterizzato dunque da:</p>
<ul>
<li>un <strong>punto di applicazione</strong> (PDA)</li>
<li>una <strong>direzione</strong></li>
<li>un <strong>verso</strong></li>
<li>una <strong>lunghezza</strong> (o modulo)</li>
</ul>
<p>Un <strong>vettore applicato</strong> è determinato da una coppia
ordinata <span class="math inline">\((A,B)\)</span> di punti, in tal
caso il vettore si denota <span
class="math inline">\(\overrightarrow{AB}\)</span>.</p>
<p>Per ogni punto di applicazione esiste il vettore applicato nullo
<span class="math inline">\(\overrightarrow{AA}\)</span>.</p>
<p>Il vettori applicati si possono sommare tra di loro, purché il punto
finale del primo coincida con il punto iniziale del secondo, ovvero
purché siano della forma <span
class="math inline">\(\overrightarrow{AB}\)</span> e <span
class="math inline">\(\overrightarrow{AC}\)</span>.</p>
<p>Definiamo <span class="math inline">\(\overrightarrow{AB} +
\overrightarrow{BC} := \overrightarrow{AC}\)</span></p>
<figure>
<img src="img/somma_vettori.png" width="200"
alt="somma di due vettori" />
<figcaption aria-hidden="true">somma di due vettori</figcaption>
</figure>
<p><strong>Attenzione</strong>: se <span class="math inline">\(B \not
={C}\)</span>, allora non sappiamo come sommare <span
class="math inline">\(\overrightarrow{AB}\)</span> e <span
class="math inline">\(\overrightarrow{CD}\)</span></p>
<p><strong>Osservazione</strong><br />
<span class="math inline">\(\overrightarrow{AB} + \overrightarrow{BB} =
\overrightarrow{AB}\)</span> e <span
class="math inline">\(\overrightarrow{AA} + \overrightarrow{AB} =
\overrightarrow{AB}\)</span></p>
<p><strong>Proposizione</strong><br />
La somma di vettori applicati, quando è possibile eseguirla, soddisfa la
proprietà associativa.</p>
<p>Nei numeri reali, la proprietà associativa della somma dice che per
ogni <span class="math inline">\(a,b,c \in \mathbb{R}\)</span> vale
che</p>
<p><span class="math inline">\((a+b) + c = a + (b+c)\)</span></p>
<p>per questo motivo possiamo scrivere <span
class="math inline">\(a+b+c\)</span> senza ambiguità.</p>
<p><strong>Dimostrazione</strong><br />
Dobbiamo dimostrare che per ogni vettore applicato <span
class="math inline">\(\overrightarrow{AB}, \overrightarrow{BC},
\overrightarrow{CD}\)</span> vale che</p>
<p><span class="math inline">\((\overrightarrow{AB} +
\overrightarrow{BC}) + \overrightarrow{CD} = \overrightarrow{AB} +
(\overrightarrow{BC} + \overrightarrow{CD})\)</span></p>
<p>Ora, vale che</p>
<p><span class="math inline">\((\overrightarrow{AB} +
\overrightarrow{BC}) + \overrightarrow{CD} = \overrightarrow{AC} +
\overrightarrow{CD} = \overrightarrow{AD}\)</span><br />
<span class="math inline">\(\overrightarrow{AB} + (\overrightarrow{BC} +
\overrightarrow{CD}) = \overrightarrow{AB} + \overrightarrow{BD} =
\overrightarrow{AD}\)</span></p>
<figure>
<img src="img/proprietà_associativa_dei_vettori.png" width="401"
alt="proprietà associativa dei vettori" />
<figcaption aria-hidden="true">proprietà associativa dei
vettori</figcaption>
</figure>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Definizione</strong><br />
Dato un vettore applicato <span
class="math inline">\(\overrightarrow{AB}\)</span> e un numero reale
<span class="math inline">\(a \in \mathbb{R}\)</span>, otteniamo <span
class="math inline">\(a \cdot \overrightarrow{AB}\)</span> in questo
modo:</p>
<ul>
<li>se <span class="math inline">\(a=0,\ a \cdot \overrightarrow{AB} :=
\overrightarrow{AA}\)</span></li>
<li>se <span class="math inline">\(a&gt;0,\ a \cdot \overrightarrow{AB}
:=\)</span> vettore applicato in <span class="math inline">\(A\)</span>
con <em>stessa direzione e stesso verso</em> di <span
class="math inline">\(\overrightarrow{AB}\)</span> e modulo <span
class="math inline">\(a \cdot\)</span> [modulo di <span
class="math inline">\(\overrightarrow{AB}\)</span>] (il modulo di <span
class="math inline">\(\overrightarrow{AB}\)</span> si può indicare con
<span class="math inline">\(|\overrightarrow{AB}|\)</span> e quindi
possiamo scrivere “modulo uguale ad a <span class="math inline">\(\cdot
|\overrightarrow{AB}|\)</span>)</li>
<li>se <span class="math inline">\(a&lt;0, a \cdot \overrightarrow{AB}
:=\)</span> vettore applicato in <span class="math inline">\(A\)</span>
con stessa direzione e verso opposto di <span
class="math inline">\(\overrightarrow{AB}\)</span> e modulo uguale ad
<span class="math inline">\(|a| \cdot |\overrightarrow{AB}|\)</span>,
(ovvero in questo caso <span class="math inline">\((-a) \cdot
(|\overrightarrow{AB}|)\)</span>).</li>
</ul>
<p>Per ottenere una teoria “più comprensiva” introduciamo un nuovo
oggetto, i vettori liberi.</p>
<p><strong>Definizione</strong><br />
Due vettori applicati <span
class="math inline">\(\overrightarrow{AB}\)</span> e <span
class="math inline">\(\overrightarrow{CD}\)</span> si dicono
<strong>equipollenti</strong> se e solo se <span
class="math inline">\(\overrightarrow{AB}\)</span> e <span
class="math inline">\(\overrightarrow{CD}\)</span> hanno la medesima
direzione, il medesimo verso e il medesimo modulo (cambia il punto di
applicazione).</p>
<p>Si verifica che quella di equipollenza è una relazione di
equivalenza, ovvero essa è riflessiva, simmetrica e transitiva.</p>
<p><strong>Definizione</strong><br />
Dato un vettore applicato <span
class="math inline">\(\overrightarrow{AB}\)</span>, si definisce la sua
classe di equipollenza.</p>
<p><span class="math inline">\([\overrightarrow{AB}] := \{\text{vettori
applicati}\ \overrightarrow{CD}\ \text{tale che}\ \overrightarrow{AB}
\equiv \overrightarrow{CD}\}\)</span></p>
<p><strong>Proposizione</strong><br />
Dai risultati della geometria euclidea segue che dato un vettore
applicato <span class="math inline">\(\overrightarrow{AB}\)</span> e un
punto <span class="math inline">\(C\)</span>, allora esiste sempre un
vettore applicato <span
class="math inline">\(\overrightarrow{CD}\)</span> equipollente ad <span
class="math inline">\(\overrightarrow{AB}\)</span>;</p>
<p>da questo segue che data una classe di equipollenza denotata <span
class="math inline">\(\vec{V}\)</span>, e dato un punto <span
class="math inline">\(C\)</span> nel piano, esiste dunque un vettore
applicato che appartiene a <span class="math inline">\(\vec{V}\)</span>
e che ha punto iniziale <span class="math inline">\(C\)</span>.</p>
<p><strong>Osservazione</strong><br />
Se <span class="math inline">\(\overrightarrow{AB} \equiv
\overrightarrow{CD}\)</span> allora <span
class="math inline">\([\overrightarrow{AB}] =
[\overrightarrow{CD}]\)</span>; si dice che <span
class="math inline">\(\overrightarrow{AB}\)</span> e <span
class="math inline">\(\overrightarrow{CD}\)</span> sono dei
<strong>rappresentanti</strong> della medesima classe di
equipollenza.</p>
<p><strong>Definizione</strong><br />
Una classe di equipollenza <span class="math inline">\(\vec{V}\)</span>
di vettori applicati si dice <strong>vettore libero</strong>.</p>
<p><strong>Osservazione</strong><br />
Tutti i vettori applicati nulli sono equipollenti e dunque formano una
sola classe di equipollenza che denotiamo <span
class="math inline">\(\overrightarrow{O}\)</span>.</p>
<p><strong>Definizione</strong><br />
Dati due vettori liberi <span class="math inline">\(\vec{U}\)</span> e
<span class="math inline">\(\vec{V}\)</span> definiamo la loro somma
<span class="math inline">\(\vec{U} + \vec{V}\)</span> nella maniera
seguente:</p>
<ol type="1">
<li>scegliamo un rappresentante <span
class="math inline">\(\overrightarrow{AB}\)</span> per <span
class="math inline">\(\vec{U}\)</span>, ovvero <span
class="math inline">\(\vec{U} = [\overrightarrow{AB}]\)</span></li>
<li>per la proposizione che abbiamo enunciato prima, possiamo scegliere
un vettore applicato e un <span class="math inline">\(\vec{V}\)</span>
tale che il suo punto iniziale sia <span
class="math inline">\(B\)</span>, ovvero un vettore <span
class="math inline">\(\overrightarrow{BC} \in \vec{V}\)</span>, ovvero
<span class="math inline">\(\vec{V} =
[\overrightarrow{BC}]\)</span></li>
<li>definiamo <span class="math inline">\(\vec{U} + \vec{V} :=
[\overrightarrow{AB} + \overrightarrow{BC}]\
(=[\overrightarrow{AC}])\)</span></li>
</ol>
<p>Questa costruzione è indipendente dalla scelta del rappresentante di
<span class="math inline">\(\vec{U}\)</span>.</p>
<p>Se denotiamo con <span class="math inline">\(V_2\)</span> l’insieme
dei vettori liberi nel piano,<br />
la somma è una <em>funzione</em></p>
<p><span class="math inline">\(+ : V_2 \times V_2\)</span><br />
l’insieme delle coppie ordinate di elementi di <span
class="math inline">\(V_2\)</span></p>
<p><span class="math inline">\((\vec{U}, \vec{V}) \mapsto \vec{U} +
\vec{V}\)</span><br />
coppia ordinata</p>
<p>Se <span class="math inline">\(\lambda \in \mathbb{R}\)</span> e
<span class="math inline">\(\vec{V}\)</span> è un vettore libero,<br />
possiamo definire <span class="math inline">\(\lambda \cdot
\vec{V}\)</span></p>
<p><span class="math inline">\(\lambda \cdot \vec{V}: = [\lambda \cdot
\overrightarrow{AB}]\)</span></p>
<p>moltiplicazione per un numero di un vettore applicato (che sappiamo
già fare).</p>
<p>Questa definizione è <em>ben posta</em>, ovvero non dipende dal
rappresentante che abbiamo scelto.</p>
<p>La moltiplicazione per uno scalare è una funzione</p>
<p><span class="math inline">\(\mathbb{R} \times V_2 \rightarrow
V_2\)</span><br />
coppia ordinata</p>
<p><span class="math inline">\((\lambda, \vec{V}) \mapsto \lambda \cdot
\vec{V}\)</span></p>
<p>Definiamo il vettore libero nullo come</p>
<p><span class="math inline">\(\vec{O} =
\overrightarrow{AA}\)</span></p>
<p>Notiamo che</p>
<p><span class="math inline">\(\vec{O} + \vec{V} = [\overrightarrow{AA}]
+ [\overrightarrow{AB}] = [\overrightarrow{AA} + \overrightarrow{AB}] =
[\overrightarrow{AB}] = \vec{V}\)</span></p>
<p><span class="math inline">\(\vec{V} + \vec{O} = [\overrightarrow{AB}]
+ [\overrightarrow{BB}] = [\overrightarrow{AB + BB}] =
[\overrightarrow{BB}] = \vec{V}\)</span></p>
<p>Quindi <span class="math inline">\(\vec{O}\)</span> si comporta come
lo zero rispetto alla somma.</p>
<p><strong>Proprietà</strong> della somma tra vettori liberi:</p>
<ul>
<li>proprietà associativa
<ul>
<li><span class="math inline">\((\vec{U} + \vec{V}) + \vec{W} = \vec{U}
+ (\vec{V} + \vec{W})\)</span> per ogni <span
class="math inline">\(\vec{U}, \vec{V}, \vec{W}\)</span></li>
</ul></li>
<li>proprietà commutativa
<ul>
<li><span class="math inline">\(\vec{U} + \vec{V} = \vec{V} +
\vec{U}\)</span> per ogni <span class="math inline">\(\vec{U},
\vec{V}\)</span></li>
</ul></li>
<li>esistenza dell’elemento neutro
<ul>
<li>per ogni <span class="math inline">\(\vec{V}\)</span> vale <span
class="math inline">\(\vec{O} + \vec{V} = \vec{V} + \vec{O} =
\vec{V}\)</span></li>
</ul></li>
<li>esistenza dell’elemento opposto
<ul>
<li>per ogni <span class="math inline">\(\vec{V}\)</span> esiste un
<span class="math inline">\(\vec{W}\)</span> tale che <span
class="math inline">\(\vec{V} + \vec{W} = \vec{W} + \vec{V} =
\vec{O}\)</span>, con <span class="math inline">\(\vec{W} = -
\vec{V}\)</span></li>
</ul></li>
</ul>
<p>Se <span class="math inline">\(\vec{v} =
[\overrightarrow{AB}]\)</span> allora <span class="math inline">\(-
\vec{v} = [\overrightarrow{BA}]\)</span></p>
<p><strong>Proprietà</strong> della moltiplicazione per uno scalare:</p>
<p>per ogni <span class="math inline">\(\vec{V}\)</span><br />
<span class="math inline">\(1 \cdot \vec{V} = \vec{V}\)</span></p>
<p>per ogni <span class="math inline">\(\vec{V}\)</span><br />
<span class="math inline">\((-1) \cdot \vec{V} = - \vec{V}\)</span></p>
<p>per ogni <span class="math inline">\(\lambda, \mu \in
\mathbb{R}\)</span>, per ogni <span
class="math inline">\(\vec{V}\)</span><br />
<span class="math inline">\((\lambda \mu) \cdot \vec{V} = \lambda (\mu
\cdot \vec{V})\)</span></p>
<p>per ogni <span class="math inline">\(\lambda, \mu \in
\mathbb{R}\)</span>, per ogni <span class="math inline">\(\vec{V},
\vec{U}\)</span><br />
<span class="math inline">\((\lambda + \mu) \cdot \vec{V} = \lambda
\cdot \vec{V} + \mu \cdot \vec{V}\)</span><br />
<span class="math inline">\(\lambda \cdot (\vec{V} + \vec{U}) = \lambda
\cdot \vec{U} + \lambda \cdot \vec{V}\)</span></p>
<p><strong>Definizione</strong>:<br />
Uno spazio vettoriale è un insieme <span
class="math inline">\(V\)</span><br />
con due operazioni:</p>
<p><span class="math inline">\(+ : V \times V \rightarrow
V\)</span><br />
<span class="math inline">\((\mu, v) \mapsto \mu + v\)</span></p>
<p><span class="math inline">\(\cdot : \mathbb{R } \times V \rightarrow
V\)</span><br />
<span class="math inline">\((\mu, V) \mapsto \mu \cdot V\)</span></p>
<p>tali per cui per ogni <span class="math inline">\(\lambda, \mu \in
\mathbb{R}\)</span> e per ogni <span class="math inline">\(u, v, w \in
V\)</span> siano soddisfatte le proprietà:</p>
<ul>
<li><span class="math inline">\(V1\)</span>. proprietà associativa</li>
<li><span class="math inline">\(V2\)</span>. proprietà commutativa</li>
<li><span class="math inline">\(V3\)</span>. esistenza del vettore
nullo</li>
<li><span class="math inline">\(V4\)</span>. esistenza del vettore
opposto</li>
<li><span class="math inline">\(V5\)</span>. proprietà distributiva di
<span class="math inline">\(+\)</span> rispetto a R</li>
<li><span class="math inline">\(V6\)</span>. proprietà distributiva di
<span class="math inline">\(\cdot\)</span> rispetto a R</li>
<li><span class="math inline">\(V7\)</span>. <span
class="math inline">\((\lambda \mu) \cdot \vec{V} = \lambda (\mu \cdot
\vec{V})\)</span></li>
<li><span class="math inline">\(V8\)</span>. <span
class="math inline">\(1 \cdot v = v\)</span></li>
</ul>
<p><strong>Proposizione</strong><br />
Ciò che abbiamo visto finora ci mostra che <span
class="math inline">\(V_2\)</span> (insieme dei vettori liberi nel
piano) è un <span class="math inline">\(\mathbb{R}\)</span>-spazio
vettoriale.</p>
<p><strong>Notazione</strong><br />
Un <span class="math inline">\(\mathbb{R}\)</span>-spazio vettoriale si
dice anche uno spazio vettoriale di <span
class="math inline">\(\mathbb{R}\)</span>.</p>
<p>Esempio:<br />
consideriamo <span class="math inline">\(V = \mathbb{R}\)</span> con
l’usuale somma o moltiplicazione<br />
allora si verifica che <span class="math inline">\(\mathbb{R}\)</span> è
un <span class="math inline">\(\mathbb{R}\)</span>-spazio
vettoriale.</p>
<p>Esempio:<br />
consideriamo <span class="math inline">\(V = \mathbb{R} \times
\mathbb{R}\)</span> ovvero<br />
<span class="math inline">\(V = \{(a,b) : a,b \in
\mathbb{R}\}\)</span><br />
<span class="math inline">\(V\)</span> si denota anche <span
class="math inline">\(\mathbb{R}^2\)</span><br />
con le operazioni<br />
<span class="math inline">\((a,b) + (c,d) = (a + c,\ c+d)\)</span> <span
class="math inline">\(\lambda (a,b) = (\lambda a, \lambda
b)\)</span></p>
<p><span class="math inline">\((V,+,\cdot)\)</span> è un <span
class="math inline">\(\mathbb{R}\)</span>-spazio vettoriale</p>
<p>Esempio:<br />
<span class="math inline">\(V = \mathbb{R} \times \dots \times
\mathbb{R} = \mathbb{R}^n\)</span><br />
<span class="math inline">\(V =\)</span> l’insieme delle n-uple ordinate
di numeri reali. <span class="math inline">\(V = \{(a_1, \dots, a_n) :
a_1, \dots, a_n \in \mathbb{R}\}\)</span></p>
<p>Con le operazioni</p>
<p><span class="math inline">\(+ : \mathbb{R}^n = \mathbb{R}^n
\rightarrow \mathbb{R^n}\)</span><br />
<span class="math inline">\((a_1, a_2), (b_1, b_2) \mapsto (a_1 + b_1,
a_2 + b_2)\)</span></p>
<p><span class="math inline">\(\cdot : \mathbb{R} = \mathbb{R}^n
\rightarrow \mathbb{R^n}\)</span><br />
<span class="math inline">\(\lambda, (a_1, a_2) \mapsto (\lambda a_1,
\lambda a_2)\)</span></p>
<p>Esempio:<br />
Consideriamo <span class="math inline">\(V = \{ \text{funzioni}\ f :
\mathbb{R} \rightarrow \mathbb{R}\}\)</span><br />
con le operazioni<br />
<span class="math inline">\(+ : V \times V \rightarrow V\)</span><br />
<span class="math inline">\((f,g) \mapsto (f+g)\)</span></p>
<p>dove <span class="math inline">\(f+g\)</span> è la funzione <span
class="math inline">\(\mathbb{R} \rightarrow \mathbb{R}\)</span> data
da:<br />
se <span class="math inline">\(a \in \mathbb{R}, (f+g)(a) := f(a) +
g(a)\)</span><br />
e<br />
<span class="math inline">\(\mathbb{R} \times V \rightarrow
V\)</span><br />
<span class="math inline">\((\lambda,f) \mapsto \lambda f\)</span></p>
<p>dove <span class="math inline">\(\lambda f\)</span> è la funzione
<span class="math inline">\(\mathbb{R} \rightarrow \mathbb{R}\)</span>
data da:</p>
<p>se <span class="math inline">\(a \in \mathbb{R}, (\lambda f)(a) =
\lambda (f(a)) \in \mathbb{R}\)</span> (moltiplicazione in <span
class="math inline">\(\mathbb{R}\)</span>).</p>
<p><strong>Notazione</strong><br />
Sia <span class="math inline">\(V\)</span> in un <span
class="math inline">\(\mathbb{R}\)</span>-spazio vettoriale; gli
elementi di <span class="math inline">\(V\)</span> si dicono
<em>vettori</em>.</p>
<p><strong>Osservazione</strong><br />
Sia <span class="math inline">\((V,+,\cdot)\)</span> è l’insieme delle
funzioni <span class="math inline">\(f : \mathbb{R} \rightarrow
\mathbb{R}\)</span>, allora il ruolo del vettore nullo è quello giocato
dalla funzione</p>
<p><span class="math inline">\(F : \mathbb{R} \rightarrow
\mathbb{R}\)</span><br />
<span class="math inline">\(x \mapsto 0\)</span></p>
<p>infatti, se <span class="math inline">\(g : \mathbb{R} \rightarrow
\mathbb{R}\)</span> è una funzione</p>
<p><span class="math inline">\((g + F) : \mathbb{R} \rightarrow
\mathbb{R}\)</span><br />
<span class="math inline">\(x \mapsto g(x) + F(x) = g(x) + 0 =
g(x)\)</span></p>
<p>abbiamo visto che <span class="math inline">\(\forall x \in
\mathbb{R}\)</span>, <span class="math inline">\((g + F)(x) =
g(x)\)</span>, pertanto <span class="math inline">\(g + F = g\)</span>;
analogamente <span class="math inline">\(F + g = g\)</span>, quindi
<span class="math inline">\(F\)</span> è l’elemento neutro.</p>
<p><strong>Proposizione</strong><br />
Se <span class="math inline">\(V\)</span> è un <span
class="math inline">\(\mathbb{R}\)</span>-spazio vettoriale, allora
l’elemento neutro è unico.</p>
<p><strong>Dimostrazione</strong><br />
Supponiamo che esistano due elementi neutri, che indichiamo con <span
class="math inline">\(0\)</span> è <span
class="math inline">\(0&#39;\)</span>:<br />
mostreremo che deve valere <span class="math inline">\(0 =
0&#39;\)</span> e quindi da questo seguirà la tesi.</p>
<p>Per ipotesi, abbiamo che</p>
<p><span class="math inline">\(\forall v \in V, 0 + v = v + 0 =
v\)</span> (@)<br />
<span class="math inline">\(\forall v \in V, 0&#39; + v = v + 0&#39; =
v\)</span> (#)</p>
<p>in (@) scegliamo <span class="math inline">\(v = 0&#39;\)</span>;
allora</p>
<p><span class="math inline">\(0 + 0&#39; = 0\)</span></p>
<p>quindi <span class="math inline">\(0 = 0 + 0&#39; = 0&#39;\)</span>,
pertanto <span class="math inline">\(0 = 0&#39;\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(V\)</span> un <span
class="math inline">\(\mathbb{R}\)</span>-spazio vettoriale, allora
<span class="math inline">\(\forall v \in V\)</span>, vale che</p>
<p><span class="math inline">\((-1) \cdot v = -v\)</span></p>
<p><strong>Dimostrazione</strong><br />
Per dimostrare la tesi, mostriamo che <span
class="math inline">\(\forall v \in V\)</span>, l’elemento <span
class="math inline">\(-1 \cdot v\)</span> soddisfa la proprietà di
essere opposto di <span class="math inline">\(v\)</span>, ovvero:</p>
<p><span class="math inline">\(v + (-1) \cdot v = (-1) \cdot v + v =
0\)</span></p>
<p>la prima uguaglianza segue dalla commutatività della somma;
dimostriamo la seconda uguaglianza</p>
<p><span class="math inline">\(v + (-1) \cdot v \stackrel{V8}{=} (-1)
\cdot v + v \stackrel{V6}{=} (-1 +1) \cdot v = 0 \cdot v =
0\)</span></p>
<p>Se sapessimo che <span class="math inline">\(0 \cdot v = 0\)</span>
(vettore nullo), allora avremmo concluso la dimostrazione,<br />
però questo non ci è dato a sapere, stanti semplicemente le 8 proprietà
definitasi degli spazi vettoriali; dimostriamo qui di seguito che questa
ulteriore proprietà segue dalle 8 proprietà.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale; allora
<span class="math inline">\(\forall v \in V\)</span>, vale che</p>
<p><span class="math inline">\(0 \cdot v = 0\)</span></p>
<p><strong>Dimostrazione</strong><br />
Sia <span class="math inline">\(v \in V\)</span>; abbiamo che</p>
<p><span class="math inline">\(0 \cdot v = (0 + 0) \cdot v
\stackrel{V6}{=} 0 \cdot v + 0 \cdot v\)</span></p>
<p>quindi</p>
<p><span class="math inline">\(0 \cdot v = 0 \cdot v + 0 \cdot
v\)</span></p>
<p>ora sommiamo ad entrambi i membri dell’uguaglianza l’opposto di <span
class="math inline">\(0 \cdot v\)</span></p>
<p><span class="math inline">\(-(0 \cdot v) + 0 \cdot v = -(0 \cdot v) +
0 \cdot v + 0 \cdot v\)</span></p>
<p>per la proprietà <span class="math inline">\(V4\)</span>, individuo i
vettori nulli; pertanto</p>
<p><span class="math inline">\(0 = 0 + 0 \cdot v\)</span><br />
<span class="math inline">\(0 = 0 \cdot v\)</span></p>
<p>che è quindi il vettore nullo.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Dimostrazione</strong><br />
Per esercizio. Il vettore opposto è unico.</p>
<p>Consideriamo ora <span class="math inline">\(\mathbb{R}^2\)</span>
con <span class="math inline">\(+\)</span> e <span
class="math inline">\(\cdot\)</span> introdotti in precedenza (“<em>le
operazioni componente per componente</em>”). Abbiamo visto che <span
class="math inline">\(\mathbb{R}^2\)</span> è un <span
class="math inline">\(\mathbb{R}\)</span>-spazio vettoriale. Ora
consideriamo il seguente sottoinsieme <span class="math inline">\(W
\subseteq \mathbb{R}^2\)</span></p>
<p><span class="math inline">\(W := \{(x,y) \in \mathbb{R}^2 : x - 3y =
0\}\)</span></p>
<p>In <span class="math inline">\(\mathbb{R}^2\)</span> esiste il
vettore nullo <span class="math inline">\((0,0)\)</span>. Vale che <span
class="math inline">\((0,0) \in W\)</span>.</p>
<p>In <span class="math inline">\(\mathbb{R}^2\)</span> è definita una
somma. Se <span class="math inline">\(v\)</span> e <span
class="math inline">\(w\)</span> sono elementi di <span
class="math inline">\(W\)</span>, allora in particolare sono entrambi di
<span class="math inline">\(\mathbb{R}^2\)</span> e quindi li posso
sommare, dunque <span class="math inline">\(v+w \in
\mathbb{R}^2\)</span>.</p>
<p>In aggiunta vale che <span class="math inline">\(v+w \in W\)</span>.
Infatti</p>
<p>se <span class="math inline">\(v = (v_1, v_2)\)</span> e <span
class="math inline">\(w = (w_1,w_2)\)</span>, allora</p>
<p><span class="math inline">\(v \in W \Rightarrow v_1 - 3 v_2 =
0\)</span><br />
<span class="math inline">\(w \in W \Rightarrow w_1 - 3 w_2 =
0\)</span></p>
<p>allora <span class="math inline">\((v_1 - 3 v_2) + (w_1 - 3 w_2) = 0
+ 0 = 0\)</span></p>
<p>ovvero per esempio <span class="math inline">\((v_1 + w_1) + 3(v_2 +
w_2) = 0\)</span></p>
<p>ovvero <span class="math inline">\(v + w \in W\)</span></p>
<p>Infine consideriamo <span class="math inline">\(v \in W\)</span> e
<span class="math inline">\(\lambda \in \mathbb{R}\)</span>; vediamo che
<span class="math inline">\(\lambda \cdot v \in W\)</span>. Infatti se
<span class="math inline">\(v = (v_1,v_2)\)</span>, allora <span
class="math inline">\(\lambda \cdot v = (\lambda v_1, \lambda
v_2)\)</span></p>
<p><span class="math inline">\(v \in W \Rightarrow v_1 - 3 v_2 =
0\)</span></p>
<p>allora <span class="math inline">\(\lambda \cdot (v_1 - 3 v_2) =
\lambda \cdot 0 = 0\)</span>, quindi <span
class="math inline">\((\lambda v_1) - 3(\lambda v_2) = 0\)</span>,
ovvero <span class="math inline">\(\lambda, v \in W\)</span>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(V\)</span> un <span
class="math inline">\(\mathbb{R}\)</span>-spazio vettoriale, un
sottoinsieme <span class="math inline">\(W \subseteq V\)</span> si dice
un <strong>sottospazio vettoriale</strong> di <span
class="math inline">\(V\)</span> se valgono:</p>
<ol type="1">
<li>il vettore nullo di <span class="math inline">\(V\)</span>
appartiene a <span class="math inline">\(W\)</span> (stringatamente
<span class="math inline">\(0 \in W\)</span>)</li>
<li><span class="math inline">\(\forall v, w \in W,\)</span> vale che
<span class="math inline">\(v + w \in W\)</span> (<strong>chiusura
rispetto alla somma</strong>)</li>
<li><span class="math inline">\(\forall \lambda \in \mathbb{R}\)</span>,
<span class="math inline">\(\forall v \in W\)</span>, vale che <span
class="math inline">\(\lambda \cdot v \in W\)</span> (<strong>chiusura
rispetto alla moltiplicazione per uno scalare</strong>)</li>
</ol>
<p><strong>Esempio</strong><br />
<span class="math inline">\(\mathbb{R}^2 \iff\)</span> {punti del
piano}</p>
<p><span class="math inline">\(W = \{(x,y) \in \mathbb{R}^2 : x - 3y =
0\}\)</span></p>
<p>(è una retta nel piano cartesiano)</p>
<p><strong>Esempio</strong><br />
<span class="math inline">\(\mathbb{R}^2 \iff\)</span> {punti del
piano}</p>
<p><span class="math inline">\(C = \{(x,y) \in \mathbb{R}^2 : x^2 + y^2
= 1\}\)</span></p>
<p>(è una circonferenza nel piano cartesiano)</p>
<p>Non è un sottospazio vettoriale di <span
class="math inline">\(\mathbb{R}^2\)</span>, perché <span
class="math inline">\((0,0) \notin W\)</span></p>
<h1 id="matrici">Matrici</h1>
<p><strong>Definizione</strong><br />
Siano <span class="math inline">\(m,n \in \mathbb{N}\)</span>; una
matrice <span class="math inline">\(m \times n\)</span> a coefficienti
reali è una tabella rettangolare di <span class="math inline">\(m \cdot
n\)</span> numeri del tipo</p>
<p><span class="math display">\[
A = \begin{pmatrix}
  a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
  a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
  a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
\end{pmatrix}
\]</span></p>
<p>dove <span class="math inline">\(a_{ij}\)</span> è un numero reale,
ovvero</p>
<p><span class="math inline">\(a_{ij} \in \mathbb{R}\)</span>, <span
class="math inline">\(\forall i \in \{1, 2, \dots, m\}\)</span> e <span
class="math inline">\(\forall j \in \{1, 2, \dots, n\}\)</span></p>
<p><strong>Esempio</strong> Sia <span class="math inline">\(A\)</span>
una matrice <span class="math inline">\(3 \times 4\)</span> a
coefficienti reali</p>
<p><span class="math display">\[
A = \begin{pmatrix}
  1 &amp; 2 &amp; 3 &amp; 4\\
  5 &amp; 6 &amp; 7 &amp; 8\\
  9 &amp; 10 &amp; 11 &amp; 12\\
\end{pmatrix}
\]</span></p>
<p><span class="math inline">\(a_{1,1} = 1\)</span><br />
<span class="math inline">\(a_{1,2} = 2\)</span><br />
<span class="math inline">\(a_{1,3} = 3\)</span><br />
<span class="math inline">\(a_{1,4} = 4\)</span><br />
<span class="math inline">\(a_{2,1} = 5\)</span><br />
<span class="math inline">\(a_{2,2} = 6\)</span><br />
<span class="math inline">\(a_{2,3} = 7\)</span><br />
<span class="math inline">\(a_{2,4} = 8\)</span><br />
<span class="math inline">\(a_{3,1} = 9\)</span><br />
<span class="math inline">\(a_{3,2} = 10\)</span><br />
<span class="math inline">\(a_{3,3} = 11\)</span><br />
<span class="math inline">\(a_{3,4} = 12\)</span></p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A = (a_{ij})\)</span> una matrice a
coefficienti reali</p>
<p>per ogni <span class="math inline">\(i \in \{1, 2, \dots,
m\}\)</span> la <span class="math inline">\(i\)</span>-esima riga è la
matrice</p>
<p><span class="math inline">\(A_{(i)} = (a_{i1}, a_{i2}, \dots,
a_{in})\)</span></p>
<p>per ogni <span class="math inline">\(j \in \{1, 2, \dots,
n\}\)</span> la <span class="math inline">\(j\)</span>-esima colonna la
matrice</p>
<p><span class="math display">\[
A^{(j)} =
\begin{pmatrix}
  \alpha_{1j} \\
  \alpha_{2j} \\
  \vdots \\
  \alpha_{mj} \\
\end{pmatrix}
\]</span></p>
<p><strong>Esempio</strong><br />
<span class="math display">\[
A =
\begin{pmatrix}
   1 &amp; 3 \\
  -2 &amp; 5
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
A_{(1)} = \begin{pmatrix}
  1 &amp; 3\\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
A_{(2)} = \begin{pmatrix}
  -2 &amp; 5\\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
A^{(1)} =
\begin{pmatrix}
  1 \\
  -2 \\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
A^{(2)} =
\begin{pmatrix}
  3 \\
  5 \\
\end{pmatrix}
\]</span></p>
<p><strong>Definizione</strong><br />
Dati <span class="math inline">\(m,n \in \mathbb{N},\ m&gt;0,\
n&gt;0\)</span>, l’insieme delle matrici <span class="math inline">\(m
\times n\)</span> è denotato con <span
class="math inline">\(M_{m,n}(\mathbb{R})\)</span>; l’insieme delle
matrici quadrate è denotato <span class="math inline">\(M_n
(\mathbb{R})\)</span>.</p>
<p><strong>Definizione</strong><br />
La matrice <span class="math inline">\(m \times n\)</span> nulla è la
matrice <span class="math inline">\(m \times n\)</span> le cui entrate
sono tutte zero.</p>
<p><strong>Esempio</strong><br />
<span class="math display">\[
\begin{pmatrix}
   0 &amp; 0 &amp; 0\\
   0 &amp; 0 &amp; 0\\
  0 &amp; 0 &amp; 0
\end{pmatrix}
\]</span></p>
<p>Introduciamo delle operazioni tra matrici al fine di rendere <span
class="math inline">\(M_{m,n}(\mathbb{R})\)</span> un <span
class="math inline">\(\mathbb{R}\)</span>-spazio vettoriale.</p>
<p><strong>Definizione</strong><br />
Siano <span class="math inline">\(m,n \in \mathbb{N},\ m&gt;0,\
n&gt;0\)</span> e siano <span class="math inline">\(A, B \in
M_{m,n}(\mathbb{R})\)</span>, definiamo la somma di <span
class="math inline">\(A\)</span> e <span
class="math inline">\(B\)</span>, che denotiamo <span
class="math inline">\(A + B\)</span>, è la matrice definita nel modo
seguente: l’entrata di posto <span class="math inline">\(i,j\)</span> di
<span class="math inline">\(A + B\)</span> è data da:</p>
<p><span class="math inline">\((A + B)_{i,j} := A_{i,j} +
B_{i,j}\)</span><br />
con tutti i membri <span class="math inline">\(\in
\mathbb{R}\)</span></p>
<p>(qui utilizziamo il fatto che per descrivere una matrice è
sufficiente determinare come ottenere ciascuna delle sue entrate)</p>
<p><strong>Esempio</strong><br />
<span class="math display">\[
A =
\begin{pmatrix}
   1 &amp; 2 \\
   3 &amp; 4
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
B =
\begin{pmatrix}
   5 &amp; 6 \\
   7 &amp; 8
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
A + B =
\begin{pmatrix}
   6 &amp; 8 \\
   10 &amp; 12
\end{pmatrix}
\]</span></p>
<p><strong>Osservazione</strong><br />
La matrice nulla è l’elemento neutro della somma tra matrici.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_{m,n} (\mathbb{R})\)</span> e
sia <span class="math inline">\(\lambda \in \mathbb{R}\)</span>;
definiamo la moltiplicazione per uno scalare di <span
class="math inline">\(\lambda\)</span> per <span
class="math inline">\(A\)</span>, che denotiamo <span
class="math inline">\(\lambda \cdot A\)</span>, come la matrice:</p>
<p><span class="math inline">\((\lambda \cdot A)_{i,j} := \lambda \cdot
A_{i,j}\)</span></p>
<p><strong>Proposizione</strong><br />
L’insieme <span class="math inline">\(M_{m,n} (\mathbb{R})\)</span> con
le operazioni di somma e moltiplicazione per uno scalare definite sopra
è un <span class="math inline">\(\mathbb{R}\)</span>-spazio
vettoriale.</p>
<p><strong>Dimostrazione</strong><br />
Per esercizio.</p>
<p><strong>Esempio</strong><br />
Sia <span class="math inline">\(\lambda = 3\)</span>, e sia</p>
<p><span class="math display">\[
A =
\begin{pmatrix}
  1 &amp; 2 &amp; 3\\
  -3 &amp; -2 &amp; -1\\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
3 \cdot A =
\begin{pmatrix}
  3 &amp; 6 &amp; 9\\
  -9 &amp; -6 &amp; -3\\
\end{pmatrix}
\]</span></p>
<p>Consideriamo una matrice <span class="math inline">\(2 \times
2\)</span>:</p>
<p><span class="math display">\[
A=
\begin{pmatrix}
   3 &amp; 1 \\
   -2 &amp; 4
\end{pmatrix}
\]</span></p>
<p>Ora consideriamo quattro matrici particolari:</p>
<p><span class="math display">\[
E :=
\begin{pmatrix}
   1 &amp; 0 \\
   0 &amp; 0
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
F :=
\begin{pmatrix}
   0 &amp; 1 \\
   0 &amp; 0
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
G :=
\begin{pmatrix}
   0 &amp; 0 \\
   1 &amp; 0
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
H :=
\begin{pmatrix}
   0 &amp; 0 \\
   0 &amp; 1
\end{pmatrix}
\]</span></p>
<p>Considero ora la seguente <strong>combinazione lineare</strong> di
queste quattro matrici:</p>
<p><span class="math display">\[
3F + F - 2G + 4H =
\begin{pmatrix}
   3 &amp; 1 \\
   -2 &amp; 4
\end{pmatrix}
= A
\]</span></p>
<p>Questa costruzione si può ripetere qualsiasi sia la matrice <span
class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
A = a_{11} \cdot E + a_{12} \cdot F + a_{21} \cdot G + a_{22} \cdot H
\]</span></p>
<p>Le quattro matrici <span class="math inline">\(E,F,G,H\)</span> sono
tali che ogni matrice si può scrivere come combinazione lineare di
queste quattro matrici con opportuni coefficienti.</p>
<p>Diciamo che <span class="math inline">\(E,F,G,H\)</span> sono un
<strong>sistema di generatori</strong> di <span
class="math inline">\(M_{2,2} (\mathbb{R})\)</span>.</p>
<p>Notiamo che questo ragionamento può essere formulato allo stesso modo
per qualsiasi insieme di matrici <span class="math inline">\(M_{m.n}
(\mathbb{R})\)</span>. Abbiamo quindi mostrato che:</p>
<p><strong>Proposizione</strong><br />
Consideriamo in <span class="math inline">\(M_{m,n}
(\mathbb{R})\)</span> l’insieme delle <span class="math inline">\(m
\cdot n\)</span> matrici costruite nel seguente modo: esse hanno tutte
le entrate nulle fuorché una, la quale è uguale a 1; allora tale insieme
è un <em>sistema di generatori</em> per <span
class="math inline">\(M_{m,n} (\mathbb{R})\)</span>.</p>
<p>Ritorniamo alla situazione delle matrici <span
class="math inline">\(2 \times 2\)</span>. La matrice nulla si può
scrivere come combinazione lineare delle quattro matrici <span
class="math inline">\(E, F, G, H\)</span>.</p>
<p><span class="math display">\[
0 \cdot E +
0 \cdot F +
0 \cdot G +
0 \cdot H =
\begin{pmatrix}
   0 &amp; 0 \\
   0 &amp; 0
\end{pmatrix}
\]</span></p>
<p>Mi chiedo: esiste un’altra combinazione lineare di <span
class="math inline">\(E, F, G, H\)</span> che restituisca la matrice
nulla? Ovvero esistono costituenti, <span class="math inline">\(e, f, g,
h \in \mathbb{R}\)</span> tali che non tutti gli <span
class="math inline">\(e, f, g, h\)</span> sono nulli e vale:</p>
<p><span class="math display">\[
\begin{pmatrix}
   0 &amp; 0 \\
   0 &amp; 0
\end{pmatrix}
= e \cdot E + f \cdot F + g \cdot G + h \cdot H
\]</span></p>
<p>Riusciamo a capire quali condizioni dobbiamo imporre a <span
class="math inline">\(e, f, g, h\)</span> affinché la precedente
uguaglianza sia vera?</p>
<p><span class="math display">\[
e \cdot E + f \cdot F + g \cdot G + h \cdot H =
\begin{pmatrix}
  e &amp; 0 \\
  0 &amp; 0
\end{pmatrix} +
\begin{pmatrix}
  0 &amp; f \\
  0 &amp; 0
\end{pmatrix} +
\begin{pmatrix}
  0 &amp; 0 \\
  g &amp; 0
\end{pmatrix} +
\begin{pmatrix}
  0 &amp; 0 \\
  0 &amp; h
\end{pmatrix} =
\begin{pmatrix}
  e &amp; f \\
  g &amp; h
\end{pmatrix}
\]</span></p>
<p>Quindi, affinché valga</p>
<p><span class="math display">\[
\begin{pmatrix}
   0 &amp; 0 \\
   0 &amp; 0
\end{pmatrix}
= e \cdot E + f \cdot F + g \cdot G + h \cdot H
\]</span></p>
<p>deve valere</p>
<p><span class="math display">\[
\begin{pmatrix}
   0 &amp; 0 \\
   0 &amp; 0
\end{pmatrix} =
\begin{pmatrix}
   e &amp; f \\
   g &amp; h
\end{pmatrix}
\]</span></p>
<p>quindi se e solo se</p>
<p><span class="math inline">\(e = 0,\ f = 0,\ g = 0,\ h =
0\)</span></p>
<p>pertanto, l’unico modo di ottenere la matrice nulla come combinazione
lineare delle matrici <span class="math inline">\(E, F, G, H\)</span>, è
prendere tutti e quattro i coefficienti nulli.</p>
<p>In questo caso diciamo che le quattro matrici sono
<strong>linearmente indipendenti</strong>.</p>
<p>Notiamo che tutti questi ragionamenti possono essere formulati anche
per matrici <span class="math inline">\(m \times n\)</span>.</p>
<p><strong>Osservazione</strong><br />
Se prendo</p>
<p><span class="math display">\[
A =
\begin{pmatrix}
   1 &amp; 1 \\
   1 &amp; 1
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
B =
\begin{pmatrix}
   2 &amp; 2 \\
   2 &amp; 2
\end{pmatrix}
\]</span></p>
<p>allora</p>
<p><span class="math display">\[
1 \cdot A + \Big(-\frac{1}{2} \Big) \cdot B =
\begin{pmatrix}
   0 &amp; 0 \\
   0 &amp; 0
\end{pmatrix}
\]</span></p>
<p>abbiamo quindi ottenuto la matrice nulla come combinazione lineare
delle matrici <span class="math inline">\(A\)</span> e <span
class="math inline">\(B\)</span>, cui coefficienti non sono tutti nulli,
pertanto le matrici <span class="math inline">\(A\)</span> e <span
class="math inline">\(B\)</span> non sono linearmente indipendenti.</p>
<p>Ritorniamo alla situazione delle matrici <span
class="math inline">\(2 \times 2\)</span> e consideriamo il seguente
insieme</p>
<p><span class="math display">\[
T_{2,2} (\mathbb{R}) := \{A \in M_{2,2} \in (\mathbb{R}) : a_{21} = 0\}
= \left\lbrace
  \begin{pmatrix}
    a_{11} &amp; a_{12} \\
    0 &amp; a_{22}
  \end{pmatrix}
  \in M_{2,2}(\mathbb{R})
  : a_{11}, a_{12}, a_{22} \in \mathbb{R}
\right\rbrace
\]</span></p>
<p>Dunque <span class="math inline">\(\begin{pmatrix}  1 &amp; 2 \\  0
&amp; 1 \end{pmatrix} \in T_{2,2} (\mathbb{R})\)</span> , ma <span
class="math inline">\(\begin{pmatrix}  0 &amp; 0 \\  1 &amp; 1
\end{pmatrix} \notin T_{2,2} (\mathbb{R})\)</span>.</p>
<p>L’insieme <span class="math inline">\(T_{2,2} (\mathbb{R})\)</span> è
l’insieme delle matrici <span class="math inline">\(2 \times 2\)</span>
a coefficienti in <span class="math inline">\(\mathbb{R}\)</span>
triangolari superiori. Notiamo che <span class="math inline">\(T_{2,2}
(\mathbb{R}) \subseteq M_{2,2} (\mathbb{R})\)</span>.</p>
<p>Vale che:</p>
<p>1: <span class="math inline">\(\begin{pmatrix}  0 &amp; 0 \\  0 &amp;
0 \end{pmatrix} \in T_{2,2} (\mathbb{R})\)</span></p>
<p>2: Se <span class="math inline">\(A,B \in T_{2,2}
(\mathbb{R})\)</span>, allora</p>
<p><span class="math display">\[A =
\begin{pmatrix}
   a_{11} &amp; a_{12} \\
   0 &amp; a_{22}
\end{pmatrix},\ B =
\begin{pmatrix}
   b_{11} &amp; b_{12} \\
   0 &amp; b_{22}
\end{pmatrix}
\]</span></p>
<p>quindi</p>
<p><span class="math display">\[A + B =
\begin{pmatrix}
   a_{11} + b_{11} &amp; a_{12} + b_{12} \\
   0 &amp; a_{22} + b_{22}
\end{pmatrix}
\]</span></p>
<p>pertanto <span class="math inline">\(A + B \in T_{2,2}
(\mathbb{R})\)</span>.</p>
<p>3: Se <span class="math inline">\(A \in T_{2,2} (\mathbb{R})\)</span>
e <span class="math inline">\(\lambda \in \mathbb{R}\)</span>,
allora</p>
<p><span class="math display">\[A =
\begin{pmatrix}
   a_{11} &amp; a_{12} \\
   0 &amp; a_{22}
\end{pmatrix}
\]</span></p>
<p>quindi</p>
<p><span class="math display">\[\lambda \cdot A =
\begin{pmatrix}
   \lambda \cdot a_{11} &amp; \lambda \cdot a_{12} \\
   0 &amp; \lambda \cdot a_{22}
\end{pmatrix}
\]</span></p>
<p>pertanto <span class="math inline">\(\lambda \cdot A \in T_{2,2}
(\mathbb{R})\)</span>.</p>
<p>Abbiamo verificato quindi che <span class="math inline">\(T_{2,2}
(\mathbb{R})\)</span> è un sottoinsieme vettoriale di <span
class="math inline">\(M_{2,2} (\mathbb{R})\)</span>.</p>
<p>Notiamo che l’analogo di <span class="math inline">\(T_{2,2}
(\mathbb{R})\)</span> per matrici <span class="math inline">\(3 \times
3\)</span> è</p>
<p><span class="math display">\[
T_{3,3} (\mathbb{R}) =
\left\lbrace
  \begin{pmatrix}
    a_{11} &amp; a_{12} &amp; a_{13} \\
    0 &amp; a_{22} &amp; a_{23} \\
    0 &amp; 0 &amp; a_{33}
  \end{pmatrix},
  \forall i, \forall j, a_{ij} \in \mathbb{R}
\right\rbrace
\]</span></p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_{n} (\mathbb{R})\)</span>;
allora la <strong>diagonale principale</strong> è la parte di <span
class="math inline">\(A\)</span> data dalle entrate di posto <span
class="math inline">\(i,i\)</span> per <span class="math inline">\(i \in
\{1,2, \dots, n\}\)</span>.</p>
<p><strong>Definizione</strong> Siano <span class="math inline">\(A,B
\in M_{m,n} (\mathbb{R})\)</span>, definiamo la
<strong>trasposta</strong> di <span class="math inline">\(A\)</span>
come quella matrice, che indichiamo con <span
class="math inline">\({}^tA\)</span>, che è un elemento di <span
class="math inline">\(M_{n,m} (\mathbb{R})\)</span> determinato dalla
seguente proprietà: l’entrata di posto <span
class="math inline">\(i,j\)</span> di <span
class="math inline">\({}^tA\)</span> è uguale alla entrata di posto
<span class="math inline">\(j,i\)</span> di <span
class="math inline">\(A\)</span>, ovvero</p>
<p><span class="math inline">\(({}^tA)_{ij} := A_{ji}\)</span></p>
<p><span class="math inline">\(\forall i \in \{1, \dots,
n\}\)</span><br />
<span class="math inline">\(\forall j \in \{1, \dots, m\}\)</span></p>
<p><strong>Esempio</strong><br />
<span class="math display">\[A =
\begin{pmatrix}
   1 &amp; 2 &amp; 3 \\
   -3 &amp; -2 &amp; -1
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[{}^tA =
\begin{pmatrix}
   1 &amp; -3 \\
   2 &amp; -2 \\
   3 &amp; -1
\end{pmatrix}
\]</span></p>
<p><strong>Proposizione</strong><br />
Siano <span class="math inline">\(A,B \in M_{m,n} (\mathbb{R})\)</span>,
allora</p>
<ol type="1">
<li><span class="math inline">\(^t(A+B) =\ ^tA +\ ^tB\)</span></li>
<li><span class="math inline">\(^t(\ ^tA) = A\)</span></li>
</ol>
<p><strong>Dimostrazione 1.</strong><br />
Notiamo che</p>
<p><span class="math inline">\(^t(A + B) \in M_{n,m}
(\mathbb{R})\)</span></p>
<p><span class="math inline">\(^tA \in M_{n,m} (\mathbb{R})\)</span></p>
<p><span class="math inline">\(^tB \in M_{n,m} (\mathbb{R})\)</span></p>
<p>Abbiamo dunque che sia a sinistra che a destra dell’uguale abbiamo
matrici dello stesso tipo, dunque ha senso chiedersi se esse sono
uguali.</p>
<p>per vedere che le due matrici sono uguali dimostriamo che tutte le
loro entrate sono uguali, ovvero che</p>
<p><span class="math inline">\(\forall i \in \{1, \dots,
n\}\)</span><br />
<span class="math inline">\(\forall j \in \{1, \dots, m\}\)</span></p>
<p><span class="math inline">\((^t(A+B))_{ij} = (^tA +\
^tB)_{ij}\)</span></p>
<p>fissiamo <span class="math inline">\(\forall i \in \{1, \dots,
n\}\)</span> e <span class="math inline">\(\forall j \in \{1, \dots,
m\}\)</span>, allora</p>
<p><span class="math inline">\((^t(A+B))_{ij} = (A + B)_{ji} = A_{ji} +
B_{ji}\)</span></p>
<p><span class="math inline">\((^tA +\ ^tB)_{ij} = (^tA)_{ij} +
(^tB)_{ij} = A_{ji} + B_{ji}\)</span></p>
<p>quindi le due quantità sono uguali.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Dimostrazione 2.</strong><br />
Notiamo</p>
<p><span class="math inline">\(^t(\ ^tA) \in M_{m,n}
(\mathbb{R})\)</span></p>
<p><span class="math inline">\(^tA \in M_{m,n} (\mathbb{R})\)</span></p>
<p>dunque ha senso chiedersi se <span class="math inline">\(^t(\ ^tA) =
A\)</span>; per mostrarlo, dimostriamo che tutte le entrate di queste
due matrici sono uguali, ovvero</p>
<p><span class="math inline">\(\forall i \in \{1, \dots,
n\}\)</span><br />
<span class="math inline">\(\forall j \in \{1, \dots, m\}\)</span></p>
<p><span class="math inline">\(^t(\ ^tA)_{ij} = A_{ij}\)</span></p>
<p>fissiamo <span class="math inline">\(\forall i \in \{1, \dots,
n\}\)</span> e <span class="math inline">\(\forall j \in \{1, \dots,
m\}\)</span>, allora</p>
<p><span class="math inline">\(^t(\ ^tA)_{ij} = (^tA)_{ji} =
A_{ij}\)</span></p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Osservazione</strong><br />
Non ha sempre senso chiedersi se vale <span class="math inline">\(A =\
^tA\)</span> perché queste due matrici in generale sono di tipo diverso;
ha però senso chiederselo se la matrice è quadrata.</p>
<p><strong>Esempio</strong><br />
<span class="math display">\[
\begin{pmatrix}
  1 &amp; 2 &amp; 3 \\
  2 &amp; 4 &amp; 5 \\
  3 &amp; 5 &amp; 6 \\
\end{pmatrix}
\]</span></p>
<p>Questa matrice soddisfa <span class="math inline">\(A =\
^tA\)</span>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_{n} (\mathbb{R})\)</span>
(matrice quadrata); la matrice <span class="math inline">\(A\)</span> si
dice <strong>simmetrica</strong> se vale <span class="math inline">\(A
=\ ^tA\)</span>; la matrice si dice <strong>antisimmetrica</strong> se
vale <span class="math inline">\(A = -\ ^tA\)</span>.</p>
<p><strong>Esempio</strong><br />
<span class="math display">\[
\begin{pmatrix}
  \begin{array}{rrr}
    0 &amp; 2 &amp; 3 \\
    -2 &amp; 0 &amp; 4 \\
    -3 &amp; -4 &amp; 0 \\
  \end{array}
\end{pmatrix}
\]</span></p>
<p><strong>Osservazione</strong><br />
Ogni matrice antisimmetrica ha la diagonale costituita da entrate tutte
nulle.</p>
<p><strong>Nuova operazione</strong><br />
Ora introduciamo una nuova operazione fra matrici.</p>
<p>Consideriamo questa situazione:</p>
<p>costo unitario della pasta: <span class="math inline">\(c_P =
1\$\)</span><br />
costo unitario del latte: <span class="math inline">\(c_L =
2\$\)</span><br />
costo unitario delle uova: <span class="math inline">\(c_U =
3\$\)</span></p>
<p>Supponiamo di dover acquistare <span class="math inline">\(n_P, n_L,
n_U\)</span> unità di pasta, latte e uova.</p>
<p>Qual è il costo totale?</p>
<p><span class="math inline">\(c_P \cdot n_P + c_L \cdot n_L + c_U \cdot
n_U\)</span></p>
<p>Facciamo una matrice <span class="math inline">\(1 \times 3\)</span>
con i costi unitari e una matrice <span class="math inline">\(3 \times
1\)</span> con il numero delle unità.</p>
<p><span class="math display">\[
\begin{pmatrix}
  \begin{array}{rrr}
    c_P &amp; c_L &amp; c_U \\
  \end{array}
\end{pmatrix},\
\begin{pmatrix}
  \begin{array}{rrr}
    n_P \\
    n_L \\
    n_U \\
  \end{array}
\end{pmatrix}
\]</span></p>
<p>Più in generale, se abbiamo una matrice riga <span
class="math inline">\(1 \times n\)</span> e una matrice colonna <span
class="math inline">\(n \times 1\)</span>, definiamo il loro prodotto
righe per colonne come:</p>
<p><span class="math display">\[
\begin{pmatrix}
  \begin{array}{rrr}
    c_P &amp; c_L &amp; c_U
  \end{array}
\end{pmatrix}
\cdot
\begin{pmatrix}
  \begin{array}{rrr}
    n_P \\
    n_L \\
    n_U \\
  \end{array}
\end{pmatrix}
:=
c_p \cdot n_p + c_l \cdot n_l + c_u \cdot n_u
\]</span></p>
<p>Più in generale, se abbiamo una matrice riga <span
class="math inline">\(1 \times n\)</span> e una matrice colonna <span
class="math inline">\(n \times 1\)</span>, definiamo il loro prodotto
righe per colonne come:</p>
<p><span class="math display">\[
\begin{pmatrix}
  a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n}
\end{pmatrix}
\cdot
\begin{pmatrix}
  b_{11} \\
  b_{21} \\
  \vdots \\
  b_{n1}
\end{pmatrix}
:=
a_{11} \cdot b_{11} + a_{12} \cdot b_{21} + \dots + a_{1n} \cdot b_{n1}
=
\displaystyle\sum_{k=1}^{n} a_{k1} \cdot b_{k1}
\]</span></p>
<p>Supponiamo che in un altro negozio valga</p>
<p><span class="math inline">\(c&#39;_P = -3 \$\)</span><br />
<span class="math inline">\(c&#39;_L = -2 \$\)</span><br />
<span class="math inline">\(c&#39;_U = -1 \$\)</span></p>
<p>Per tenere sotto controllo i due totali di spesa potrei impacchettare
le due righe dei costi unitari in un unica matrice.</p>
<p><span class="math display">\[
\begin{pmatrix}
  \begin{array}{rrr}
    c_P &amp; c_L &amp; c_U \\
    c&#39;_P &amp; c&#39;_L &amp; c&#39;_U \\
  \end{array}
\end{pmatrix}
\in M_{2,2} (\mathbb{R})
\]</span></p>
<p>Potrebbe essere ragionevole definire il prodotto di</p>
<p><span class="math display">\[
\begin{pmatrix}
  \begin{array}{rrr}
    c_P &amp; c_L &amp; c_U \\
    c&#39;_P &amp; c&#39;_L &amp; c&#39;_U \\
  \end{array}
\end{pmatrix}
\cdot
\begin{pmatrix}
  \begin{array}{rrr}
    n_P \\
    n_L \\
    n_U \\
  \end{array}
\end{pmatrix}
\]</span></p>
<p>come la matrice <span class="math inline">\(2 \times 1\)</span>:</p>
<p><span class="math display">\[
\begin{pmatrix}
  \begin{array}{rrr}
    c_P \cdot n_P + c_L \cdot n_L + c_U \cdot n_U \\
    c&#39;_P \cdot n_P + c&#39;_L \cdot n_L + c&#39;_U \cdot n_U \\
  \end{array}
\end{pmatrix}
\in M_{2,1} (\mathbb{R})
\]</span></p>
<p>Ricapitolando, abbiamo moltiplicato una matrice <span
class="math inline">\(2 \times 3\)</span> per una matrice <span
class="math inline">\(3 \times 1\)</span> e abbiamo ottenuto una matrice
<span class="math inline">\(2 \times 1\)</span>. In altre parole, la
matrice ottenuta dalla moltiplicazione è quella matrice le cui entrate
sono date dalla moltiplicazione di ciascuna delle due righe della prima
matrice con la colonna della seconda matrice.</p>
<p>In questo modo, se volessimo aggiungere una seconda colonna di
quantitativi</p>
<p><span class="math display">\[
\begin{pmatrix}
  \begin{array}{rrr}
    n&#39;_P \\
    n&#39;_L \\
    n&#39;_U \\
  \end{array}
\end{pmatrix}
\]</span></p>
<p>quella che andremo a ottenere è una situazione del tipo:</p>
<p><span class="math display">\[
\begin{pmatrix}
  \begin{array}{rrr}
    c_P &amp; c_L &amp; c_U \\
    c&#39;_P &amp; c&#39;_L &amp; c&#39;_U \\
  \end{array}
\end{pmatrix}
\cdot
\begin{pmatrix}
  \begin{array}{rrr}
    n_P &amp; n&#39;_P \\
    n_L &amp; n&#39;_L \\
    n_U &amp; n&#39;_U \\
  \end{array}
\end{pmatrix} =
\]</span></p>
<p><span class="math display">\[
= \begin{pmatrix}
  (c_P \cdot n_P + c_L \cdot n_L + c_U \cdot n_U) &amp; (c_P \cdot
n&#39;_P + c_L \cdot n&#39;_L + c_U \cdot n&#39;_U) \\
  (c&#39;_P \cdot n_P + c&#39;_L \cdot n_L + c&#39;_U \cdot n_U) &amp;
(c&#39;_P \cdot n&#39;_P + c&#39;_L \cdot n&#39;_L + c&#39;_U \cdot
n&#39;_U) \\
\end{pmatrix}
\]</span></p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_{1,n} (\mathbb{R})\)</span> e
sia <span class="math inline">\(B \in M_{n,1} (\mathbb{R})\)</span>;
allora definiamo il prodotto <span class="math inline">\(A \cdot
B\)</span> come il numero (o equivalentemente la matrice <span
class="math inline">\(1 \times 1\)</span>) dato da</p>
<p><span class="math inline">\(A \cdot B = a_{11} \cdot b_{11} + \dots +
a_{1n} \cdot b_{n1} = \displaystyle\sum_{k=1}^{n} a_{k1} \cdot
b_{k1}\)</span></p>
<p>(questo è il prodotto di una riga per una colonna)</p>
<p>in generale, se <span class="math inline">\(A \in M_{m,p}
(\mathbb{R})\)</span> e <span class="math inline">\(B \in M_{p,n}
(\mathbb{R})\)</span>, allora il prodotto <span class="math inline">\(A
\cdot B\)</span> è la matrice <span class="math inline">\(m \times
n\)</span> la cui entrata di posto <span
class="math inline">\(i,j\)</span> è data da</p>
<p><span class="math inline">\((A \cdot B)_{ij} = A_{(i)} \cdot B^{(j)}
= a_{i1} \cdot b_{1j} + \dots + a_{ip} \cdot b_{pj} =
\displaystyle\sum_{k=1}^{p} a_{ik} \cdot b_{kj}\)</span></p>
<p><strong>Osservazione</strong><br />
Il prodotto tra due matrici <span class="math inline">\(A\)</span> e
<span class="math inline">\(B\)</span> è definito solo se il numero di
colonne di <span class="math inline">\(A\)</span> coincide con il numero
di righe di <span class="math inline">\(B\)</span>.</p>
<blockquote>
<p>Esempio moltiplicazione due matrici (<span class="math inline">\(2
\times 3\)</span> per <span class="math inline">\(3 \times 2\)</span> e
<span class="math inline">\(3 \times 3\)</span> per <span
class="math inline">\(3 \times 2\)</span>).</p>
</blockquote>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(n \in \mathbb{N}, n&gt;0\)</span>,
allora la matrice unità è quella matrice quadrata <span
class="math inline">\(n \times n\)</span> le cui entrate sono tutte
nulle, fuorché quella della diagonale principale, che sono tutte uguali
a <span class="math inline">\(1\)</span>; denotiamo questa matrice con
<span class="math inline">\(1_n\)</span> oppure <span
class="math inline">\(I_n\)</span> oppure <span
class="math inline">\(Id_n\)</span>.</p>
<p>quindi vale che</p>
<p><span class="math inline">\((1_n)_{ij} := 0\ se\ i \neq j,\ 1\ se\ i
= j\)</span></p>
<blockquote>
<p>Esempio per vedere che <span class="math inline">\(A \cdot B \neq B
\cdot A\)</span></p>
</blockquote>
<p><strong>Proposizione</strong><br />
Siano <span class="math inline">\(A, B \in M_{m,p}(\mathbb{R})\)</span>
e sia <span class="math inline">\(C, D \in
M_{p,n}(\mathbb{R})\)</span>,<br />
allora valgono le seguenti uguaglianze:</p>
<ol type="1">
<li><span class="math inline">\((A+B) \cdot C = A \cdot C + B \cdot
C\)</span> (<strong>proprietà distributiva a destra</strong>)</li>
<li><span class="math inline">\(A \cdot (C + D) = A \cdot C + A \cdot
D\)</span> (<strong>proprietà distributiva a sinistra</strong>)</li>
</ol>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(A \in M_{m,p}(\mathbb{R})\)</span>,
<span class="math inline">\(B \in M_{p,q}(\mathbb{R})\)</span> e <span
class="math inline">\(C \in M_{q,n}(\mathbb{R})\)</span>,<br />
allora vale che</p>
<p><span class="math inline">\((A \cdot B) \cdot C = A \cdot (B \cdot
C)\)</span> (<strong>proprietà associativa del prodotto</strong>)</p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(A \in M_{m,p}(\mathbb{R})\)</span> e
<span class="math inline">\(B \in M_{p,n}(\mathbb{R})\)</span>,<br />
allora</p>
<p><span class="math inline">\(\underbrace{{}^t(A \cdot B)}_{n \times m}
\neq \underbrace{{}^t A}_{p \times m} \cdot \underbrace{{}^tB}_{n \times
p}\)</span></p>
<p>Le matrici <span class="math inline">\({}^tA \cdot {}^tB\)</span>
<strong>non</strong> si possono moltiplicare tra loro in generale (ne
<span class="math inline">\(m \neq n\)</span>).</p>
<p>Vale invece che</p>
<p><span class="math inline">\(\underbrace{{}^t(A \cdot B)}_{n \times m}
= \underbrace{{}^tB}_{n \times p} \cdot \underbrace{{}^tA}_{p \times
m}\)</span></p>
<p><strong>Dimostrazione</strong><br />
Per mostrare che <span class="math inline">\({}^t (A \cdot B)\)</span> è
uguale a <span class="math inline">\({}^tB \cdot {}^tA\)</span>,
mostriamo che tutte le loro entrate sono uguali: sta dunque <span
class="math inline">\(i \in \{1, \dots, n\}\)</span> e sia <span
class="math inline">\(j \in \{1, \dots, m\}\)</span>; allora</p>
<p><span class="math inline">\(({}^t(A \cdot B))_{ij} = (A \cdot B)_{ji}
= A_{(j)} \cdot B^{(i)}\)</span></p>
<p><span class="math inline">\(({}^t B \cdot {}^tA)_{ij} = ({}^t
B)_{(i)} \cdot ({}^t A)^{(j)} = A_{(j)} \cdot B^{(i)}\)</span></p>
<p>questo mostra che le due matrici sono uguali.</p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(A \in M_{m,n}(\mathbb{R})\)</span>,
allora</p>
<p><span class="math inline">\(1_m \cdot A = A\)</span> e <span
class="math inline">\(A \cdot 1_m = A\)</span></p>
<p><strong>Osservazione</strong><br />
Nel caso delle matrici quadrate, la matrice unità <span
class="math inline">\(1_n\)</span> funge dunque da elemento neutro per
il prodotto righe per colonne:</p>
<p>per ogni <span class="math inline">\(A \in M_{n}(\mathbb{R})\)</span>
vale che</p>
<p><span class="math inline">\(1_n \cdot A = A\)</span> e <span
class="math inline">\(A \cdot 1_n = A\)</span></p>
<p><strong>Osservazione</strong><br />
Nei numeri reali, dato <span class="math inline">\(a \in
\mathbb{R}\)</span>, diciamo che <span class="math inline">\(b\)</span>
è inverso di <span class="math inline">\(a\)</span> se vale che <span
class="math inline">\(a \cdot b = b \cdot a = 1\)</span>; ogni numero
reale non nullo ammette un unico inverso; l’inverso di <span
class="math inline">\(a \in \mathbb{R} \smallsetminus 0\)</span> si
denota <span class="math inline">\(a^{-1}\)</span>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_n(\mathbb{R})\)</span>; A si
dice invertibile se esiste <span class="math inline">\(B \in
M_n(\mathbb{R})\)</span> tale che valga:</p>
<p><span class="math inline">\(A \cdot B = B \cdot A = 1_n\)</span></p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(A, B \in M_n (\mathbb{R})\)</span>,
allora</p>
<ol type="1">
<li>se <span class="math inline">\(A\)</span> è invertibile, allora
l’inversa di <span class="math inline">\(A\)</span> è unica</li>
<li>se <span class="math inline">\(A\)</span> e <span
class="math inline">\(B\)</span> sono invertibili, allora anche <span
class="math inline">\(A \cdot B\)</span> è invertibile e la sua inversa
è <span class="math inline">\(B^{-1} \cdot A^{-1}\)</span></li>
</ol>
<p><strong>Dimostrazione</strong><br />
1- siano <span class="math inline">\(B, C\)</span> entrambe inverse di
<span class="math inline">\(A\)</span>, allora</p>
<p><span class="math inline">\(A \cdot B = B \cdot A =
1_n\)</span><br />
<span class="math inline">\(A \cdot C = C \cdot A = 1_n\)</span></p>
<p>allora</p>
<p><span class="math inline">\(B = B \cdot 1_n = B \cdot (A \cdot C) =
(B \cdot A) \cdot C = 1_n \cdot C = C\)</span></p>
<p>2- mostriamo che <span class="math inline">\(B^{-1} \cdot
A^{-1}\)</span> è inversa di <span class="math inline">\(A \cdot
B\)</span> :</p>
<p><span class="math inline">\((A \cdot B) \cdot (B^{-1} \cdot A^{-1}) =
A \cdot (B \cdot B^{-1}) \cdot A^{-1} = A \cdot 1_n \cdot A^{-1} = A
\cdot A^{-1} = 1_n\)</span></p>
<p>analogamente</p>
<p><span class="math inline">\((B^{-1} \cdot A^{-1}) \cdot (A \cdot B) =
B^{-1} \cdot (A^{-1} \cdot A) \cdot B = B^{-1} \cdot 1_n \cdot B =
B^{-1} \cdot B = 1_n\)</span></p>
<p><strong>Osservazione</strong> L’analogia tra invertibilità rispetto
al prodotto di due numeri reali e l’invertibilità rispetto al prodotto
righe per colonne di matrici <em>non</em> si estende fino al punto di
dire che ogni matrice non nulla è invertibile.</p>
<p><strong>Esempio</strong><br />
Consideriamo la matrice <span class="math inline">\(A \in
M_{2}(\mathbb{R})\)</span>,</p>
<p><span class="math display">\[
A=
\begin{pmatrix}
  1 &amp; 1 \\
  1 &amp; 1
\end{pmatrix}
\]</span></p>
<p>mostriamo che <span class="math inline">\(A\)</span> non è
invertibile; supponiamo che esiste un’inversa <span
class="math inline">\(B\)</span></p>
<p><span class="math display">\[
B =
\begin{pmatrix}
  b_{11} &amp; b_{12} \\
  b_{21} &amp; b_{22}
\end{pmatrix}
\]</span></p>
<p>dovrebbe essere che</p>
<p><span class="math display">\[
A \cdot B =
\begin{pmatrix}
  \underline{1} &amp; 0 \\
  \underline{0} &amp; 1
\end{pmatrix}
\]</span></p>
<p>quindi in particolare</p>
<p><span class="math display">\[
(1 , 1) \cdot
\begin{pmatrix}
  b_{11} \\
  b_{21} \\
\end{pmatrix}
= 1
\]</span></p>
<p>ovvero <span class="math inline">\(b_{11} + b_{21} = 1\)</span></p>
<p><span class="math display">\[(1 , 1) \cdot
\begin{pmatrix}
  b_{11} \\
  b_{21} \\
\end{pmatrix}
= 0
\]</span></p>
<p>ovvero <span class="math inline">\(b_{11} + b_{21} = 0\)</span></p>
<p>e questo è impossibile perché implicherebbe <span
class="math inline">\(1 = 0\)</span>.</p>
<p>Fermiamoci un attimo! Se ci guardiamo indietro, vediamo che
ogniqualvolta abbiamo usato i numeri reali, abbiamo solamente utilizzato
le loro proprietà rispetto a somma e moltiplicazione. Queste sono
infatti le proprietà che lo rendono un campo.</p>
<p><strong>Definizione</strong><br />
Sia K un insieme su cui siano definite un’operazione di somma e una
operazione di moltiplicazione, ovvero</p>
<p><span class="math inline">\(+ : K \times K \rightarrow
K\)</span><br />
<span class="math inline">\((a,b) \mapsto a + b\)</span></p>
<p><span class="math inline">\(\cdot : K \times K \rightarrow
K\)</span><br />
<span class="math inline">\((a,b) \mapsto a \cdot b\)</span></p>
<p>tale per cui siano soddisfatte le seguenti proprietà:</p>
<p>K1: commutatività:<br />
<span class="math inline">\(\forall a, b \in K, a + b = b + a, a \cdot b
= b \cdot a\)</span></p>
<p>K2: associatività:<br />
<span class="math inline">\(\forall a, b, c \in K, (a + b) + c = a + (b
+ c), (a \cdot b) \cdot c = a \cdot (b \cdot c)\)</span></p>
<p>K3: esistenza dell’elemento neutro:<br />
<span class="math inline">\(\exists 0 \in K\)</span>, tale che <span
class="math inline">\(\forall a \in K, a + 0 = 0 + a = a\)</span><br />
<span class="math inline">\(\exists 1 \in K\)</span>, tale che <span
class="math inline">\(\forall a \in K, a \cdot 1 = 1 \cdot a =
a\)</span><br />
e inoltre <span class="math inline">\(0 \neq 1\)</span></p>
<p>K4: esistenza di opposto e inverso:<br />
<span class="math inline">\(\forall a \in K, \exists b \in K\)</span>,
tale che <span class="math inline">\(a + b = b + a = 0\)</span><br />
(denotiamo <span class="math inline">\(b\)</span> con <span
class="math inline">\(-a\)</span>)<br />
<span class="math inline">\(\forall a \in K \smallsetminus \{0\},
\exists c \in K\)</span>, tale che <span class="math inline">\(a \cdot c
= c \cdot a = 1\)</span><br />
(denotiamo <span class="math inline">\(c\)</span> con <span
class="math inline">\(a^{-1}\)</span> o con <span
class="math inline">\(1/a\)</span>)</p>
<p>K5: distributività: <span class="math inline">\(\forall a, b, c \in
K, a \cdot (b + c) = (a \cdot b) + (a \cdot c)\)</span></p>
<p>un tale insieme si dice <strong>campo</strong>.</p>
<p><strong>Esempio</strong><br />
<span class="math inline">\(\mathbb{Q}\)</span> è un
<strong>campo</strong><br />
<span class="math inline">\(\mathbb{R}\)</span> è un
<strong>campo</strong><br />
<span class="math inline">\(\mathbb{C}\)</span> è un
<strong>campo</strong><br />
<span class="math inline">\(\mathbb{N}\)</span> non è un campo<br />
<span class="math inline">\(\mathbb{Z}\)</span> non è un campo</p>
<p><strong>Osservazione</strong><br />
L’insieme delle funzioni razionali</p>
<p><span class="math inline">\(\{\frac{p}{q}, \text{p e q sono polinomi
in una variabile}\}\)</span></p>
<p>può essere dotata di somma e prodotto in modo da renderlo un
campo.</p>
<p><strong>Esempio</strong><br />
L’insieme <span class="math inline">\(\mathbb{Z}_2 = \{0, 1\}\)</span>
su cui definiamo una somma e un prodotto nel modo seguente</p>
<p><span class="math inline">\(+ : \mathbb{Z}_2 \times \mathbb{Z}_2
\rightarrow \mathbb{Z}_2\)</span><br />
<span class="math inline">\((a,b) \mapsto a + b\)</span></p>
<p><span class="math inline">\(\cdot : \mathbb{Z}_2 \times \mathbb{Z}_2
\rightarrow \mathbb{Z}_2\)</span><br />
<span class="math inline">\((a,b) \mapsto a \cdot b\)</span></p>
<p><span class="math display">\[
\begin{array}{c|cc}
+ &amp; 0 &amp; 1 \\
\hline
0 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0 \\
\end{array}
\qquad \qquad \qquad
\begin{array}{c|cc}
\cdot &amp; 0 &amp; 1 \\
\hline
0 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
\end{array}
\]</span></p>
<p>è un campo.</p>
<p>Pertanto la precedente nozione di <span
class="math inline">\(\mathbb{R}\)</span>-spazio vettoriale sarà d’ora
in poi sostituita da quella di <span
class="math inline">\(K\)</span>-spazio vettoriale, con <span
class="math inline">\(K\)</span> un campo.</p>
<h1 id="sistemi-lineari">Sistemi lineari</h1>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(K\)</span> un campo; un sistema di <span
class="math inline">\(m\)</span> equazioni in <span
class="math inline">\(n\)</span> incognite a coefficienti in <span
class="math inline">\(K\)</span> è un sistema di equazioni della forma
seguente</p>
<p><span class="math inline">\(a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n
= b_1\)</span><br />
<span class="math inline">\(a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n =
b_2\)</span><br />
<span class="math inline">\(\vdots\)</span><br />
<span class="math inline">\(a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n =
b_m\)</span></p>
<p>dove ogni <span class="math inline">\(a_{ij}\)</span> è un elemento
di <span class="math inline">\(K\)</span> per ogni <span
class="math inline">\(i \in \{1, \dots, m\}\)</span>, <span
class="math inline">\(j \in \{1, \dots, n\}\)</span> e ogni <span
class="math inline">\(b\)</span> è un elemento di <span
class="math inline">\(K\)</span> per ogni <span class="math inline">\(i
\in \{1, \dots, m\}\)</span>; <span class="math inline">\(x_1, \dots,
x_n\)</span> sono dette <strong>incognite</strong>, mentre gli elementi
<span class="math inline">\(b_1, \dots, b_m\)</span> sono detti i
<strong>termini noti</strong> e gli elementi <span
class="math inline">\(a_{ij}\)</span> sono detti i
<strong>coefficienti</strong> del sistema; una soluzione del sistema è
una <span class="math inline">\(n\)</span>-upla ordinata (che
rappresentiamo come vettore colonna) <span class="math inline">\(s \in
K^n\)</span>, ovvero <span class="math inline">\(s = \begin{pmatrix} s_1
\\ \vdots \\ s_n \end{pmatrix}\)</span> con <span
class="math inline">\(s_i \in K\)</span> tale per cui se per ogni <span
class="math inline">\(i \in \{1, \dots, n\}\)</span> sostituiamo <span
class="math inline">\(x_i\)</span> con <span
class="math inline">\(s_i\)</span>, allora tutte le uguaglianze del
sistema saranno vere; il sistema si dice <strong>omogeneo</strong> se
<span class="math inline">\(b_1 = \dots = b_m = 0\)</span>, ovvero tutti
i termini noti sono nulli; un sistema si dice <strong>non
omogeneo</strong> se non è omogeneo; un sistema di dice
<strong>compatibile</strong> se ammette almeno una soluzione; altrimenti
si dice <strong>incompatibile</strong>.</p>
<p><strong>Osservazione</strong><br />
La <span class="math inline">\(n\)</span>-upla nulla <span
class="math inline">\(\begin{pmatrix} 0 \\ \vdots \\ 0
\end{pmatrix}\)</span> è sempre soluzione di un sistema omogeneo;
pertanto ogni sistema omogeneo è compatibile.</p>
<p><strong>Definizione</strong><br />
Dato un sistema lineare come nella definizione precedente, denotiamo</p>
<p><span class="math inline">\(A = (a_{ij}) \in M_{m,n}(K)\)</span></p>
<p><span class="math inline">\(X = \begin{pmatrix} x_1 \\ \vdots \\ x_n
\end{pmatrix}\)</span></p>
<p><span class="math inline">\(b = \begin{pmatrix} b_1 \\ \vdots \\ b_m
\end{pmatrix} \in M_{m,1}(K)\)</span></p>
<p>allora il sistema precedente può essere scritto nella forma</p>
<p><span class="math inline">\(\underbrace{A}_{m \times n} \cdot
\underbrace{X}_{n \times 1} = \underbrace{b}_{m \times 1}\)</span></p>
<p><strong>Esempio</strong><br />
Consideriamo il sistema</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    x_1 +2x_2 = 3 \\
    x_1 +2x_2 = 5 \\
  \end{array}
\right.
\]</span></p>
<p><span class="math display">\[
A =
\begin{pmatrix}
  1 &amp; 2 \\
  1 &amp; 2
\end{pmatrix}
,\
x =
\begin{pmatrix}
  x_1 \\
  x_2
\end{pmatrix}
,\
b =
\begin{pmatrix}
  3 \\
  5
\end{pmatrix}
\]</span></p>
<p>Il sistema <strong>non è omogeneo</strong>: almeno un termine noto
non è nullo.<br />
Il sistema <strong>è incompatibile</strong>: infatti esso non ha
soluzioni.</p>
<p>Se <span class="math display">\[
s = \begin{pmatrix}
  s_1 \\
  s_2 \\
\end{pmatrix}
\]</span> fosse una soluzione, allora verrebbe che:</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    s_1 + 2s_2 = 3 \\
    s_1 + 2s_2 = 5 \\
  \end{array}
\right.
\Rightarrow
3 = 5
\]</span></p>
<p>Che è impossibile.</p>
<p><strong>Esempio</strong><br />
Consideriamo il sistema</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    x_1 +2x_2 = 3\\
    x_1 -x_2 = 1 \\
  \end{array}
\right.
\]</span></p>
<p>Non è chiaro a priori se il sistema sia compatibile o meno.</p>
<p>Per sostituzione ottengo</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    x_1 = \frac{5}{3} \\
    x_2 = \frac{2}{3} \\
  \end{array}
\right.
\]</span></p>
<p>Il sistema ha dunque un’unica soluzione</p>
<p><span class="math display">\[
s =
\begin{pmatrix}
  \frac{5}{3} \\
  \frac{2}{3}
\end{pmatrix}
\]</span></p>
<p>esso è quindi compatibile (è non omogeneo).</p>
<p>Il motivo per il quale siamo certi che ci sia un’unica soluzione è
che per ottenere il sistema finale abbiamo trasformato il sistema
iniziale tramite operazioni che non cambiano l’insieme delle
soluzioni.</p>
<p><strong>Definizione</strong><br />
Due sistemi lineari si dicono <strong>equivalenti</strong> se ammettono
le medesime soluzioni (ovvero gli insiemi delle soluzioni sono
uguali).</p>
<p><strong>Esempio</strong><br />
Consideriamo il sistema:</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    x_1 +2x_2 = 3 \\
    2x_1 + 4x_2 = 6 \\
  \end{array}
\right.
\]</span></p>
<p><span class="math display">\[
A =
\begin{pmatrix}
  1 &amp; 2 \\
  2 &amp; 4
\end{pmatrix}
,\
x =
\begin{pmatrix}
  x_1 \\
  x_2
\end{pmatrix}
,\
b =
\begin{pmatrix}
  3 \\
  6
\end{pmatrix}
\]</span></p>
<p><span class="math inline">\(x_1 = 3 - 2x_2\)</span><br />
<span class="math inline">\(2(3 - 2x_2) = 6 - 4x_2\)</span><br />
<span class="math inline">\(6 - 4x_2 + 4x_2 = 6\)</span><br />
<span class="math inline">\(6 = 6\)</span> sempre vera per ogni <span
class="math inline">\(x_2\)</span></p>
<p>Il sistema è pertanto equivalente ad un’unica equazione</p>
<p><span class="math inline">\(x_1 = 3 - 2x_2\)</span></p>
<p>Le soluzioni di questa equazione si possono esprimere cosi:</p>
<p>se a <span class="math inline">\(x_2\)</span> assegno il valore <span
class="math inline">\(t \in \mathbb{R}\)</span> allora a <span
class="math inline">\(x_1\)</span> devo assegnare il valore <span
class="math inline">\(3 - 2t\)</span>, ovvero le soluzioni sono della
forma</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    \begin{pmatrix}
      3 -2t \\
      t
    \end{pmatrix}
    : t \in \mathbb{R}
  \end{array}
\right\rbrace
\]</span></p>
<p>vediamo che le soluzioni sono infinite;</p>
<p><span class="math display">\[
\begin{pmatrix}
  3 \\
  0
\end{pmatrix}
+
t
\begin{pmatrix}
  -2 \\
  1
\end{pmatrix}
\]</span></p>
<p>graficamente queste soluzioni corrispondono ai punti di una retta che
passa per il punto <span class="math inline">\((3, 0)\)</span>.</p>
<h2 id="teorema-di-cramer">Teorema di Cramer</h2>
<p>Consideriamo un sistema lineare con <span
class="math inline">\(n\)</span> equazioni ad <span
class="math inline">\(n\)</span> incognite</p>
<p><span class="math inline">\(AX = b\)</span></p>
<p>ovvero la matrice <span class="math inline">\(A\)</span> è quadrata,
<span class="math inline">\(A \in M_{n}(K)\)</span>, supponiamo inoltre
che <span class="math inline">\(A\)</span> sia invertibile, allora
esiste un’unica soluzione del sistema ed essa è data da <span
class="math inline">\(s = A^{-1} \cdot b\)</span>.</p>
<p><strong>Osservazione</strong><br />
Questo teorema non ci dice soltanto che una soluzione esiste, ma ci
fornisce anche un modo per calcolarla.</p>
<p><strong>Dimostrazione</strong><br />
Per dimostrare il teorema, dimostriamo due cose:</p>
<ol type="1">
<li>che <span class="math inline">\(A^{-1} \cdot b\)</span> è soluzione
del sistema.</li>
<li>che <span class="math inline">\(A^{-1} \cdot b\)</span> è l’unica
soluzione del sistema.</li>
</ol>
<p>1- <span class="math inline">\(A^{-1} \cdot b\)</span> è soluzione
del sistema se e solo se, se sostituiamo <span
class="math inline">\(X\)</span> con <span class="math inline">\(A^{-1}
\cdot b\)</span>, otteniamo una uguaglianza vera nel sistema (notiamo
che la sostituzione ha senso dato che <span
class="math inline">\(X\)</span> è una matrice <span
class="math inline">\(n \times 1\)</span> e <span
class="math inline">\(A^{-1} \cdot b\)</span> è una matrice <span
class="math inline">\(n \times 1\)</span>).</p>
<p><span class="math inline">\(A \cdot (A^{-1} \cdot b) \stackrel{?}{=}
b\)</span><br />
<span class="math inline">\((A \cdot A^{-1}) \cdot b \stackrel{?}{=}
b\)</span><br />
<span class="math inline">\(1_n \cdot b \stackrel{?}{=} b\)</span><br />
<span class="math inline">\(b = b\)</span><br />
vero!</p>
<p><span class="math inline">\(\square\)</span></p>
<p>Abbiamo verificato che l’uguaglianza è vera, dunque <span
class="math inline">\(A^{-1} \cdot b\)</span> è soluzione del sistema,
il quale è quindi compatibile.</p>
<p>2- per dimostrare che <span class="math inline">\(A^{-1} \cdot
b\)</span> sia l’unica soluzione del sistema, supponiamo che ve ne sia
un’altra, ovvero che <span class="math inline">\(s&#39; \in
M_{n,1}(K)\)</span> sia soluzione del sistema e mostriamo che deve
essere <span class="math inline">\(s&#39; = A^{-1} \cdot b\)</span></p>
<p>(ovvero mostriamo che dal fatto che <span
class="math inline">\(s&#39;\)</span> è soluzione del sistema, discende
che <span class="math inline">\(s&#39;\)</span> deve essere uguale a
<span class="math inline">\(A^{-1} \cdot b\)</span>)</p>
<p>abbiamo quindi supposto che</p>
<p><span class="math inline">\(A \cdot s&#39; = b\)</span></p>
<p>ora moltiplico entrambi i membri a sinistra per <span
class="math inline">\(A^{-1}\)</span></p>
<p><span class="math inline">\(A^{-1} \cdot (A \cdot s&#39;) = A^{-1}
\cdot b\)</span><br />
<span class="math inline">\((A^{-1} \cdot A) \cdot s&#39; = A^{-1} \cdot
b\)</span><br />
<span class="math inline">\(1_n \cdot s&#39; = A^{-1} \cdot
b\)</span><br />
<span class="math inline">\(s&#39; = A^{-1} \cdot b\)</span></p>
<p>Quindi <span class="math inline">\(s&#39; = A^{-1} \cdot
b\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Notazione</strong><br />
D’ora in poi andremo ad identificare i seguenti due spazi
vettoriali:</p>
<p><span class="math display">\[
\begin{pmatrix}
  b_1 \\
  \vdots \\
  b_m
\end{pmatrix}
\in M_{m,1}(K),
\begin{pmatrix}
  b_1 \\
  \vdots \\
  b_m
\end{pmatrix}
\in K^m
\]</span></p>
<p>(<strong>isomorfi</strong>)</p>
<p>in questo senso, diremo ad esempio che un elemento <span
class="math inline">\(s \in K^n\)</span> è soluzione di un sistema
lineare <span class="math inline">\(AX = b\)</span>.</p>
<h2 id="teorema-di-struttura-per-sistemi-lineari-omogenei">Teorema di
struttura per sistemi lineari omogenei</h2>
<p>Consideriamo un sistema lineare omogeneo di <span
class="math inline">\(m\)</span> equazioni ad <span
class="math inline">\(n\)</span> incognite</p>
<p><span class="math inline">\(AX = 0\)</span></p>
<p>(<span class="math inline">\(0\)</span> è la matrice <span
class="math inline">\(n \times 1\)</span> con tutte le entrate
nulle)</p>
<p>siano <span class="math inline">\(s, s&#39; \in K^n\)</span> due
soluzioni del sistema e sia <span class="math inline">\(\lambda \in
K\)</span>; allora</p>
<ol type="1">
<li><span class="math inline">\(s + s&#39;\)</span> è soluzione del
sistema lineare</li>
<li><span class="math inline">\(\lambda s\)</span> è soluzione del
sistema lineare</li>
</ol>
<p>pertanto ricordando che il vettore nullo <span
class="math inline">\(0 \in K^n\)</span> è sempre soluzione del sistema
omogeneo, otteniamo che l’insieme delle soluzioni di <span
class="math inline">\(AX = 0\)</span>, ovvero l’insieme</p>
<p><span class="math inline">\(\left\lbrace r \in K^n : A \cdot r =
0\right\rbrace\)</span></p>
<p>è un sottospazio vettoriale di <span
class="math inline">\(K^n\)</span>.</p>
<p><strong>Osservazione</strong><br />
Vale che, se <span class="math inline">\(A \in M_{n,m}(K)\)</span> e
<span class="math inline">\(s \in K^n\)</span> e <span
class="math inline">\(\lambda \in K\)</span>, allora <span
class="math inline">\(A \cdot (\lambda \cdot s) = \lambda \cdot (A \cdot
s)\)</span>.</p>
<p><strong>Dimostrazione</strong><br />
1- Dato che <span class="math inline">\(s, s&#39;\)</span> siano
soluzioni, vale che</p>
<p><span class="math inline">\(A \cdot s = 0\)</span> e <span
class="math inline">\(A \cdot s&#39; = 0\)</span></p>
<p>per mostrare che <span class="math inline">\(s + s&#39;\)</span> è
soluzione dobbiamo dimostrare che</p>
<p><span class="math inline">\(A \cdot (s + s&#39;) = 0\)</span></p>
<p><span class="math inline">\(A \cdot (s + s&#39;) = A \cdot s + A
\cdot s&#39; = 0 + 0 = 0\)</span></p>
<p>(il prodotto righe per colonne soddisfa la proprietà
distributiva)</p>
<p>Quindi <span class="math inline">\(s + s&#39;\)</span> è
soluzione.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>2- Dato che <span class="math inline">\(s\)</span> è soluzione, vale
che</p>
<p><span class="math inline">\(A \cdot s = 0\)</span></p>
<p>per mostrare che <span class="math inline">\(\lambda \cdot s\)</span>
è soluzione dobbiamo dimostrare che</p>
<p>A <span class="math inline">\(\cdot (\lambda \cdot s) =
0\)</span></p>
<p>A <span class="math inline">\(\cdot (\lambda \cdot s) = \lambda \cdot
(A \cdot s) = \lambda \cdot 0 = 0\)</span></p>
<p>Quindi <span class="math inline">\(\lambda \cdot s\)</span> è
soluzione.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Osservazione</strong><br />
Sia <span class="math inline">\(A \in M_n (K)\)</span> e <span
class="math inline">\(A\)</span> invertibile;<br />
consideriamo il sistema lineare omogeneo</p>
<p><span class="math inline">\(AX = 0\)</span></p>
<p>allora per il teorema di Cramer, <span class="math inline">\(A^{-1}
\cdot 0\)</span> è l’unica soluzione del sistema<br />
e dato che <span class="math inline">\(A^{-1} \cdot 0 = 0\)</span>
abbiamo che <span class="math inline">\(0\)</span> è l’unica soluzione
di un tale sistema lineare omogeneo.</p>
<h2
id="teorema-di-struttura-per-sistemi-lineari-qualsiasiarbitrari">Teorema
di struttura per sistemi lineari qualsiasi/arbitrari</h2>
<p>Consideriamo un sistema lineare<br />
<span class="math inline">\(AX = b\)</span> con <span
class="math inline">\(A \in M_{m,n}(K)\)</span> e <span
class="math inline">\(b \in K^m\)</span>,<br />
e sia <span class="math inline">\(\tilde{s}\)</span> una sua
soluzione;</p>
<p>allora un elemento <span class="math inline">\(s \in K^n\)</span> è
soluzione di <span class="math inline">\(AX = b\)</span><br />
se e solo se possiamo scrivere <span class="math inline">\(s = \tilde{s}
+ s_0\)</span>,<br />
dove <span class="math inline">\(s_0\)</span> è una soluzione del
sistema lineare omogeneo <span class="math inline">\(AX =
0\)</span>.</p>
<p>In altre parole, l’insieme delle soluzioni di <span
class="math inline">\(AX = b\)</span> è l’insieme</p>
<p><span class="math inline">\(\left\lbrace s \in K^n : s = \tilde{s} +
s_0\right\rbrace\)</span> per <span class="math inline">\(s_0\)</span>
soluzione di <span class="math inline">\(AX = 0\)</span></p>
<p>(il sistema <span class="math inline">\(AX = 0\)</span> si dice il
sistema lineare omogeneo associato al sistema <span
class="math inline">\(AX = b\)</span>).</p>
<p><strong>Dimostrazione</strong><br />
<span class="math inline">\(s \in K^n\)</span> è soluzione di <span
class="math inline">\(AX = b\)</span> <span
class="math inline">\(\iff\)</span> <span class="math inline">\(\exists
s_0 \in K^n\)</span> soluzione di <span class="math inline">\(AX =
0\)</span> tale che <span class="math inline">\(s = \tilde{s} +
s_0\)</span></p>
<p>“<span class="math inline">\(\Rightarrow\)</span>”<br />
supponiamo che <span class="math inline">\(s\)</span> sia soluzione di
<span class="math inline">\(AX = 0\)</span></p>
<p>dobbiamo mostrare che esiste <span class="math inline">\(s \in
K^n\)</span> soluzione di <span class="math inline">\(AX = 0\)</span>
tale che <span class="math inline">\(s = \tilde{s} + s_0\)</span>;
definiamo <span class="math inline">\(s_0 = s - \tilde{s}\)</span>;
allora vale che <span class="math inline">\(s = \tilde{s} +
s_0\)</span>; ci resta da verificare che <span
class="math inline">\(s_0\)</span> così ottenuto è soluzione del sistema
lineare omogeneo associato; calcoliamo dunque <span
class="math inline">\(A \cdot s_0\)</span> e verifichiamo che valga
<span class="math inline">\(0\)</span>:</p>
<p><span class="math inline">\(A \cdot s_0 = A(s - \tilde{s}) = As -
A\tilde{s} = b - b = 0\)</span></p>
<p>(per definizione; per la proprietà distributiva; per ipotesi)</p>
<p><span class="math inline">\(\square\)</span></p>
<p>“<span class="math inline">\(\Leftarrow\)</span>”<br />
supponiamo che <span class="math inline">\(\exists s_0 \in K^n\)</span>
soluzione di <span class="math inline">\(AX = 0\)</span> tale che <span
class="math inline">\(s = \tilde{s} + s_0\)</span>; dobbiamo mostrare
che <span class="math inline">\(s\)</span> è soluzione di <span
class="math inline">\(AX = b\)</span>; calcoliamo dunque <span
class="math inline">\(A \cdot s\)</span> e verifichiamo che sia uguale a
<span class="math inline">\(b\)</span>.</p>
<p><span class="math inline">\(A \cdot s = A(\tilde{s} + s_0) =
A\tilde{s} + A s_0 = b + 0 = b\)</span></p>
<p>(per ipotesi; per la proprietà distributiva; per ipotesi)</p>
<p><span class="math inline">\(\square\)</span></p>
<p>Quindi, data una soluzione particolare <span
class="math inline">\(\tilde{s}\)</span> di <span
class="math inline">\(AX = b\)</span>, possiamo scrivere che l’insieme
di tutte le soluzioni di <span class="math inline">\(AX = b\)</span>
è</p>
<p><span class="math inline">\(\left\lbrace \tilde{s} + s_0 : s_0 \text{
soluzione di } AX = 0 \right\rbrace\)</span></p>
<p><strong>Osservazione</strong><br />
Le soluzioni di <span class="math inline">\(AX = b\)</span> formano un
sottospazio vettoriale di <span class="math inline">\(K^n\)</span> se e
solo se <span class="math inline">\(b = 0\)</span>. Infatti</p>
<p>“<span class="math inline">\(\Rightarrow\)</span>”<br />
se le soluzioni di <span class="math inline">\(AX = b\)</span> sono un
sottospazio vettoriale di <span class="math inline">\(K^n\)</span>
allora <span class="math inline">\(0 \in K^n\)</span> è soluzione,
dunque <span class="math inline">\(A \cdot 0 = b\)</span>, pertanto
<span class="math inline">\(b = 0\)</span>.</p>
<p>“<span class="math inline">\(\Leftarrow\)</span>”<br />
se <span class="math inline">\(b = 0\)</span>, allora il sistema è
omogeneo e la tesi segue dal teorema di struttura per sistemi lineari
omogenei.</p>
<p><strong>Esempio</strong><br />
Consideriamo il sistema</p>
<p><span class="math inline">\(x + 2y -3z = -1\)</span> con coefficienti
in <span class="math inline">\(\mathbb{Q}\)</span></p>
<p><span class="math display">\[
A = \begin{pmatrix}
  1 &amp; 2 &amp; -3 \\
\end{pmatrix}
,\
X =
\begin{pmatrix}
  x \\
  y \\
  z \\
\end{pmatrix}
,\
b = (-1)
\]</span></p>
<p>consideriamo <span class="math inline">\(\tilde{s} =
\begin{pmatrix}  -1 \\  0 \\  0 \\ \end{pmatrix}\)</span> soluzione di
<span class="math inline">\(AX = b\)</span></p>
<p>per calcolare tutte le soluzioni di <span class="math inline">\(AX =
b\)</span>,<br />
determiniamo tutte le soluzioni di <span class="math inline">\(AX =
b\)</span>, ovvero di</p>
<p><span class="math inline">\(x +2y -3z = 0\)</span></p>
<p>vediamo che il sistema <span class="math inline">\(AX = 0\)</span> è
equivalente a</p>
<p><span class="math inline">\(x = -2y + 3z\)</span></p>
<p>quindi possiamo assegnare un qualsiasi valore <span
class="math inline">\(u\)</span> ad <span
class="math inline">\(y\)</span> e un qualsiasi valore <span
class="math inline">\(v\)</span> a <span
class="math inline">\(z\)</span>: quindi le soluzioni di <span
class="math inline">\(AX = 0\)</span> si possono scrivere come</p>
<p><span class="math display">\[
\begin{pmatrix}
  -2u + 3v \\
  u \\
  v
\end{pmatrix}
\]</span></p>
<p>per <span class="math inline">\(u, v \in \mathbb{Q}\)</span></p>
<p>notiamo che</p>
<p><span class="math display">\[
\begin{pmatrix}
  -2u + 3v \\
  u \\
  v
\end{pmatrix} = u \cdot
\begin{pmatrix}
  -2 \\
  1 \\
  0
\end{pmatrix} + v \cdot
\begin{pmatrix}
  3 \\
  0 \\
  1
\end{pmatrix}
\]</span></p>
<p>Le soluzioni di <span class="math inline">\(AX = b\)</span> sono
allora:</p>
<p><span class="math display">\[
\begin{pmatrix}
  -1 \\
  0 \\
  0
\end{pmatrix} + u \cdot
\begin{pmatrix}
  -2 \\
  1 \\
  0
\end{pmatrix} + v \cdot
\begin{pmatrix}
  3 \\
  0 \\
  1
\end{pmatrix}, u, v \in \mathbb{Q}
\]</span></p>
<p>Il nostro obbiettivo ora diventa essere in grado si risolvere un
qualsiasi sistema lineare (omogeneo e non). Per cominciare, ci
focalizziamo su un sottoinsieme particolare di sistemi lineari, i
cosiddetti sistemi lineari a scala.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_{n,m}(K)\)</span> e sia <span
class="math inline">\(r \in \left\{0, 1, \dots, m\right\}\)</span> il
numero di righe non nulle di <span class="math inline">\(A\)</span>;
diciamo che <span class="math inline">\(A\)</span> è una <strong>matrice
a scala</strong> se:</p>
<ul>
<li><span class="math inline">\(r = 0\)</span> (ovvero <span
class="math inline">\(A\)</span> è una matrice nulla)</li>
<li><span class="math inline">\(r &gt; 0\)</span> e vale che <span
class="math inline">\(A_{(i)} \neq (0, 0, \dots, 0), \forall i \in
\left\lbrace 1, \dots, r\right\rbrace\)</span> (ovvero le eventuali
righe di <span class="math inline">\(A\)</span> sono “in basso”) ed
inoltre sia <span class="math inline">\(\overline{j}\)</span> l’indice
della prima colonna non nulla e sia <span class="math inline">\(\forall
i \in \left\lbrace 1, \dots, r\right\rbrace\)</span></li>
</ul>
<p><span class="math inline">\(j_i = min \left\lbrace j : a_{ij} \neq 0
\right\rbrace\)</span></p>
<p>allora deve valere che <span class="math inline">\(j_1 &lt; j_2 &lt;
\dots &lt; j_r\)</span></p>
<p>(tutti questi valori sono maggiori o uguali di <span
class="math inline">\(\overline{j}\)</span>);<br />
gli elementi <span class="math inline">\(a_{ij}\)</span> sono detti
elementi di <strong>pivot</strong>.</p>
<figure>
<img src="img/esempi_matrice_a_scala.png" width="350"
alt="esempi di matrici a scala" />
<figcaption aria-hidden="true">esempi di matrici a scala</figcaption>
</figure>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(AX = b\)</span> un sistema lineare dove
<span class="math inline">\(A \in M_{n,m}(K)\)</span> e supponiamo che
<span class="math inline">\(A\)</span> sia a scala, con <span
class="math inline">\(r\)</span> righe non nulle; allora</p>
<p><span class="math inline">\(AX = b\)</span> è compatibile (ovvero
ammette almeno una soluzione) se e solo se <span
class="math inline">\(b_{r+1} = b_{r+2} = \dots = b_m = 0\)</span></p>
<p><strong>Dimostrazione</strong><br />
“<span class="math inline">\(\Rightarrow\)</span>”<br />
Sia <span class="math inline">\(s \in K^n\)</span> una soluzione, ovvero
<span class="math inline">\(s = \begin{pmatrix}  s_1 \\  s_2 \\  \vdots
\\  s_m \end{pmatrix}\)</span> soddisfa <span class="math inline">\(As =
b\)</span></p>
<p>per ipotesi, <span class="math inline">\(A\)</span> è a scala e
dunque le righe <span class="math inline">\(A_{(r + 1)}, \dots,
A_{(m)}\)</span> sono tutte nulle; le corrispondenti equazioni sono
quindi</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    0 \cdot x_1 + 0 \cdot x_2 + \dots + 0 \cdot x_n = b_{r+1} \\
    \vdots \\
    0 \cdot x_1 + 0 \cdot x_2 + \dots + 0 \cdot x_n = b_{m}
  \end{array}
\right. \Rightarrow
\left\lbrace
  \begin{array}{l}
    0 \cdot s_1 + 0 \cdot s_2 + \dots + 0 \cdot s_n = b_{r+1} \\
    \vdots \\
    0 \cdot s_1 + 0 \cdot s_2 + \dots + 0 \cdot s_n = b_{m}
  \end{array}
\right. \Rightarrow
\left\lbrace
  \begin{array}{l}
    b_{r+1} = 0 \\
    \vdots \\
    b_{m} = 0
  \end{array}
\right.
\]</span></p>
<p>“<span class="math inline">\(\Leftarrow\)</span>”<br />
Supponiamo che <span class="math inline">\(b_{r+1} = \dots = b_m =
0\)</span>, costruiamo una soluzione <span class="math inline">\(s =
\begin{pmatrix} s_1 \\ \vdots \\ s_n \end{pmatrix}\)</span> di <span
class="math inline">\(AX = b\)</span>; dato che <span
class="math inline">\(A\)</span> è a scala, le ultime righe di <span
class="math inline">\(A\)</span> sono nulle e quindi le ultime equazioni
del sistema sono del tipo <span class="math inline">\(0 = 0\)</span>;
l’ultima equazione non identicamente nulla è quella data dalla riga
<span class="math inline">\(r\)</span>-esima di <span
class="math inline">\(A\)</span>:</p>
<p><span class="math inline">\(0 \neq a_{r,j_{r}} \cdot x_{j_{r}} +
a_{r,j_{r}+1} \cdot x_{j_{r}+1} + \dots + a_{r,n} \cdot x_{n} =
b_{r}\)</span></p>
<p>dove <span class="math inline">\(a_{r,j_{r}} \neq 0\)</span> perché
abbiamo scelto <span class="math inline">\(j_r\)</span> e tale che<br />
<span class="math inline">\(j_r = min \{j : a_{r,j} \neq
0\}\)</span></p>
<figure>
<img src="img/ultimo_pivot.png" width="300"
alt="l’ultimo elemento di pivot" />
<figcaption aria-hidden="true">l’ultimo elemento di pivot</figcaption>
</figure>
<p>a questo punto scegliamo valori <span class="math inline">\(s_{j_{r +
1}}, \dots, s_n \in K\)</span> a piacimento e definiamo</p>
<p><span class="math inline">\(s_{j_r} := \dfrac{b_{r} - (a_{r,j_{r}+1}
\cdot s_{j_{r}+1} + \dots + a_{r,n} \cdot s_n)}{a_{r,j_r}}\)</span></p>
<p>scegliendo i valori <span class="math inline">\(s_{j_r}, \dots,
s_n\)</span> in questa, la soluzione che stiamo costruendo soddisferà
l’ultima equazione:</p>
<p>consideriamo ora la penultima equazione non nulla:</p>
<p><span class="math inline">\(a_{r-1,j_{r-1}} \cdot x_{j_{r-1}} +
a_{r-1,j_{r-1}+1} \cdot x_{j_{r-1}+1} + \dots + a_{r-1,n} \cdot x_{n} =
b_{r-1}\)</span></p>
<p>dato che <span class="math inline">\(A\)</span> è a scala. abbiamo
che <span class="math inline">\(j_{r-1} &lt; j_{r}\)</span></p>
<figure>
<img src="img/pivot_minore.png" width="300"
alt="il pivot della riga r-1 è minore di quello della riga r" />
<figcaption aria-hidden="true">il pivot della riga r-1 è minore di
quello della riga r</figcaption>
</figure>
<p>ora possiamo scegliere a nostro piacimento dei valori</p>
<p><span class="math inline">\(s_{j_{r-1}+1}, \dots, s_{j_{r}-1} \in
K\)</span></p>
<p><span class="math inline">\(s_{j_{r-1}} = \dfrac{b_{r-1} -
(a_{r-1,j_{r-1}+1} \cdot s_{j_{r-1}+1} + \dots + a_{r-1,n} \cdot
s_n)}{a_{r-1,j_{r-1}}}\)</span></p>
<p>e in questa maniera abbiamo determinato valori <span
class="math inline">\(s_{j_{r-1}}, \dots, s_n\)</span> in modo che la
soluzione che otterremo soddisfi le ultime due equazioni non nulle.</p>
<p>A questo punto, ripetiamo lo stesso processo per tutte le altre
righe.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Esempio</strong> <span class="math display">\[
A = \begin{pmatrix}
  \begin{array}{rrrr}
    1 &amp; -1 &amp; 1 &amp; 1 \\
    0 &amp; -1 &amp; 3 &amp; 2 \\
    0 &amp; 0 &amp; 0 &amp; 1 \\
    0 &amp; 0 &amp; 0 &amp; 0 \\
  \end{array}
\end{pmatrix} \in M_{4,6}(\mathbb{R})
\]</span></p>
<p><span class="math inline">\(b_4\)</span> deve essere <span
class="math inline">\(0\)</span></p>
<p><span class="math display">\[
b = \begin{pmatrix}
  1 \\
  1 \\
  1 \\
  0
\end{pmatrix}
\]</span></p>
<p>Le equazioni non nulle sono:</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{rrrrrrrl}
    x_1&amp; - x_2&amp; + x_3&amp; + x_4&amp; +2x_5&amp; +x_6&amp;
=&amp; 1 \\
    &amp; - x_2&amp; + 3x_3&amp; + 2x_4&amp; +x_5&amp; -x_6&amp; =&amp;
1 \\
    &amp; &amp; &amp; x_4&amp; +2x_5&amp; +x_6&amp; =&amp; 1
  \end{array}
\right.
\]</span></p>
<p>Partiamo dall’ultima equazione<br />
<span class="math inline">\(x_4 = 1 - 2x_5 - x_6\)</span></p>
<p>Scegliamo ad esempio <span class="math inline">\(s_5 = 1\)</span>,
<span class="math inline">\(s_6 = 0\)</span> e otteniamo<br />
<span class="math inline">\(s_4 = 1 - 2s_5 - s_6 = -1\)</span></p>
<p>Passiamo alla penultima equazione<br />
<span class="math inline">\(x_2 = -1 + 3x_3 + 2x_4 + x_5 -
x_6\)</span></p>
<p>Scegliamo a piacere <span class="math inline">\(s_3 = -1\)</span>,
otteniamo<br />
<span class="math inline">\(s_2 = -1 + 3s_3 + 2s_4 + s_5 - s_6 = -1 +
3(-1) + 2(-1) + 1 - 0 = -1 -3 -2 +1 = -5\)</span></p>
<p>Ci resta la prima equazione<br />
<span class="math inline">\(x_1 = 1 + x_2 - x_3 - x_4 - 2x_5 -
x_6\)</span></p>
<p><span class="math inline">\(s_1 = 1 + s_2 - s_3 - s_4 - 2s_5 - s_6 =
1 -5 -(-1) -(-1) -2(-1) - 0 = 1 -5 +1 +1 -2 = -4\)</span></p>
<p>Perciò la soluzione del sistema <span
class="math inline">\(AX=b\)</span> risulta</p>
<p><span class="math display">\[
s = \begin{pmatrix}
  -4 \\
  -5 \\
  -1 \\
  -1 \\
  1 \\
  0
\end{pmatrix}
\]</span></p>
<p><strong>Definizione</strong><br />
Siamo due sistemi lineari <span class="math inline">\(AX = b\)</span> e
<span class="math inline">\(A&#39;X = b&#39;\)</span> con</p>
<p><span class="math inline">\(A \in M_{n,m}(K)\)</span> e <span
class="math inline">\(b \in K^m\)</span><br />
<span class="math inline">\(A&#39; \in M_{m&#39;,n}(K)\)</span> e <span
class="math inline">\(b&#39; \in K^{m&#39;}\)</span></p>
<p>(quindi i due sistemi hanno lo steso numero di incognite, ma possono
avere un numero diverso di equazioni)</p>
<p>si dicono <strong>equivalenti</strong> se hanno le medesime
soluzioni.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(AX = b\)</span> un sistema lineare,
allora la matrice ottenuta aggiungendo ad <span
class="math inline">\(A\)</span> la colonna data da <span
class="math inline">\(b\)</span>, ovvero <span
class="math inline">\((A|b)\)</span> è detta la <strong>matrice
completa</strong> del sistema <span class="math inline">\(AX =
b\)</span>.</p>
<p><strong>Esempio</strong><br />
Dato il sistema</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    2x_1 + x_2 = 3 \\
    -x_1 + 2x_2 = 5
  \end{array}
\right.
\]</span></p>
<p>Le due matrici sono</p>
<p><span class="math display">\[
A = \begin{pmatrix}
  \begin{array}{rr}
    2 &amp; -1 \\
    -1 &amp; 2
  \end{array}
\end{pmatrix},\
b = \begin{pmatrix}
  3 \\
  5
\end{pmatrix}
\]</span></p>
<p>Quindi la matrice completa è</p>
<p><span class="math display">\[
(A|b) =
\begin{pmatrix}
  \begin{array}{rrr}
    2 &amp; -1 &amp; 3 \\
    -1 &amp; 2 &amp; 5
  \end{array}
\end{pmatrix}
\]</span></p>
<hr />
<p>Introduciamo tre <strong>operazioni elementari</strong> che
trasformano un sistema lineare in un sistema lineare equivalente:</p>
<p><strong>OE1</strong>:<br />
Scambio di equazioni del sistema.<br />
Dati <span class="math inline">\(i,j \in \{1, \dots, m\}\)</span>,
scambiamo di posto la <span class="math inline">\(i\)</span>-esima e
l’equazione <span class="math inline">\(j\)</span>-esima. Equivale a
scambiare due righe della matrice completa.</p>
<p><strong>OE2</strong>:<br />
Moltiplicazione di un’equazione per uno scalare non nullo.<br />
Dati <span class="math inline">\(i \in \{1, \dots, m\}\)</span> e <span
class="math inline">\(\lambda \in K\)</span>, moltiplichiamo l’equazione
<span class="math inline">\(i\)</span>-esima per <span
class="math inline">\(\lambda\)</span>.<br />
Equivale a moltiplicare per <span class="math inline">\(\lambda\)</span>
l’equazione <span class="math inline">\(i\)</span>-esima della matrice
completa.</p>
<p><strong>OE3</strong>:<br />
Somma ad un’equazione un multiplo di un’altra equazione.<br />
Dati <span class="math inline">\(i,j \in \{1, \dots, m\}\)</span> e
<span class="math inline">\(\lambda \in K\)</span>, sommiamo
all’equazione <span class="math inline">\(i\)</span>-esima la <span
class="math inline">\(j\)</span>-esima equazione, dopo aver moltiplicato
quest’ultima per <span class="math inline">\(\lambda\)</span>.<br />
Equivale a sommare alla riga <span
class="math inline">\(i\)</span>-esima dalla matrice completa, <span
class="math inline">\(\lambda\)</span> volte l’equazione <span
class="math inline">\(j\)</span>-esima.</p>
<p><strong>Proposizione</strong><br />
Se applichiamo a un sistema lineare <span class="math inline">\(AX =
b\)</span> una delle tre operazioni elementari, OE1, OE2, OE3, otteniamo
un sistema lineare equivalente.</p>
<p>Se mostriamo che possiamo trasformare un sistema lineare in uno
equivalente a scala tramite queste tre operazioni, saremo quindi in
grado si calcolare soluzioni di un qualsiasi sistema lineare.</p>
<h2 id="algoritmo-di-gauss-di-gradinizzazione">Algoritmo di Gauss (di
gradinizzazione)</h2>
<p><em>Input</em>: matrice completa <span
class="math inline">\((A|b)\)</span> di un sistema lineare.<br />
<em>Output</em>: matrice completa <span
class="math inline">\((\tilde{A}|\tilde{b})\)</span> tale che <span
class="math inline">\(\tilde{A}\)</span> è a scala e <span
class="math inline">\(\tilde{A}X = \tilde{b}\)</span> è equivalente a
<span class="math inline">\(AX = b\)</span>.</p>
<p><span class="math inline">\(1)\)</span> Determino <span
class="math inline">\(\overline{j}\)</span> indice colonna minima per
cui abbiamo una colonna non nulla di <span
class="math inline">\(A\)</span>.</p>
<p><span class="math inline">\(\overline{j} = min \left\lbrace j :
A^{(j)} \neq 0\right\rbrace\)</span></p>
<p><span class="math display">\[
A =
\begin{pmatrix}
  0 &amp; 0 &amp; \underline{*} &amp; \dots &amp; * \\
  0 &amp; 0 &amp; * &amp; \dots &amp; * \\
  0 &amp; 0 &amp; * &amp; \dots &amp; * \\
\end{pmatrix}
\]</span></p>
<p><span class="math inline">\(2)\)</span> Determino un indice <span
class="math inline">\(\overline{i}\)</span> tale per cui l’elemento
<span class="math inline">\(a_{ij}\)</span> è non nullo (l’esistenza di
un tale <span class="math inline">\(\overline{i}\)</span> dipende dalla
scelta di <span class="math inline">\(\overline{j}\)</span>).</p>
<p><span class="math inline">\(3)\)</span> Scambio le righe <span
class="math inline">\(\overline{i}\)</span> e <span
class="math inline">\(1\)</span>; in questo modo posso supporre che
l’elemento <span class="math inline">\(a_{1j}\)</span> sia non
nullo.</p>
<p><span class="math display">\[
\begin{pmatrix}
  0 &amp; 0 &amp; a_{1j} &amp; * &amp; * \\
  0 &amp; 0 &amp; \underline{*} &amp; * &amp; * \\
  0 &amp; 0 &amp; \underline{*} &amp; * &amp; * \\
\end{pmatrix}
\]</span></p>
<p><span class="math inline">\(4)\)</span> Moltiplico la prima riga per
<span class="math inline">\(\frac{1}{a_{1j}}\)</span>.</p>
<p><span class="math display">\[
\begin{pmatrix}
  0 &amp; 0 &amp; 1 &amp; * &amp; * \\
  0 &amp; 0 &amp; \underline{*} &amp; * &amp; * \\
  0 &amp; 0 &amp; \underline{*} &amp; * &amp; * \\
\end{pmatrix}
\]</span></p>
<p><span class="math inline">\(5)\)</span> Per ogni <span
class="math inline">\(i \in \{2, \dots, m\}\)</span>, sommo alla riga
<span class="math inline">\(i\)</span>-esima un opportuno multiplo della
prima riga; più precisamente, sostituisco l’<span
class="math inline">\(i\)</span>-esima riga con</p>
<p><span class="math inline">\(A_{(i)} - a_{i\overline{j}}
A_{(1)}\)</span></p>
<p><span class="math display">\[
\begin{pmatrix}
  0 &amp; 0 &amp; 1 &amp; * &amp; * \\
  0 &amp; 0 &amp; 0 &amp; * &amp; * \\
  0 &amp; 0 &amp; 0 &amp; * &amp; * \\
\end{pmatrix}
\]</span></p>
<p><span class="math inline">\(6)\)</span> Ripeto il procedimento
precedente sulla sottomatrice con righe <span class="math inline">\(\{2,
\dots, m\}\)</span> e colonne <span class="math inline">\(\{\overline{j}
+ 1, \dots, n\}\)</span>.</p>
<p><span class="math display">\[
\begin{pmatrix}
  0 &amp; 0 &amp; 1 &amp; * &amp; * \\
  0 &amp; 0 &amp; 0 &amp; \underline{*} &amp; \underline{*} \\
  0 &amp; 0 &amp; 0 &amp; \underline{*} &amp; \underline{*} \\
\end{pmatrix}
\]</span></p>
<p>Questo algoritmo termina in un tempo finito e restituisce un
risultato che rispetta le prescrizioni della specificazione.</p>
<p><strong>Esempio</strong></p>
<figure>
<img src="img/esempio_gauss.png" width="500"
alt="esempio dell’algoritmo di Gauss" />
<figcaption aria-hidden="true">esempio dell’algoritmo di
Gauss</figcaption>
</figure>
<h1 id="sistemi-di-generatori-e-indipendenza-lineare">Sistemi di
generatori e indipendenza lineare</h1>
<p><strong>Lemma</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span>; siano <span
class="math inline">\(U, W \in V\)</span> sottospazi vettoriali; allora
<span class="math inline">\(U \cap W\)</span> è uno sottospazio
vettoriale di <span class="math inline">\(V\)</span>.</p>
<p><strong>Dimostrazione</strong><br />
Verifichiamo che <span class="math inline">\(U \cap W\)</span> soddisfa
le tre proprietà di sottospazio vettoriale:</p>
<ol type="1">
<li>mostriamo che <span class="math inline">\(0 \in U \cap W\)</span>;
dato che <span class="math inline">\(U\)</span> e <span
class="math inline">\(W\)</span> sono sottospazi vettoriali, allora
<span class="math inline">\(0 \in U\)</span> e <span
class="math inline">\(0 \in W\)</span>; quindi <span
class="math inline">\(0 \in U \cap W\)</span>.<br />
</li>
<li>mostriamo che se <span class="math inline">\(v_1, v_2 \in U \cap
W\)</span>, allora <span class="math inline">\(v_1 + v_2 \in U \cap
W\)</span>; supponiamo che <span class="math inline">\(v_1, v_2 \in U
\cap W\)</span>; allora <span class="math inline">\(v_1, v_2 \in
U\)</span> e <span class="math inline">\(v_1, v_2 \in W\)</span>; dato
che <span class="math inline">\(U\)</span> e <span
class="math inline">\(W\)</span> sono sottospazi vettoriali, allora
<span class="math inline">\(v_1 + v_2 \in U\)</span> e <span
class="math inline">\(v_1 + v_2 \in W\)</span>; quindi <span
class="math inline">\(v_1 + v_2 \in U \cap W\)</span>.</li>
<li>mostriamo che se <span class="math inline">\(v \in U \cap W\)</span>
e <span class="math inline">\(\lambda \in K\)</span>, allora <span
class="math inline">\(\lambda \cdot v \in U \cap W\)</span>;
consideriamo quindi <span class="math inline">\(\lambda \in K\)</span> e
<span class="math inline">\(v \in U \cap W\)</span>; allora <span
class="math inline">\(v \in U\)</span> e <span class="math inline">\(v
\in W\)</span>; dato che <span class="math inline">\(U\)</span> e <span
class="math inline">\(W\)</span> sono sottospazi vettoriali, allora
<span class="math inline">\(\lambda \cdot v \in U\)</span> e <span
class="math inline">\(\lambda \cdot v \in W\)</span>; quindi <span
class="math inline">\(\lambda \cdot v \in U \cap W\)</span>.</li>
</ol>
<p><strong>Osservazione</strong><br />
Non è vero che se <span class="math inline">\(V\)</span> è spazio
vettoriale su <span class="math inline">\(K\)</span> e <span
class="math inline">\(U, W \in V\)</span> sono sottospazi vettoriali,
allora <span class="math inline">\(U \cup W\)</span> è uno sottospazio
vettoriale di <span class="math inline">\(V\)</span> (non soddisfa la
somma).</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span>; siano <span
class="math inline">\(U, W \in V\)</span> sottospazi vettoriali;
definiamo:</p>
<p><span class="math inline">\(U + W = \{u + w : u \in U, w \in
W\}\)</span></p>
<p>e chiamiamo questo insieme il sottospazio vettoriale somma di <span
class="math inline">\(U\)</span> e <span
class="math inline">\(W\)</span>.</p>
<p><strong>Lemma</strong><br />
<span class="math inline">\(U + W\)</span> è uno sottospazio vettoriale
di <span class="math inline">\(V\)</span>.</p>
<p><strong>Dimostrazione</strong><br />
Per esercizio.</p>
<p><strong>Lemma</strong><br />
Con la notazione precedente, vale che <span class="math inline">\(U
\subseteq U + W\)</span> e <span class="math inline">\(W \subseteq U +
W\)</span>.</p>
<p><strong>Dimostrazione</strong><br />
Mostrare che <span class="math inline">\(U \subseteq U + W\)</span>
significa mostrare che <span class="math inline">\(\forall u \in
U\)</span>, vale che <span class="math inline">\(u \in U + W\)</span> (e
analogamente per gli elementi di <span
class="math inline">\(W\)</span>); per farlo, dato <span
class="math inline">\(u \in U\)</span>, dobbiamo mostrare che <span
class="math inline">\(u\)</span> si può scrivere come somma di un
elemento di <span class="math inline">\(U\)</span> e di un elemento di
<span class="math inline">\(W\)</span>; ora <span
class="math inline">\(u = u + 0\)</span> quindi <span
class="math inline">\(u \in U + W\)</span>.</p>
<p><strong>Corollario</strong><br />
<span class="math inline">\(U \cup W \in U + W\)</span>; inoltre si può
dimostrare che <span class="math inline">\(U + W\)</span> è il più
piccolo sottospazio di <span class="math inline">\(V\)</span> che
contiene <span class="math inline">\(U \cup W\)</span>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span>; siano <span
class="math inline">\(v_1, \dots, v_n \in V\)</span>; allora una
<strong>combinazione lineare</strong> di <span
class="math inline">\(v_1, \dots, v_n\)</span> è un qualsiasi vettore
della forma <span class="math inline">\(\lambda_1 \cdot v_1 + \dots +
\lambda_n \cdot v_n\)</span>, dove <span
class="math inline">\(\lambda_1, \dots, \lambda_n \in K\)</span>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span> e siano <span
class="math inline">\(v_1, \dots, v_n \in V\)</span>; definiamo<br />
<span class="math inline">\(span(v_1, \dots, v_n) := \{\lambda_1 \cdot
v_1 + \dots + \lambda_n \cdot v_n : \lambda_1, \dots, \lambda_n \in
K\}\)</span> = {combinazioni lineari di <span class="math inline">\(v_1,
\dots, v_n\)</span>}</p>
<p><strong>Lemma</strong><br />
<span class="math inline">\(span(v_1, \dots, v_n)\)</span> è uno
sottospazio vettoriale di <span class="math inline">\(V\)</span>.</p>
<p><strong>Dimostrazione</strong><br />
<span class="math inline">\(1)\)</span> <span class="math inline">\(0 =
0 \cdot v_1 + \dots + 0 \cdot v_n\)</span>, dunque <span
class="math inline">\(0 \in span(v_1, \dots, v_n)\)</span></p>
<p><span class="math inline">\(2)\)</span> siano <span
class="math inline">\(u, w \in span(v_1, \dots, v_n)\)</span>, dobbiamo
mostrare che <span class="math inline">\(u + w \in span(v_1, \dots,
v_n)\)</span>; per ipotesi <span class="math inline">\(u =
\displaystyle\sum_{i = 1}^n \lambda_i v_i\)</span>, con <span
class="math inline">\(\lambda_i \in K\)</span> e <span
class="math inline">\(W = \displaystyle\sum_{i = 1}^n \mu_i
w_i\)</span>, con <span class="math inline">\(\mu_i \in K\)</span>;
allora <span class="math inline">\(u + w = \displaystyle\sum_{i = 1}^n
(\lambda_i + \mu_i) v_i\)</span>, dunque <span class="math inline">\(u +
w \in span(v_1, \dots, v_n)\)</span></p>
<p><span class="math inline">\(3)\)</span> siano <span
class="math inline">\(u \in span(v_1, \dots, v_n)\)</span> e <span
class="math inline">\(\lambda \in K\)</span>, devo mostrare che <span
class="math inline">\(\lambda \cdot u \in span(v_1, \dots,
v_n)\)</span>; per ipotesi <span class="math inline">\(u = \lambda \cdot
v_1 + \dots + \lambda \cdot v_n\)</span>, con <span
class="math inline">\(\lambda \in K\)</span>; allora <span
class="math inline">\(\lambda \cdot u = (\lambda \cdot \lambda_1) \cdot
v_1 + \dots + (\lambda \cdot \lambda_n) \cdot v_n\)</span>, dunque <span
class="math inline">\(\lambda \cdot u \in span(v_1, \dots,
v_n)\)</span></p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span> e sia <span
class="math inline">\(U \in V\)</span> uno sottospazio vettoriale; un
insieme di elementi <span class="math inline">\(\{u_1, \dots, u_n\}
\subseteq U\)</span> si dicono un <strong>sistema di generatori</strong>
di/per <span class="math inline">\(U\)</span> se ogni vettore <span
class="math inline">\(u \in U\)</span> è combinazione lineare di <span
class="math inline">\(u_1, \dots, u_n\)</span>; equivalentemente <span
class="math inline">\(\{u_1, \dots, u_n\}\)</span> è un sistema di
generatori di <span class="math inline">\(U\)</span> se e solo se <span
class="math inline">\(span(u_1, \dots, u_n) = U\)</span>.</p>
<p><strong>Osservazione</strong><br />
Se <span class="math inline">\(\{u_1, \dots, u_n\}\)</span> è un sistema
di generatori di uno sottospazio vettoriale <span
class="math inline">\(U\)</span>, allora per ogni <span
class="math inline">\(u \in U\)</span> vale che <span
class="math inline">\(\{u_1, \dots, u_n, u\}\)</span> è anch’esso un
sistema di generatori di <span class="math inline">\(U\)</span>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale e siano
<span class="math inline">\(v_1, \dots, v_n \in V\)</span>; gli elementi
<span class="math inline">\(v_1, \dots, v_n\)</span> si dicono
<strong>linearmente dipendenti</strong> se possiamo scrivere <span
class="math inline">\(0 \in V\)</span> come una combinazione lineare di
<span class="math inline">\(v_1, \dots, v_n\)</span> in cui non tutti i
coefficienti in <span class="math inline">\(K\)</span> sono nulli,
ovvero se vale che <span class="math inline">\(0 = \lambda_1 \cdot v_1 +
\dots + \lambda_n \cdot v_n\)</span>, con <span
class="math inline">\(\lambda_1, \dots, \lambda_n \in K\)</span> non
tutti nulli.</p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span> e siano <span
class="math inline">\(v_1, \dots, v_n \in V\)</span>; allora <span
class="math inline">\(v_1, \dots, v_n\)</span> si sono
<strong>linearmente dipendenti</strong> se e solo se uno di essi può
essere scritto come combinazione lineare degli altri [equivalentemente,
se e solo se esiste <span class="math inline">\(j \in \{1, \dots,
n\}\)</span> tale che <span class="math inline">\(v_j \in span(v_1,
\dots, v_ {j - 1}, v_{j + 1}, \dots, v_n)\)</span>, indicato anche come
<span class="math inline">\(span(v_1, \dots, \hat{v}_j, \dots,
v_n)\)</span>].</p>
<p><strong>Dimostrazione</strong><br />
“<span class="math inline">\(\Rightarrow\)</span>”<br />
Supponiamo che <span class="math inline">\(v_1, \dots, v_n\)</span>
siano linearmente dipendenti; allora <span
class="math inline">\(\lambda_1 \cdot v_1 + \dots + \lambda_n \cdot v_n
= 0\)</span> con <span class="math inline">\(\lambda_1, \dots, \lambda_n
\in K\)</span> non tutti nulli; allora esiste <span
class="math inline">\(j \in \{1, \dots, n\}\)</span> tale che <span
class="math inline">\(\lambda_j \neq 0\)</span>, allora vale che<br />
<span class="math inline">\(-\lambda_j \cdot v_j = \lambda_1 \cdot v_1 +
\dots + \lambda_{j - 1} \cdot v_{j - 1} + \lambda_{j + 1} \cdot v_{j +
1} + \dots + \lambda_n \cdot v_n\)</span><br />
e quindi<br />
<span class="math inline">\(v_j =
\Big(-\dfrac{\lambda_1}{\lambda_j}\Big) \cdot v_1 + \dots +
\Big(-\dfrac{\lambda_{j - 1}}{\lambda_j}\Big) \cdot v_{j - 1} + \dots +
\Big(-\dfrac{\lambda_{j + 1}}{\lambda_j}\Big) \cdot v_{j + 1} + \dots +
\Big(-\dfrac{\lambda_n}{\lambda_j}\Big) \cdot v_n\)</span><br />
ovvero <span class="math inline">\(v_j \in span(v_1, \dots, \hat{v}_j,
\dots, v_n)\)</span></p>
<p>“<span class="math inline">\(\Leftarrow\)</span>”<br />
Supponiamo che esista un <span class="math inline">\(j \in \{1, \dots,
n\}\)</span> tale che <span class="math inline">\(v_j \in span(v_1,
\dots, \hat{v}_j, \dots, v_n)\)</span>; allora<br />
<span class="math inline">\(v_j = \mu_1 \cdot v_1 + \dots + \mu_{j - 1}
\cdot v_{j - 1} + \mu_{j + 1} \cdot v_{j + 1} + \dots + \mu_n \cdot
v_n\)</span><br />
allora<br />
<span class="math inline">\(\mu_1 \cdot v_1 + \dots + \mu_{j - 1} \cdot
v_{j - 1} + v_j + \mu_{j + 1} \cdot v_{j + 1} + \dots + \mu_n \cdot v_n
= 0\)</span><br />
e il coefficiente di <span class="math inline">\(v_j\)</span> è <span
class="math inline">\(1\)</span>, dunque è diverso da zero, pertanto
<span class="math inline">\(v_1, \dots, v_n\)</span> sono linearmente
dipendenti.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span> e siano <span
class="math inline">\(v_1, \dots, v_n \in V\)</span>; diciamo che <span
class="math inline">\(v_1, \dots, v_n\)</span> sono <strong>linearmente
indipendenti</strong> se non sono linearmente dipendenti;
equivalentemente <span class="math inline">\(v_1, \dots, v_n\)</span>
sono linearmente indipendenti se e solo se l’unico modo di scrivere
<span class="math inline">\(0\)</span> come combinazione lineare di
<span class="math inline">\(v_1, \dots, v_n\)</span> è quello di usare
tutti i coefficienti nulli; equivalentemente, <span
class="math inline">\(v_1, \dots, v_n\)</span> sono linearmente
indipendenti se, dal supporre che valga <span
class="math inline">\(\lambda_1 \cdot v_1 + \dots + \lambda_n \cdot v_n
= 0\)</span>, discende che <span class="math inline">\(\lambda_1 = 0,
\dots, \lambda_n = 0\)</span>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span> e sia <span
class="math inline">\(U \subseteq V\)</span> un sottospazio vettoriale;
una <strong>base</strong> di <span class="math inline">\(U\)</span> è un
insieme di vettori <span class="math inline">\(u_1, \dots, u_n \in
U\)</span> tali che:</p>
<ol type="1">
<li><span class="math inline">\(\{u_1, \dots, u_n\}\)</span> sono un
sistema di generatori di <span class="math inline">\(U\)</span></li>
<li><span class="math inline">\(\{u_1, \dots, u_n\}\)</span> sono
linearmente indipendenti</li>
</ol>
<h1 id="basi-di-spazi-vettoriali">Basi di spazi vettoriali</h1>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span>; se esiste un sistema di
generatori <span class="math inline">\(\{v_1\ \dots, v_n\}\)</span>
finito di <span class="math inline">\(V\)</span>, allora <span
class="math inline">\(V\)</span> si dice <strong>finitamente
generato</strong>.</p>
<p><strong>Teorema</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span> finitamente generato; un
sottoinsieme <span class="math inline">\(B \subseteq V\)</span>, <span
class="math inline">\(B = \{v_1\ \dots, v_n\}\)</span> è una base di
<span class="math inline">\(V\)</span> se e solo se <span
class="math inline">\(v \in V\)</span> si può scrivere in modo unico
come combinazione lineare di <span class="math inline">\(B\)</span>.</p>
<p><strong>Dimostrazione</strong><br />
“<span class="math inline">\(\Rightarrow\)</span>”<br />
Sia <span class="math inline">\(B\)</span> una base di <span
class="math inline">\(V\)</span>, devo dimostrare che ogni <span
class="math inline">\(v \in V\)</span> si può scrivere come combinazione
lineare di <span class="math inline">\(B\)</span> in modo unico; sia
<span class="math inline">\(v \in V\)</span>; dato che <span
class="math inline">\(B\)</span> è in particolare un sistema di
generatori per <span class="math inline">\(V\)</span>, allora <span
class="math inline">\(v\)</span> si scrive come combinazione lineare di
<span class="math inline">\(B\)</span>, <span class="math inline">\(v =
\lambda v_1 + \dots + \lambda v_n\)</span>, dobbiamo mostrare l’unicità
di tale scrittura; supponiamo che ne esista un’altra</p>
<p><span class="math inline">\(v = \mu_1 v_1 + \dots + \mu_n v_n, \mu_i
\in K\)</span></p>
<p>allora <span class="math inline">\(\lambda_1 v_1 + \dots + \lambda_n
= \mu_1 v_1 + \dots + \mu_n v_n\)</span>, pertanto</p>
<p><span class="math inline">\((\lambda_1 + \mu_1) v_1 + \dots +
(\lambda_n + \mu_n) v_n = 0\)</span></p>
<p>questa è una combinazione lineare nulla di <span
class="math inline">\(v_1, \dots, v_n\)</span>; dato che <span
class="math inline">\(B\)</span> è linearmente indipendente, l’unica
possibilità è che valga</p>
<p><span class="math inline">\(\lambda_1 - \mu_1 = \dots = \lambda_n -
\mu_n = 0\)</span> se e solo se <span class="math inline">\(\lambda_1 =
\mu_1, \dots, \lambda_n = \mu_n\)</span></p>
<p>il che prova l’unicità della scrittura.</p>
<p>“<span class="math inline">\(\Leftarrow\)</span>”</p>
<p>supponiamo che ogni <span class="math inline">\(v \in V\)</span> si
scriva come unica combinazione lineare di <span
class="math inline">\(B\)</span>; allora in particolare <span
class="math inline">\(B\)</span> è un sistema di generatori per <span
class="math inline">\(V\)</span>; dimostriamo che gli elementi di <span
class="math inline">\(B\)</span> sono linearmente indipendenti; per
farlo, supponiamo che esista una combinazione lineare nulla di <span
class="math inline">\(v_1, \dots, v_n\)</span></p>
<p><span class="math inline">\(\lambda_1 v_1 + \dots + \lambda_n v_n =
0\)</span></p>
<p>d’altra parte, possiamo scrivere <span class="math inline">\(0 = 0
\cdot v_1 + \dots + 0 \cdot v_n\)</span>; dato che la scrittura di <span
class="math inline">\(0\)</span> come combinazione lineare di <span
class="math inline">\(\{v_1, \dots, v_n\}\)</span> è unica, discende che
<span class="math inline">\(\lambda_1 = \dots = \lambda_n = 0\)</span>,
ovvero che <span class="math inline">\(v_1, \dots, v_n\)</span> sono
linearmente indipendenti.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span> finitamente generato; sia <span
class="math inline">\(B = \{v_1\ \dots, v_n\}\)</span> una base di <span
class="math inline">\(V\)</span> e sia <span class="math inline">\(v \in
V\)</span>; allora possiamo scrivere <span class="math inline">\(v =
\lambda_1 v_1 + \dots + \lambda_n v_n\)</span> in modo unico con <span
class="math inline">\(\lambda_1, \dots, \lambda_n \in K\)</span>; gli
scalari <span class="math inline">\(\lambda_1, \dots, \lambda_n\)</span>
sono detti le <strong>coordinate</strong> di <span
class="math inline">\(V\)</span> rispetto a <span
class="math inline">\(B\)</span>.</p>
<p><strong>Esempi</strong><br />
In <span class="math inline">\(K^n\)</span> possiamo considerare</p>
<p><span class="math display">\[
B =
\left\lbrace
  \begin{pmatrix}
    1 \\
    0 \\
    \vdots \\
    0
  \end{pmatrix},
  \begin{pmatrix}
    0 \\
    1 \\
    \vdots \\
    0
  \end{pmatrix},
  \dots,
  \begin{pmatrix}
    0 \\
    0 \\
    \vdots \\
    1
  \end{pmatrix}
\right\rbrace
\]</span></p>
<p>si può dimostrare che <span class="math inline">\(B\)</span> è una
base di <span class="math inline">\(K^n\)</span>; tale base è chiamata
la <strong>base standard</strong> di <span
class="math inline">\(K^n\)</span>; per ogni vettore</p>
<p><span class="math display">\[
v =
\begin{pmatrix}
  v_1 \\
  v_2 \\
  \vdots \\
  v_n
\end{pmatrix}
\]</span></p>
<p>le coordinate di <span class="math inline">\(v\)</span> rispetto alla
base standard sono <span class="math inline">\(v_1, \dots,
v_n\)</span></p>
<p><strong>Esempio</strong><br />
In <span class="math inline">\(M_{m, n}(K)\)</span> possiamo
considerare</p>
<p><span class="math display">\[
B =
\left\lbrace
  \begin{pmatrix}
    1 &amp; 0 &amp; 0 &amp; \dots \\
    0 &amp; 0 &amp; 0 &amp; \dots \\
    0 &amp; 0 &amp; 0 &amp; \dots \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots
  \end{pmatrix},
  \begin{pmatrix}
    0 &amp; 1 &amp; 0 &amp; \dots \\
    0 &amp; 0 &amp; 0 &amp; \dots \\
    0 &amp; 0 &amp; 0 &amp; \dots \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots
  \end{pmatrix},
  \dots,
  \begin{pmatrix}
    \ddots &amp; \vdots &amp; \vdots &amp; \vdots \\
    \dots &amp; 0 &amp; 0 &amp; 0 \\
    \dots &amp; 0 &amp; 0 &amp; 0 \\
    \dots &amp; 0 &amp; 0 &amp; 1
  \end{pmatrix}
\right\rbrace
\]</span></p>
<p>si può dimostrare che <span class="math inline">\(B\)</span> è una
base di <span class="math inline">\(M_{m, n}(K)\)</span>.</p>
<p><strong>Osservazione</strong><br />
In <span class="math inline">\(K^n\)</span>, l’essere un sistema di
generatori può essere parafrasato in termini di sistemi lineari;
infatti, se <span class="math inline">\(\{v_1, \dots, v_n\} \subseteq
K^n\)</span> è un sistema di generatori, allora per ogni <span
class="math inline">\(v \in K^n\)</span> esistono <span
class="math inline">\(\lambda_1, \dots, \lambda_n \in K\)</span> tali
che <span class="math inline">\(v = \lambda_1 v_1 + \dots + \lambda_n
v_n\)</span>.</p>
<p>Scriviamo</p>
<p><span class="math display">\[
v_1 =
\begin{pmatrix}
  a_{11} \\
  a_{12} \\
  \vdots \\
  a_{1n}
\end{pmatrix},
\dots,
v_s =
\begin{pmatrix}
  a_{s1} \\
  a_{s2} \\
  \vdots \\
  a_{sn}
\end{pmatrix},
v =
\begin{pmatrix}
  b_1 \\
  b_2 \\
  \vdots \\
  b_n
\end{pmatrix}
\]</span></p>
<p>allora l’essere <span class="math inline">\(v\)</span> una
combinazione lineare di <span class="math inline">\(v_1 \dots,
v_n\)</span> equivale ad avere</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    \lambda_1 a_{11} + \dots + \lambda_s a_{s1} = b_1 \\
    \vdots \\
    \lambda_1 a_{s1} + \dots + \lambda_n a_{sn} = b_n
  \end{array}
\right.
\]</span></p>
<p>quindi <span class="math inline">\(v\)</span> è una combinazione
lineare di <span class="math inline">\(v_1, \dots, v_n\)</span> se e
solo se il sistema lineare</p>
<p><span class="math display">\[
\begin{pmatrix}
  a_{11} &amp; \dots &amp; a_{s1} \\
  \vdots &amp; \ddots &amp; \vdots \\
  a_{1n} &amp; \dots &amp; a_{sn}
\end{pmatrix}
\begin{pmatrix}
  \lambda_1 \\
  \vdots \\
  \lambda_n
\end{pmatrix} =
\begin{pmatrix}
  b_1 \\
  \vdots \\
  b_n
\end{pmatrix}
\]</span></p>
<p>è compatibile, ovvero ammette una soluzione.</p>
<p>Analogamente, anche l’essere linearmente indipendenti può essere
parafrasato in termini di sistemi lineari: con la notazione precedente,
<span class="math inline">\(v_1, \dots, v_n\)</span> sono linearmente
indipendenti se e solo se il sistema lineare omogeneo dato da:</p>
<p><span class="math display">\[
\begin{pmatrix}
  a_{11} &amp; \dots &amp; a_{s1} \\
  \vdots &amp; \ddots &amp; \vdots \\
  a_{1n} &amp; \dots &amp; a_{sn}
\end{pmatrix}
\begin{pmatrix}
  \lambda_1 \\
  \vdots \\
  \lambda_n
\end{pmatrix} =
\begin{pmatrix}
  0 \\
  \vdots \\
  0
\end{pmatrix}
\]</span></p>
<p>ammette come unica soluzione la soluzione nulla:</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    \lambda_1 = 0 \\
    \vdots \\
    \lambda_s = 0
  \end{array}
\right.
\]</span></p>
<hr />
<p>Consideriamo alcuni importanti risultati riguardo alle basi.</p>
<p><strong>Teorema (di estrazione di una base)</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span> finitamente generato; sia <span
class="math inline">\(\{v_1, \dots, v_k\}\)</span> un sistema di
generatori di <span class="math inline">\(V\)</span>; allora esiste
<span class="math inline">\(B \subseteq \{v_1, \dots, v_k\}\)</span>
tale che <span class="math inline">\(B\)</span> è una base di <span
class="math inline">\(V\)</span>.</p>
<p><strong>Dimostrazione</strong><br />
(idea) costruiamo questa base <span class="math inline">\(B\)</span> in
maniera algoritmica e possiamo supporre che <span
class="math inline">\(V \neq \{0\}\)</span> (perché il caso <span
class="math inline">\(V = \{0\}\)</span> è di facile dimostrazione),
useremo l’<strong>algoritmo dello scarto</strong>:</p>
<ol type="1">
<li>inizializziamo <span class="math inline">\(B = \{\}\)</span></li>
<li>consideriamo <span class="math inline">\(v_1\)</span>; se <span
class="math inline">\(v_1 = 0\)</span>, non facciamo nulla; se <span
class="math inline">\(v_1 \neq 0\)</span>, aggiungiamo <span
class="math inline">\(v_1\)</span> a <span
class="math inline">\(B\)</span></li>
<li>consideriamo <span class="math inline">\(v_2\)</span>; se <span
class="math inline">\(v_2 \in span(v_1)\)</span>, lo scartiamo;
altrimenti lo aggiungiamo a <span class="math inline">\(B\)</span></li>
<li>consideriamo <span class="math inline">\(v_3\)</span>; se <span
class="math inline">\(v_3 \in span(v_1, v_2)\)</span>, lo scartiamo;
altrimenti lo aggiungiamo a <span class="math inline">\(B\)</span></li>
<li><span class="math inline">\(\dots\)</span></li>
</ol>
<p>otteniamo un sottoinsieme di <span class="math inline">\(\{v_1,
\dots, v_k\}\)</span> che si può dimostrare essere una base di <span
class="math inline">\(V\)</span>.</p>
<p><strong>Teorema (del completamento o dell’estensione)</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span> finitamente generato e siano
<span class="math inline">\(\{v_1, \dots, v_p\}\)</span> vettori
linearmente indipendenti; allora esiste una base <span
class="math inline">\(B\)</span> di <span
class="math inline">\(V\)</span> tale che <span
class="math inline">\(\{v_1, \dots, v_p\} \subseteq span(B)\)</span>
(ovvero <span class="math inline">\(\{v_1, \dots, v_p\}\)</span> possono
essere completati a una base di <span
class="math inline">\(B\)</span>).</p>
<p><strong>Dimostrazione</strong><br />
(idea) dato che <span class="math inline">\(B\)</span> è finitamente
limitato, esiste <span class="math inline">\(\{w_1, \dots,
w_r\}\)</span> un sistema di generatori finito di <span
class="math inline">\(V\)</span>; allora <span
class="math inline">\(\{v_1, \dots, v_p\, w_1, \dots, w_r\}\)</span> è
anch’esso un sistema di generatori per <span
class="math inline">\(V\)</span>; ora applico a quest’ultimo insieme
l’algoritmo dello scarto, ottenendo una base <span
class="math inline">\(B\)</span> di <span
class="math inline">\(V\)</span>; per come è fatto l’algoritmo dello
scarto e dato che <span class="math inline">\(v_1, \dots, v_p\)</span>
sono linearmente indipendenti per ipotesi, essi saranno sempre scelti
dall’algoritmo, e pertanto avremmo che <span
class="math inline">\(\{v_1, \dots, v_p\} \subseteq B\)</span>.</p>
<hr />
<p>I due teoremi precedenti ci fanno capire perché le basi possono
essere equivalentemente caratterizzate come sistemi di generatori
minimali oppure come insiemi linearmente indipendenti massimali.</p>
<p><strong>Lemma (di Steinitz)</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span> finitamente generato e sia <span
class="math inline">\(B = \{v_1, \dots, v_n\}\)</span> una base di <span
class="math inline">\(V\)</span>; allora per ogni <span
class="math inline">\(k&gt;n\)</span> e per ogni scelta di vettori <span
class="math inline">\(w_1, \dots, w_k \in V\)</span> vale che <span
class="math inline">\(w_1, \dots, w_k\)</span> sono linearmente
dipendenti.</p>
<p><strong>Dimostrazione</strong><br />
(idea) per ipotesi vale che</p>
<p><span class="math inline">\(w_1 = c_{11} v_1 + \dots + c_{n1}
v_n\)</span><br />
<span class="math inline">\(w_2 = c_{12} v_1 + \dots + c_{n2}
v_n\)</span><br />
<span class="math inline">\(\vdots\)</span><br />
<span class="math inline">\(w_k = c_{1k} v_1 + \dots + c_{nk}
v_n\)</span></p>
<p>si può dimostrare che, se definiamo</p>
<p><span class="math display">\[
C =
\left\lbrace
  \begin{pmatrix}
    c_{11} &amp; \dots &amp; c_{1k} \\
    \vdots &amp; \ddots &amp; \vdots \\
    c_{n1} &amp; \dots &amp; c_{nk}
  \end{pmatrix}
\right\rbrace
\]</span></p>
<p>allora <span class="math inline">\(w_1, \dots, w_k\)</span> sono
linearmente dipendenti se e solo se il sistema lineare omogeneo <span
class="math inline">\(C \cdot X = 0\)</span> ammette una soluzione non
tutta nulla; osserviamo la matrice <span
class="math inline">\(C\)</span>; essa ha <span
class="math inline">\(n\)</span> righe e <span
class="math inline">\(k\)</span> colonne; per ipotesi <span
class="math inline">\(k&gt;n\)</span>, quindi ci sono più colonne che
righe; se, tramite l’algoritmo di gradinizzazione di Gauss portiamo
<span class="math inline">\(C\)</span> nella forma a scala, otterremo
dunque una matrice del tipo:</p>
<p><span class="math display">\[
\tilde{C} =
\begin{pmatrix} * &amp; * &amp; * &amp; \dots &amp; * \\
  0 &amp; * &amp; * &amp; \dots &amp; * \\
  0 &amp; 0 &amp; * &amp; \dots &amp; * \\
  0 &amp; 0 &amp; 0 &amp; \dots &amp; * \\
  \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  0 &amp; 0 &amp; 0 &amp; \dots &amp; 0
\end{pmatrix}
\]</span></p>
<p>dato che si sono più colonne che righe, almeno uno di questi scalini
sarà lungo più di <span class="math inline">\(1\)</span>, il che
significa che almeno un’incognita nel sistema lineare può essere scelta
liberamente e quindi in particolare può essere scelta non nulla,
determinando dunque una soluzione non tutta nulla dell’equazione.</p>
<p><strong>Teorema</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
<span class="math inline">\(K\)</span> finitamente generato; siano <span
class="math inline">\(\{v_1, \dots, v_n\}\)</span> e <span
class="math inline">\(\{w_1, \dots, w_m\}\)</span> due basi di <span
class="math inline">\(V\)</span>; allora <span class="math inline">\(n =
m\)</span>.</p>
<p>(equivalentemente due basi di uno spazio vettoriale su <span
class="math inline">\(K\)</span> finitamente generato, hanno lo stesso
numero di elementi)</p>
<p><strong>Dimostrazione</strong><br />
Dato che <span class="math inline">\(\{v_1, \dots, v_n\}\)</span> è una
base di <span class="math inline">\(V\)</span>, allora deve essere <span
class="math inline">\(m \leq n\)</span> per il lemma di Steinitz (perché
altrimenti <span class="math inline">\(\{w_1, \dots, w_m\}\)</span> non
sarebbero linearmente indipendenti); dal momento che <span
class="math inline">\(\{w_1, \dots, w_m\}\)</span> è una base di <span
class="math inline">\(V\)</span>, allora deve essere <span
class="math inline">\(n \leq m\)</span> per il lemma di Steinitz (perché
altrimenti <span class="math inline">\(\{v_1, \dots, v_n\}\)</span> non
sarebbero linearmente indipendenti); quindi <span
class="math inline">\(n = m\)</span>.</p>
<h1 id="dimensione-e-rango">Dimensione e rango</h1>
<p><strong>Definizione</strong> Sia <span
class="math inline">\(V\)</span> uno spazio vettoriale su <span
class="math inline">\(K\)</span> finitamente generato</p>
<ul>
<li>se <span class="math inline">\(V = \{0\}\)</span>, definiamo la
dimensione di <span class="math inline">\(V\)</span> come <span
class="math inline">\(0\)</span></li>
<li>se <span class="math inline">\(V \neq \{0\}\)</span> definiamo la
dimensione di <span class="math inline">\(V\)</span> come il numero di
elementi di una sua qualsiasi base</li>
</ul>
<p>indichiamo la dimensione di <span class="math inline">\(V\)</span>
con <span class="math inline">\(dim_k V\)</span> (o anche <span
class="math inline">\(dim V\)</span>).</p>
<p><strong>Esempio</strong><br />
<span class="math inline">\(dimR^2 = 2\)</span> (infatti <span
class="math inline">\(\left\lbrace \begin{pmatrix}1 \\ 0\end{pmatrix},
\begin{pmatrix}0 \\ 1\end{pmatrix}\right\rbrace\)</span> è base di <span
class="math inline">\(R^2\)</span>)<br />
<span class="math inline">\(dim_k K^2 = 2\)</span><br />
<span class="math inline">\(dim_k K^n = n\)</span><br />
<span class="math inline">\(dim_k M_{m,n}(K) = m \cdot n\)</span> (una
base è data dalle matrici con una sola entrata uguale a <span
class="math inline">\(1\)</span> e tutte le altre uguali a <span
class="math inline">\(0\)</span>)</p>
<p><strong>Osservazione</strong><br />
Il concetto di dimensione si applica anche ai sottospazi vettoriali di
uno spazio vettoriale.</p>
<p><strong>Proprietà</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale
finitamente generato; sia <span class="math inline">\(W \subseteq
V\)</span> un sottospazio vettoriale allora:</p>
<ol type="1">
<li><span class="math inline">\(dim W \leq dim V\)</span></li>
<li><span class="math inline">\(dim W = dim V \iff W = V\)</span></li>
</ol>
<hr />
<p>Con la dimensione abbiamo associato un numero ad uno spazio
vettoriale finitamente generato. Tramite questa nozione, associamo un
numero a una matrice.</p>
<p><strong>Osservazione</strong><br />
Se <span class="math inline">\(A \in M_{m,n}(K)\)</span>, allora le
colonne di <span class="math inline">\(A\)</span> sono elementi di <span
class="math inline">\(K^m\)</span>.</p>
<p><span class="math inline">\(A^{(1)}, \dots, A^{(m)} \in
K^m\)</span></p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_{m,n}(K)\)</span>; definiamo il
<strong>rango</strong> di <span class="math inline">\(A\)</span>, e lo
denotiamo <span class="math inline">\(rg(A)\)</span>.</p>
<p>Il numero naturale</p>
<p><span class="math inline">\(rg(A) = dim(span(A^{(1)}, \dots,
A^{(m)})\)</span>)</p>
<p><strong>Osservazione</strong><br />
Se <span class="math inline">\(A \in M_{m,n}(K)\)</span>, allora</p>
<ul>
<li><span class="math inline">\(rg(A) \leq m\)</span></li>
<li><span class="math inline">\(rg(A) \leq n\)</span></li>
</ul>
<p><strong>Osservazione</strong><br />
Se <span class="math inline">\(A \in M_{m,n}(K)\)</span>, allora</p>
<ul>
<li><span class="math inline">\(rg(A) \leq m\)</span>, infatti <span
class="math inline">\(A^{(1)}, \dots, A^{(m)} \in K^m\)</span>, dunque
<span class="math inline">\(span(A^{(1)}, \dots, A^{(m)}) \subseteq
K^{m}\)</span>, dunque <span class="math inline">\(dim(span(A^{(1)},
\dots, A^{(m)})) \leq dim(K^m)\)</span></li>
<li><span class="math inline">\(rg(A) \leq n\)</span>, infatti <span
class="math inline">\(A^{(1)}, \dots, A^{(n)}\)</span> ha <span
class="math inline">\(n\)</span> generatori, pertanto una base <span
class="math inline">\(span(A^{(1)}, \dots, A^{(n)})\)</span> ha al più
<span class="math inline">\(n\)</span> generatori, quindi <span
class="math inline">\(dim(span(A^{(1)}, \dots, A^{(n)})) \leq
n\)</span></li>
</ul>
<p>quindi <span class="math inline">\(rg(A) \leq min\{m,
n\}\)</span></p>
<p><strong>Esempio</strong><br />
Consideriamo</p>
<p><span class="math display">\[
A =
\begin{pmatrix}
  2 &amp; 1 &amp; 3 \\
  1 &amp; 0 &amp; -1
\end{pmatrix}
\]</span></p>
<p><span class="math inline">\(rg(A) = dim(span(\begin{pmatrix} 2 \\ 1
\end{pmatrix}, \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 3
\\ -1 \end{pmatrix}))\)</span></p>
<p>vale che <span class="math inline">\(rg(A) \leq min\{2, 3\} =
2\)</span></p>
<p>se fosse <span class="math inline">\(rg(A) = 1\)</span>, allora tutte
le colonne sarebbero proporzionali tra loro (ovvero ottenibili una
dall’altra tramite moltiplicazione per uno scalare), ma questo non è il
caso di <span class="math inline">\(A\)</span>. pertanto <span
class="math inline">\(rg(A) = 2\)</span>.</p>
<p><strong>Esempio</strong><br />
Qual è il rango della matrice unità?</p>
<p><span class="math inline">\(rg(1_n) = dim(span(\begin{pmatrix} 1 \\ 0
\\ \vdots \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0
\end{pmatrix}, \dots, \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1
\end{pmatrix})) = dim(K^n) = n\)</span></p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(A \in M_{m,n}(K)\)</span> e sia <span
class="math inline">\(\tilde{A}\)</span> una matrice ottenuta da <span
class="math inline">\(A\)</span> applicando le <span
class="math inline">\(3\)</span> operazioni elementari <span
class="math inline">\(OE1\)</span>, <span
class="math inline">\(OE2\)</span> e <span
class="math inline">\(OE3\)</span>; allora</p>
<ol type="1">
<li><span class="math inline">\(rg(\tilde{A}) = rg(A)\)</span></li>
<li>se <span class="math inline">\(\tilde{A}\)</span> è a scala, allora
<span class="math inline">\(rg(\tilde{A}) =\)</span> numero di righe non
nulle di <span class="math inline">\(\tilde{A}\)</span></li>
</ol>
<p><strong>Proposizione</strong><br />
<span class="math inline">\(rg(A) = rg({}^t A)\)</span></p>
<p>Questo ci dà un algoritmo per calcolare il rango di una matrice.</p>
<p><strong>Esempio</strong><br />
<span class="math display">\[
A =
\begin{pmatrix}
  1 &amp; 2 &amp; 3 \\
  4 &amp; 5 &amp; 6
\end{pmatrix}
\]</span></p>
<p><span class="math inline">\(rg(A) = 1\)</span> perché le righe (o le
colonne) sono proporzionali.</p>
<p><strong>Teorema (di dimensione per soluzioni di sistemi
lineari)</strong><br />
Sia <span class="math inline">\(A \in M_{m,n}(K)\)</span>; sia</p>
<p><span class="math inline">\(W = \left\lbrace s \in K^n : As =
0\right\rbrace \subseteq K^n\)</span></p>
<p>ovvero <span class="math inline">\(W\)</span> è l’insieme delle
soluzioni del sistema lineare omogeneo associato ad <span
class="math inline">\(A\)</span>; allora</p>
<p><span class="math inline">\(dim W = n - rg(A)\)</span></p>
<p><strong>Dimostrazione</strong><br />
Seguirà dalla teoria delle applicazioni lineari.</p>
<p><strong>Teorema (di Rouchè - Capelli)</strong><br />
Sia <span class="math inline">\(A \in M_{m,n}(K)\)</span> e sia <span
class="math inline">\(B \in K^n\)</span>, allora il sistema lineare
<span class="math inline">\(A \cdot X = b\)</span> è compatibile (ovvero
ammette almeno una soluzione) se e solo se <span
class="math inline">\(rg(A) = rg(A|B)\)</span>; in tal caso la generica
soluzione del sistema dipende da <span class="math inline">\(n -
rg(A)\)</span> parametri liberi.</p>
<p><strong>Dimostrazione</strong><br />
Per mostrare la prima parte del teorema, notiamo che se <span
class="math inline">\(s \in K^n\)</span>, con <span
class="math inline">\(s = \begin{pmatrix} s_1 \\ \vdots \\ s_n
\end{pmatrix}\)</span>, allora</p>
<p><span class="math inline">\(A \cdot s = b \iff s_1 \cdot A^{(1)} +
\dots + s_n \cdot A^{(n)} = b\)</span></p>
<p>(questa osservazione si ottiene scrivendo esplicitamente il prodotto
righe per colonne di <span class="math inline">\(A\)</span> per <span
class="math inline">\(s\)</span>); dimostriamo la prima parte</p>
<p>“<span class="math inline">\(\Rightarrow\)</span>”<br />
Supponiamo che <span class="math inline">\(A \cdot X = b\)</span> sia
compatibile; allora esiste <span class="math inline">\(s \in
K^n\)</span> soluzione del sistema, dunque <span class="math inline">\(A
\cdot s = b\)</span>; per quanto osservato, questo equivale a dire che
<span class="math inline">\(s_1 \cdot A^{(1)} + \dots + s_n \cdot
A^{(n)} = b\)</span>, il che significa che <span
class="math inline">\(b\)</span> è combinazione lineare di <span
class="math inline">\(A^{(1)}, \dots, A^{(n)}\)</span>, ovvero delle
colonne di <span class="math inline">\(A\)</span>, pertanto <span
class="math inline">\(b \in span(A^{(1)}, \dots, A^{(n)})\)</span>,
questo implica che</p>
<p><span class="math inline">\(span(A^{(1)}, \dots, A^{(n)}) =
span(A^{(1)}, \dots, A^{(n)}, b)\)</span></p>
<p>infatti</p>
<p>“<span class="math inline">\(\subseteq\)</span>”<br />
se <span class="math inline">\(u \in span(A^{(1)}, \dots,
A^{(n)})\)</span>, allora <span class="math inline">\(u = \lambda_1
\cdot A^{(1)} + \cdot + \lambda_n \cdot A^{(n)}\)</span>, pertanto <span
class="math inline">\(u = \lambda_1 A^{(1)} + \dots + \lambda_n A^{(n)}
+ 0 \cdot b\)</span>, ovvero <span class="math inline">\(u \in
span(A^{(1)}, \dots, A^{(n)}, b)\)</span></p>
<p>“<span class="math inline">\(\supseteq\)</span>”<br />
se <span class="math inline">\(u \in span(A^{(1)}, \dots, A^{(n)},
b)\)</span>, allora <span class="math inline">\(u = \lambda_1 A^{(1)} +
\dots + \lambda_n A^{(n)} + \lambda \cdot b\)</span>, ma <span
class="math inline">\(b = s_1 \cdot A^{(1)} + \dots + s_n \cdot
A^{(n)}\)</span>, quindi <span class="math inline">\(u = \lambda_1
A^{(1)} + \dots + \lambda_n A^{(n)} + \lambda \cdot (s_1 \cdot A^{(1)} +
\dots + s_n \cdot A^{(n)}) = (\lambda_1 + \lambda \cdot s_1) A^{(1)} +
\dots + (\lambda_n + \lambda \cdot s_n) A^{(n)}\)</span>, pertanto <span
class="math inline">\(u \in span(A^{(1)}, \dots, A^{(n)})\)</span></p>
<p>allora</p>
<p><span class="math inline">\(dim(span(A^{(1)}, \dots, A^{(n)})) =
dim(span(A^{(1)}, \dots, A^{(n)}, b))\)</span><br />
<span class="math inline">\(rg(A) = rg(A|b)\)</span></p>
<p>“<span class="math inline">\(\Leftarrow\)</span>”<br />
Supponiamo che valga <span class="math inline">\(rg(A) =
rg(A|B)\)</span>; allora per definizione</p>
<p><span class="math inline">\(dim(span(A^{(1)}, \dots, A^{(n)})) =
dim(span(A^{(1)}, \dots, A^{(n)}, b))\)</span></p>
<p>dato che vale <em>sempre</em> che lo <span
class="math inline">\(span(A^{(1)}, \dots, A^{(n)}) \subseteq
span(A^{(1)}, \dots, A^{(n)}, b)\)</span>, il fatto che le dimensioni di
questi due sottospazi sono uguali implica che gli sottospazi stessi
siano uguali; dunque</p>
<p><span class="math inline">\(span(A^{(1)}, \dots, A^{(n)}) =
span(A^{(1)}, \dots, A^{(n)}, b)\)</span></p>
<p>pertanto, dato che <span class="math inline">\(b \in span(A^{(1)},
\dots, A^{(n)}, b)\)</span>, segue che <span class="math inline">\(b \in
span(A^{(1)}, \dots, A^{(n)})\)</span>, ma abbiamo osservato che
quest’ultima condizione è equivalente al fatto che esista una soluzione
<span class="math inline">\(s \in K^n\)</span> del sistema <span
class="math inline">\(A \cdot X = b\)</span>, ovvero che quest’ultimo
sia compatibile.</p>
<p>Abbiamo quindi mostrato la prima parte del teorema; ora mostriamo la
seconda parte, ovvero che, quando il sistema <span
class="math inline">\(A \cdot X = b\)</span> è compatibile, la sua
generica soluzione dipende da <span class="math inline">\(n -
rg(A)\)</span> parametri liberi; per farlo, usiamo il teorema di
struttura per sistemi lineari e il teorema di dimensione per le
soluzioni di un sistema lineare omogeneo, il primo ci dice che la
generica soluzione <span class="math inline">\(s\)</span> del sistema
<span class="math inline">\(A \cdot X = b\)</span> o della forma <span
class="math inline">\(s = \tilde{s} + s_o\)</span> dove <span
class="math inline">\(\tilde{s}\)</span> è una soluzione fissata di
<span class="math inline">\(A \cdot X = b\)</span> ed <span
class="math inline">\(s_0\)</span> è una soluzione del sistema omogeneo
associato <span class="math inline">\(AX = 0\)</span>; il teorema di
dimensione ci dice che il sottospazio vettoriale <span
class="math inline">\(W\)</span> delle soluzioni <span
class="math inline">\(AX = 0\)</span> ha dimensione <span
class="math inline">\(n - rg(A)\)</span>; sia <span
class="math inline">\(k = n -rg(A)\)</span>; allora esiste una base
<span class="math inline">\(B\)</span> di <span
class="math inline">\(W\)</span> formata da <span
class="math inline">\(k\)</span> elementi, <span class="math inline">\(B
= \{w_1, \dots, w_k\}\)</span> e ogni elemento di <span
class="math inline">\(W\)</span> è combinazione lineare in maniera unica
di <span class="math inline">\(B\)</span>; pertanto <span
class="math inline">\(s_0\)</span> è della forma <span
class="math inline">\(s_0 = t_1 \cdot w_1 + \dots + t_k \cdot
w_k\)</span> per certe <span class="math inline">\(t_1, \dots, t_k \in
K\)</span>; in definitiva, la generica soluzione <span
class="math inline">\(s\)</span> di <span class="math inline">\(AX =
b\)</span> è della forma <span class="math inline">\(s = \tilde{s} + t_1
\cdot w_1 + \dots + t_k \cdot w_k\)</span> dove <span
class="math inline">\(t_1, \dots, t_k \in K\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
<p><strong>Esempio</strong><br />
Consideriamo il sistema lineare</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    x_1 - 2x_2 + 3x_3 - x_4 = 1 \\
    x_2 - x_4 = 2
  \end{array}
\right.
\]</span></p>
<p>usiamo il teorema di Rouchè - Capelli per dimostrare che il sistema
sia compatibile</p>
<p><span class="math display">\[
A =
\begin{pmatrix}
  1 &amp; -2 &amp; 3 &amp; -1 \\
  0 &amp; 1 &amp; 0 &amp; -1
\end{pmatrix},
B =
\begin{pmatrix}
  1 \\
  2
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
(A|B) =
\begin{pmatrix}
  1 &amp; -2 &amp; 3 &amp; -1 &amp; 1 \\
  0 &amp; 1 &amp; 0 &amp; -1 &amp; 2
\end{pmatrix}
\]</span></p>
<p><span class="math inline">\(rg(A) = 2\)</span> perché <span
class="math inline">\(A\)</span> a scala e ho <span
class="math inline">\(2\)</span> righe non nulle <span
class="math inline">\(rg(A|b) = 2\)</span> perché <span
class="math inline">\((A|b)\)</span> è a scala e ha <span
class="math inline">\(2\)</span> righe non nulle, dunque <span
class="math inline">\(rg(A) = rg(A|b)\)</span>, pertanto il sistema è
compatibile e la generica soluzione dipende da <span
class="math inline">\(4 - 2 = 2\)</span> parametri liberi per
determinare tutte le soluzioni, cominceremo col calcolare una soluzione
particolare:</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    x_1 - 2x_2 + 3x_3 - x_4 = 1 \\
    x_2 - x_4 = 2
  \end{array}
\right.
\iff
\left\lbrace
  \begin{array}{l}
    x_1 = 2x_2 - 3c_3 + x_4 + 1 \\
    x_2 = x_4 + 2
  \end{array}
\right.
\]</span></p>
<p><span class="math display">\[
\iff
\left\lbrace
  \begin{array}{l}
    x_1 = 2(x_4 + 2) - 3x_3 + x_4 + 1 \\
    x_2 = x_4 + 2
  \end{array}
\right.
\iff
\left\lbrace
  \begin{array}{l}
    x_1 = -3x_3 + 3x_4 + 5 \\
    x_2 = x_4 + 2
  \end{array}
\right.
\]</span></p>
<p>per determinare una soluzione particolare, assegno dei valori a <span
class="math inline">\(x_3\)</span> e <span
class="math inline">\(x_4\)</span> ottenendo <span
class="math inline">\(\tilde{s} = \begin{pmatrix} 5 \\ 2 \\ 0 \\ 0
\end{pmatrix}\)</span>; a questo punto determiniamo una base delle
soluzioni del sistema <span class="math inline">\(AX = 0\)</span>,
ovvero</p>
<p><span class="math display">\[
\left\lbrace
  \begin{array}{l}
    x_2 - 2x_2 + 3x_3 - x_4 = 0 \\
    x_2 - x_4 = 0
  \end{array}
\right.
\iff
\left\lbrace
  \begin{array}{l}
    x_1 = 3x_4 - 3x_3 \\
    x_2 = x_4
  \end{array}
\right.
\]</span></p>
<p>le soluzioni sono della forma</p>
<p><span class="math display">\[
\begin{pmatrix}
  -3x_3 + 3x_4 \\
  x_4 \\
  x_3 \\
  x_4
\end{pmatrix} = x_3
\begin{pmatrix}
  -3 \\
  0 \\
  1 \\
  0
\end{pmatrix} + x_4
\begin{pmatrix}
  3 \\
  1 \\
  0 \\
  1
\end{pmatrix}
\]</span></p>
<p>verificare che questi due vettori formano una base delle soluzioni di
<span class="math inline">\(AX = 0\)</span>.</p>
<h1 id="rango-e-determinante">Rango e determinante</h1>
<p><strong>Corollario</strong><br />
Sia <span class="math inline">\(A \in M_{n}(K)\)</span>, allora <span
class="math inline">\(rg(A) = n\)</span> (ovvero il rango di <span
class="math inline">\(A\)</span> è il massimo possibile) se e solo se
per ogni <span class="math inline">\(b \in K^n\)</span>, il sistema
lineare <span class="math inline">\(A \cdot X = b\)</span> è
compatibile.</p>
<p><strong>Dimostrazione</strong><br />
Abbiamo visto nella dimostrazione di Rouchè - Capelli che <span
class="math inline">\(A \cdot X = b\)</span> è compatibile se e solo se
<span class="math inline">\(b \in span(A^{(1)}, \dots,
A^{(n)})\)</span>, nella nostra dimostrazione abbiamo che <span
class="math inline">\(span(A^{(1)}, \dots, A^{(n)}) \subseteq
K^n\)</span>, e vale che <span class="math inline">\(dim_k K^n =
n\)</span>; pertanto</p>
<p><span class="math inline">\(rg(A) = n \iff dim\ span(A^{(1)}, \dots,
A^{(n)}) = n \iff span(A^{(1)}, \dots, A^{(n)}) = K^n \iff \forall b \in
K^n, b \in span(A^{(1)}, \dots, A^{(n)}) \iff \forall b \in K^n, AX =
b\)</span> è compatibile.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>Ritorniamo ora al teorema di Cramer, il quale ci dice che se <span
class="math inline">\(A \in M_{n}(K)\)</span>, ed <span
class="math inline">\(A\)</span> è invertibile, allora <span
class="math inline">\(\forall b \in K^n\)</span> il sistema lineare
<span class="math inline">\(A \cdot X = b\)</span> è compatibile.</p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(A \in M_{n}(K)\)</span>, allora <span
class="math inline">\(rg(A) = n\)</span> (ovvero il rango di <span
class="math inline">\(A\)</span> è massimo) se e solo se <span
class="math inline">\(A\)</span> è invertibile.</p>
<p><strong>Dimostrazione</strong><br />
“<span class="math inline">\(\Leftarrow\)</span>”<br />
Sia <span class="math inline">\(A\)</span> invertibile, allora per il
teorema di Cramer, <span class="math inline">\(\forall b \in
K^n\)</span> il sistema <span class="math inline">\(A \cdot X =
b\)</span> è compatibile, dunque per il corollario precedente <span
class="math inline">\(rg(A) = n\)</span>.</p>
<p>“<span class="math inline">\(\Rightarrow\)</span>”<br />
Supponiamo che <span class="math inline">\(rg(A) = n\)</span>, vogliamo
mostrare che una certa <span class="math inline">\(B \in
M_{n}(K)\)</span> tale che <span class="math inline">\(AB = BA =
1_n\)</span>; è sufficiente costruire <span
class="math inline">\(B\)</span> tale che <span class="math inline">\(AB
= 1_n\)</span>; ora, vale che</p>
<p><span class="math inline">\(AB = 1_n\)</span> se e solo se <span
class="math inline">\(A \cdot B^{(i)} = \begin{pmatrix} 0 \\ \vdots \\ 1
&amp; (i) \text{ cioè l&#39;uno va in posizione i-esima}\\ \vdots \\ 0
&amp; \end{pmatrix}\)</span></p>
<p>chiamiamo <span class="math inline">\(e_i\)</span> il vettore <span
class="math inline">\(\begin{pmatrix} 0 \\ \vdots \\ 1 &amp; (i)\\
\vdots \\ 0 &amp; \end{pmatrix}\)</span></p>
<p>se e solo se tutti i sistemi lineari <span class="math inline">\(A
\cdot B^{(i)} = e_i\)</span>, con <span class="math inline">\(i \in \{1,
\dots, n\}\)</span>, hanno soluzione; dato che <span
class="math inline">\(rg(A) = n\)</span>, sappiamo che tutti questi
sistemi lineari sono compatibili e dunque le loro soluzioni determinano
le colonne di <span class="math inline">\(B\)</span>.</p>
<p>Da questo risultato otteniamo un algoritmo per determinare l’inversa
di una matrice quadrata <span class="math inline">\(A \in
M_{n}(K)\)</span> quando essa è invertibile.</p>
<p>Abbiamo visto che per risolvere l’inversa di <span
class="math inline">\(A\)</span> dobbiamo risolvere tutti i sistemi
lineari del tipo <span class="math inline">\(AX = e_i\)</span>, con
<span class="math inline">\(i \in \{1, \dots, n\}\)</span>. Cerchiamo di
risolverli tutti contemporaneamente, ovvero consideriamo la matrice</p>
<p><span class="math display">\[
\left(
\begin{array}{c|cccc}
  &amp; 1 &amp; 0 &amp; \dots &amp; 0 \\
  &amp; 0 &amp; 1 &amp; \dots &amp; 0 \\
A &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
  &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  &amp; 0 &amp; 0 &amp; \dots &amp; 1
\end{array}
\right)
\]</span></p>
<p>Notiamo che dato che <span class="math inline">\(A\)</span> è
invertibile, il suo rango è <span class="math inline">\(n\)</span>,
quindi la sua forma a scala dopo l’algoritmo di Gauss è</p>
<p><span class="math display">\[
\tilde{A} =
\begin{pmatrix} * &amp; * &amp; * &amp; \dots &amp; * \\
  0 &amp; * &amp; * &amp; \dots &amp; * \\
  0 &amp; 0 &amp; * &amp; \dots &amp; * \\
  0 &amp; 0 &amp; 0 &amp; \dots &amp; * \\
  \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  0 &amp; 0 &amp; 0 &amp; \dots &amp; 1
\end{pmatrix}
\]</span></p>
<p>infatti <span class="math inline">\(\tilde{A}\)</span> è una matrice
<span class="math inline">\(n \times n\)</span> e dato che <span
class="math inline">\(rg(A) = n\)</span>, <span
class="math inline">\(\tilde{A}\)</span> deve avere <span
class="math inline">\(n\)</span> righe non nulle.</p>
<p>Usando ancora operazioni elementari possiamo portare <span
class="math inline">\(\tilde{A}\)</span> nella forma</p>
<p><span class="math display">\[
\begin{pmatrix}
  0 &amp; * &amp; * &amp; \dots &amp; 0 \\
  0 &amp; 0 &amp; * &amp; \dots &amp; 0 \\
  0 &amp; 0 &amp; 1 &amp; \dots &amp; 0 \\
  \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  0 &amp; 0 &amp; 0 &amp; \dots &amp; 1
\end{pmatrix} \rightarrow
\begin{pmatrix}
  1 &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
  0 &amp; 1 &amp; 0 &amp; \dots &amp; 0 \\
  0 &amp; 0 &amp; 1 &amp; \dots &amp; 0 \\
  \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  0 &amp; 0 &amp; 0 &amp; \dots &amp; 1
\end{pmatrix}
\]</span></p>
<p>Riassumendo, usando operazioni elementari possiamo portare <span
class="math inline">\(A\)</span> nella matrice <span
class="math inline">\(1_n\)</span>.</p>
<p>Applichiamo ora queste operazioni elementari alla matrice <span
class="math inline">\((A | 1_n)\)</span></p>
<p>otterremo una matrice del tipo <span class="math inline">\((1_n |
B)\)</span>, con <span class="math inline">\(B\)</span> una certa
matrice <span class="math inline">\(M_{n}(K)\)</span>.</p>
<p>Ora la matrice di partenza codificava i sistemi <span
class="math inline">\(AX = e_i\)</span>, le cui soluzioni sono le
colonne dell’inversa di <span class="math inline">\(A\)</span>. Le
operazioni elementari non cambiano le soluzioni del sistema. L’ultima
matrice codifica i sistemi lineari <span class="math inline">\(1X =
B^{(i)}\)</span>, pertanto le soluzioni di quest’ultimo sistema sono le
colonne della matrice inversa di <span class="math inline">\(A\)</span>.
Le soluzioni di questo sistema sono proprio le colonne di <span
class="math inline">\(B\)</span>, il che ci mostra che <span
class="math inline">\(B\)</span> è l’inversa di <span
class="math inline">\(A\)</span>.</p>
<p><strong>Esempio</strong><br />
Consideriamo</p>
<p><span class="math display">\[
A =
\begin{pmatrix}
  2 &amp; 1 \\
  5 &amp; 3
\end{pmatrix}
\]</span></p>
<p>abbiamo che <span class="math inline">\(rg(A) = 2\)</span>, quindi
<span class="math inline">\(A\)</span> è invertibile</p>
<p>calcoliamo l’inversa di <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
(A | 1_n) =
\begin{pmatrix}
  2 &amp; 1 &amp; | &amp; 1 &amp; 0 \\
  5 &amp; 3 &amp; | &amp; 0 &amp; 1
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
\begin{pmatrix}
  1 &amp; \frac{1}{2} &amp; | &amp; \frac{1}{2} &amp; 0 \\
  5 &amp; 3 &amp; | &amp; 0 &amp; 1
\end{pmatrix} \rightarrow
\begin{pmatrix}
  1 &amp; \frac{1}{2} &amp; | &amp; \frac{1}{2} &amp; 0 \\
  0 &amp; \frac{1}{2} &amp; | &amp; -\frac{5}{2} &amp; 1
\end{pmatrix} \rightarrow
\begin{pmatrix}
  1 &amp; \frac{1}{2} &amp; | &amp; \frac{1}{2} &amp; 0 \\
  0 &amp; 1 &amp; | &amp; -5 &amp; 2
\end{pmatrix} \rightarrow
\begin{pmatrix}
  1 &amp; 0 &amp; | &amp; 3 &amp; -1 \\
  0 &amp; 1 &amp; | &amp; -5 &amp; 2
\end{pmatrix}
\]</span></p>
<p>pertanto <span class="math inline">\(\begin{pmatrix} 3 &amp; -1 \\ -5
&amp; 2 \end{pmatrix}\)</span> è l’inversa di <span
class="math inline">\(\begin{pmatrix} 2 &amp; 1 \\ 5 &amp; 3
\end{pmatrix}\)</span>.</p>
<p>Andiamo ora ad associare ad ogni matrice quadrata <span
class="math inline">\(A \in M_{n}(K)\)</span> un elemento di <span
class="math inline">\(K\)</span> tramite il quale possiamo determinare
se <span class="math inline">\(A\)</span> sua invertibile o meno.</p>
<p>Questo elemento si chiamerà il <strong>determinante</strong> di <span
class="math inline">\(A\)</span>.</p>
<p>Consideriamo <span class="math inline">\(A \in M_2 (K)\)</span>,
ovvero</p>
<p><span class="math display">\[
A =
\begin{pmatrix}
  a_{11} &amp; a_{12} \\
  a_{21} &amp; a_{22}
\end{pmatrix}
\]</span></p>
<p>Considero a questo punto la matrice:</p>
<p><span class="math display">\[
B =
\begin{pmatrix}
  a_{11} &amp; -a_{12} \\
  -a_{21} &amp; a_{11}
\end{pmatrix}
\]</span></p>
<p>Svolgiamo il prodotto righe per colonne:</p>
<p><span class="math display">\[
AB =
\begin{pmatrix}
  a_{11} &amp; a_{12} \\
  a_{21} &amp; a_{22}
\end{pmatrix}
\begin{pmatrix}
  a_{11} &amp; -a_{12} \\
  -a_{21} &amp; a_{11}
\end{pmatrix} =
\begin{pmatrix}
  a_{11}a_{22} - a_{12}a_{21} &amp; -a_{11}a_{12} + a_{12}a_{11} \\
  a_{21}a_{22} - a_{21}a_{22} &amp; -a_{21}a_{12} + a_{11}a_{21}
\end{pmatrix} =
\begin{pmatrix}
  a_{11}a_{22} - a_{12}a_{21} &amp; 0 \\
  0 &amp; a_{11}a_{12} - a_{12}a_{21}
\end{pmatrix}
\]</span></p>
<p>Notiamo quindi che</p>
<p><span class="math inline">\(A\)</span> è invertibile <span
class="math inline">\(\iff\)</span> <span
class="math inline">\(a_{11}a_{22} - a_{12}a_{21} \neq 0\)</span></p>
<p>e in tal caso l’inversa di <span class="math inline">\(A\)</span> è
data da</p>
<p><span class="math display">\[
\frac{1}{a_{11}a_{22} - a_{12}a_{21}} \cdot \begin{pmatrix}
  a_{22} &amp; -a_{12} \\
  -a_{21} &amp; a_{11}
\end{pmatrix}
\]</span></p>
<p>(moltiplicazione per uno scalare)</p>
<p>Infatti</p>
<p><span class="math display">\[
A \cdot (\frac{1}{a_{11} a_{22} - a_{12} a_{21}} B) =
\frac{1}{a_{11}a_{22} - a_{12}a_{21}} \cdot (A \cdot B) =
\frac{1}{a_{11}a_{22} - a_{12}a_{21}} \cdot \begin{pmatrix}
  a_{11} a_{22} - a_{12} a_{21} &amp; 0 \\
  0 &amp; a_{11}a_{12} - a_{12}a_{21}
\end{pmatrix} = 1_2
\]</span></p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_2 (K)\)</span>, <span
class="math inline">\(A = (a_{ij})\)</span>, definiamo il
<strong>determinante</strong> di <span class="math inline">\(A\)</span>
come lo scalare</p>
<p><span class="math inline">\(det(A) = a_{11}a_{22} - a_{12}a_{21},
det(A) \in K\)</span></p>
<p>Questo abbiamo visto finora ci permette di enunciare.</p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(A \in M_2 (K)\)</span>, allora</p>
<p><span class="math inline">\(A\)</span> è invertibile <span
class="math inline">\(\iff\)</span> <span class="math inline">\(rg(A) =
2\)</span> <span class="math inline">\(\iff\)</span> <span
class="math inline">\(det(A) \neq 0\)</span></p>
<p>in tal caso, se <span class="math inline">\(A = \begin{pmatrix}
a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{pmatrix}\)</span>, vale
che <span class="math inline">\(A^{-1} = \frac{1}{det(A)} \cdot
\begin{pmatrix} a_{22} &amp; -a_{12} \\ -a_{21} &amp; a_{11}
\end{pmatrix}\)</span>.</p>
<p><strong>Esempio</strong><br />
<span class="math inline">\(A = \begin{pmatrix} 2 &amp; 1 \\ 5 &amp; 3
\end{pmatrix}\)</span>, <span class="math inline">\(det(A) = 2 \cdot 3 -
1 \cdot 5 = 1\)</span></p>
<p><span class="math inline">\(A^{-1} = \frac{1}{det(A)} \begin{pmatrix}
3 &amp; -1 \\ -5 &amp; 2 \end{pmatrix} = \begin{pmatrix} 3 &amp; -1 \\
-5 &amp; 2 \end{pmatrix}\)</span></p>
<p><strong>Osservazione</strong><br />
Se <span class="math inline">\(A \in M_2 (\mathbb{R})\)</span> e
consideriamo i suoi due vettori colonna e supponiamo che valga <span
class="math inline">\(a_{ij} &gt; 0,\ \forall i, j \in \{1,
2\}\)</span>, allora possiamo rappresentare i due vettori colonna nel
piano:</p>
<figure>
<img src="img/determinante_come_area.png" width="301"
alt="determinante come area" />
<figcaption aria-hidden="true">determinante come area</figcaption>
</figure>
<p>si può verificare che se <span
class="math inline">\(\mathcal{P}\)</span> è il parallelogramma
determinato dai due vettori di <span class="math inline">\(A\)</span>,
allora</p>
<p><span class="math inline">\(det(A) = area(\mathcal{P})\)</span></p>
<p>Andiamo ora a definire il determinante di una qualsiasi matrice
quadrata.</p>
<p>Lo faremo in maniera ricorsiva.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_n (K)\)</span> e siano <span
class="math inline">\(i, j \in \{1, \ldots, n\}\)</span> due indici
fissati; definiamo la matrice <span class="math inline">\(A_{ij} \in
M_{n-1} (K)\)</span> come la sottomatrice di <span
class="math inline">\(A\)</span> ottenuta eliminando la <span
class="math inline">\(i\)</span>-esima riga e la <span
class="math inline">\(j\)</span>-esima colonna; tale matrice si dice il
<strong>minore</strong> ij-esimo di <span
class="math inline">\(A\)</span>.</p>
<p><strong>Esempio</strong><br />
Se <span class="math inline">\(A = \begin{pmatrix} a_{11} &amp; a_{12}
&amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \\ a_{31} &amp; a_{32}
&amp; a_{33} \end{pmatrix}\)</span>, allora <span
class="math inline">\(A_{13} = \begin{pmatrix} a_{21} &amp; a_{22} \\
a_{31} &amp; a_{32} \end{pmatrix}\)</span></p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_n (K)\)</span>; definiamo il
<strong>determinante</strong> di <span class="math inline">\(A\)</span>
in maniera ricorsiva nel modo seguente:</p>
<ul>
<li>se <span class="math inline">\(n = 1\)</span>, allora <span
class="math inline">\(A = (a_{11})\)</span> e definiamo <span
class="math inline">\(det(A) = a_{11}\)</span></li>
<li>se <span class="math inline">\(n &gt; 1\)</span>, allora
definiamo</li>
</ul>
<p><span class="math inline">\(det(A) = \sum\limits_{i = 1}^n (-1)^{i +
1} \cdot a_{i1} \cdot det(A_{i1})\)</span></p>
<p><strong>Esempio</strong><br />
notiamo che con la definizione precedente ritroviamo il determinante di
una matrice <span class="math inline">\(2 \times 2\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\text{det} \begin{pmatrix}
  a_{11} &amp; a_{12} \\
  a_{21} &amp; a_{22}
\end{pmatrix}
&amp; = (-1)^{1 + 1} \cdot a_{11} \cdot \text{det}(A_{11}) + (-1)^{2 +
1} \cdot a_{21} \cdot \text{det}(A_{21}) = \\
&amp; = 1 \cdot A_{11} \cdot \text{det}(a_{22}) - 1 \cdot A_{21} \cdot
\text{det}(a_{12}) = \\
&amp; = a_{11} \cdot a_{22} - a_{12} \cdot a_{21}
\end{aligned}\]</span></p>
<p><strong>Esempio</strong><br />
<span class="math display">\[A = \begin{pmatrix}
  1 &amp; 0 &amp; 2 \\
  0 &amp; 1 &amp; 1 \\
  2 &amp; 3 &amp; 1 \\
\end{pmatrix}\]</span></p>
<p><span class="math display">\[\begin{aligned}
det(A) &amp;=
1 \cdot det \begin{pmatrix} 1 &amp; 1 \\ 3 &amp; 1 \end{pmatrix} -
0 \cdot det \begin{pmatrix} 0 &amp; 2 \\ 3 &amp; 1 \end{pmatrix} +
2 \cdot det \begin{pmatrix} 0 &amp; 2 \\ 1 &amp; 1 \end{pmatrix} =\\
&amp;=
1 \cdot (1 \cdot 1 - 1 \cdot 3) -
0 \cdot (0 \cdot 1 - 2 \cdot 3) +
2 \cdot (0 \cdot 1 - 2 \cdot 1) =\\
&amp;= - 2 - 0 - 4 = -6
\end{aligned}
\]</span></p>
<p><strong>Teorema</strong><br />
Sia <span class="math inline">\(A \in M_n (K)\)</span>; allora</p>
<p><span class="math inline">\(A\)</span> è invertibile <span
class="math inline">\(\iff\)</span> <span class="math inline">\(det(A)
\neq 0\)</span></p>
<p><strong>Formula di Sarrus</strong><br />
Solo per le matrici <span class="math inline">\(3 \times 3\)</span> vale
la formula di Sarrus:</p>
<p><span class="math inline">\(A = \begin{pmatrix} a_{11} &amp; a_{12}
&amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \\ a_{31} &amp; a_{32}
&amp; a_{33} \end{pmatrix}\)</span></p>
<p><span class="math inline">\(det(A) = a_{11} \cdot a_{22} \cdot a_{33}
+ a_{12} \cdot a_{23} \cdot a_{31} + a_{13} \cdot a_{21} \cdot a_{32} -
\\ - a_{13} \cdot a_{22} \cdot a_{31} - a_{11} \cdot a_{23} \cdot a_{32}
- a_{12} \cdot a_{21} \cdot a_{33}\)</span></p>
<figure>
<img src="img/sarrus.png" width="344" alt="graficamente con Sarrus" />
<figcaption aria-hidden="true">graficamente con Sarrus</figcaption>
</figure>
<p><strong>Proposizione</strong><br />
Il determinante gode della seguenti 3 proprietà:</p>
<p><span class="math inline">\(D1\)</span>: (multilinearità)</p>
<p>Sia <span class="math inline">\(A \in M_n (K)\)</span>e supponiamo
che <span class="math inline">\(A_{(i)} = R_1 + R_2\)</span> (la i-esima
riga è la somma di due vettori riga), allora</p>
<p><span class="math display">\[det \begin{pmatrix}
  A_{(1)} \\
  \vdots \\
  R_1 + R_2 \\
  \vdots \\
  A_{(n)}
\end{pmatrix} =
det \begin{pmatrix}
  A_{(1)} \\
  \vdots \\
  R_1 \\
  \vdots \\
  A_{(n)}
\end{pmatrix} +
det \begin{pmatrix}
  A_{(1)} \\
  \vdots \\
  R_2 \\
  \vdots \\
  A_{(n)}
\end{pmatrix}\]</span></p>
<p>inoltre se invece <span class="math inline">\(A_{(i)} = c \cdot
R\)</span> per qualche <span class="math inline">\(c \in K\)</span> e
qualche vettore riga <span class="math inline">\(R\)</span>, allora</p>
<p><span class="math display">\[det \begin{pmatrix}
  A_{(1)} \\
  \vdots \\
  c \cdot R \\
  \vdots \\
  A_{(n)}
\end{pmatrix} =
c \cdot det \begin{pmatrix}
  A_{(1)} \\
  \vdots \\
  R \\
  \vdots \\
  A_{(n)}
\end{pmatrix}\]</span></p>
<p>analoghe proprietà valgono se al posto delle righe consideriamo le
colonne.</p>
<p><span class="math inline">\(D2\)</span>: (alternanza o
antisimmetria)</p>
<p>se scambiamo due righe o due colonne di posto, il determinante cambia
di segno, ovvero</p>
<p><span class="math display">\[det \begin{pmatrix}
  A_{(1)} \\
  \vdots \\
  A_{(i)} \\
  \vdots \\
  A_{(j)} \\
  \vdots \\
  A_{(n)}
\end{pmatrix} = -det \begin{pmatrix}
  A_{(1)} \\
  \vdots \\
  A_{(j)} \\
  \vdots \\
  A_{(i)} \\
  \vdots \\
  A_{(n)}
\end{pmatrix}\]</span></p>
<p>se ci sono <span class="math inline">\(k\)</span> scambi, il segno va
dunque moltiplicato per <span class="math inline">\((-1)^k\)</span>.</p>
<p><span class="math inline">\(D3\)</span>: (normalizzazione)</p>
<p><span class="math inline">\(det\ 1_n = 1\)</span></p>
<p><strong>Teorema (di caratterizzazione del
determinante)</strong><br />
Il determinante è l’unica funzione <span class="math inline">\(M_n (K)
\rightarrow K\)</span> che soddisfa le proprietà <span
class="math inline">\(D1\)</span>, <span
class="math inline">\(D2\)</span> e <span
class="math inline">\(D3\)</span>.</p>
<p><strong>Corollario</strong><br />
<span class="math inline">\(i)\)</span> se <span
class="math inline">\(A\)</span> ha due righe uguali, allora <span
class="math inline">\(det(A) = -det(A)\)</span> (per <span
class="math inline">\(D2\)</span>)<br />
e dunque <span class="math inline">\(det(A) = 0\)</span>; analogamente
per le colonne.</p>
<p><span class="math inline">\(ii)\)</span> se <span
class="math inline">\(A\)</span> ha una riga nulla, allora <span
class="math inline">\(det(A) = 0\)</span> (per <span
class="math inline">\(D1\)</span>)</p>
<p><strong>Corollario</strong><br />
<span class="math inline">\(i)\)</span> se <span
class="math inline">\(\tilde{A}\)</span> è ottenuta da <span
class="math inline">\(A\)</span> con una operazione elementare <span
class="math inline">\(OE1\)</span>,<br />
allora <span class="math inline">\(det(\tilde{A}) = -det(A)\)</span></p>
<p><span class="math inline">\(ii)\)</span> se <span
class="math inline">\(\tilde{A}\)</span> è ottenuta da <span
class="math inline">\(A\)</span> con una operazione elementare <span
class="math inline">\(OE2\)</span>,<br />
allora <span class="math inline">\(det(\tilde{A}) = c \cdot
det(A)\)</span></p>
<p><span class="math inline">\(iii)\)</span> se <span
class="math inline">\(\tilde{A}\)</span> è ottenuta da <span
class="math inline">\(A\)</span> con una operazione elementare <span
class="math inline">\(OE3\)</span>,<br />
allora <span class="math inline">\(det(\tilde{A}) = det(A)\)</span>;
infatti se</p>
<p><span class="math display">\[A = \begin{pmatrix}
    A_{(1)} \\
    \vdots \\
    A_{(n)}
\end{pmatrix}\]</span></p>
<p>allora</p>
<p><span class="math display">\[\tilde{A} = \begin{pmatrix}
    A_{(1)} \\
    \vdots \\
    A_{(i)} + c \cdot A_{(j)} \\
    \vdots \\
    A_{(n)}
\end{pmatrix}\]</span></p>
<p><span class="math display">\[det \tilde{A} =
det \begin{pmatrix}
    A_{(1)} \\
    \vdots \\
    A_{(i)} + c \cdot A_{(j)} \\
    \vdots \\
    A_{(n)}
\end{pmatrix} \stackrel{D1}{=}
det \begin{pmatrix}
    A_{(1)} \\
    \vdots \\
    A_{(i)} \\
    \vdots \\
    A_{(n)}
\end{pmatrix} +
det \begin{pmatrix}
    A_{(1)} \\
    \vdots \\
    c \cdot A_{(j)} \\
    \vdots \\
    A_{(n)}
\end{pmatrix} \stackrel{D1}{=}
det(A) +
c \cdot \underbrace{det \begin{pmatrix}
    A_{(1)} \\
    \vdots \\
    A_{(j)} \\
    \vdots \\
    A_{(n)}
\end{pmatrix}}_{\text{ha due righe uguali, quindi } det = 0} = det
A\]</span></p>
<p><strong>Corollario</strong><br />
Se <span class="math inline">\(A \in M_n(K)\)</span> e <span
class="math inline">\(\tilde{A}\)</span> è la matrice a scala ottenuta
applicando l’algoritmo di gradinizzazione di Gauss a <span
class="math inline">\(A\)</span>, allora<br />
<span class="math inline">\(det(\tilde{A}) = \lambda \cdot
det(A)\)</span> per un certo <span class="math inline">\(\lambda \in K
\smallsetminus \{0\}\)</span><br />
in particolare<br />
<span class="math inline">\(det(A) = 0 \Leftarrow det(\tilde{A}) =
0\)</span></p>
<p>inoltre se nell’algoritmo di gradinizzazione di Gauss non effettuiamo
noi la normalizzazione dei pivot a <span
class="math inline">\(1\)</span>, allora <span
class="math inline">\(det(\tilde{A}) = (-1)^k \cdot det(A)\)</span>,
dove <span class="math inline">\(k\)</span> è il numero di scambi di
righe che abbiamo effettuato.</p>
<p><strong>Proposizione</strong><br />
Se <span class="math inline">\(A \in M_n(K)\)</span> è una matrice
triangolare superiore, ovvero <span class="math inline">\(a_{ij} =
0\)</span> per <span class="math inline">\(i &gt; j\)</span>,<br />
allora <span class="math inline">\(det(A) = a_{11} \cdot a_{22} \cdot
\ldots \cdot a_{nn}\)</span></p>
<p><strong>Dimostrazione</strong><br />
Dimostriamo questo risultato per induzione su <span
class="math inline">\(n\)</span>:</p>
<p><span class="math inline">\(\boxed{n = 1}\)</span> in questo caso
<span class="math inline">\(A = a_{11}\)</span> e <span
class="math inline">\(det(A) = a_{11}\)</span> per definizione.<br />
Quindi vale la tesi.</p>
<p><span class="math inline">\(\boxed{\text{passo induttivo}}\)</span>
possiamo supporre <span class="math inline">\(n &gt; 1\)</span>, allora
per definizione<br />
<span class="math inline">\(det(A) = a_{11} \cdot det(A_{11}) - 0 \cdot
det(A_{21}) + 0 \cdot det(A_{31}) + \dots + (-1)^{n + 1} \cdot
det(A_{n-1}) = a_{11} \cdot det(A_{11})\)</span></p>
<p><span class="math display">\[A_{11} = \begin{pmatrix}
    a_{22} &amp; \dots &amp; \dots &amp; a_{2n} \\
    0 &amp; \ddots &amp; \ddots &amp; \vdots \\
    \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; 0 &amp; a_{nn}
\end{pmatrix}\]</span></p>
<p>dunque <span class="math inline">\(A_{11} \in M_{n-1}(K)\)</span>, ed
è triangolare superiore,<br />
allora posso usare l’ipotesi induttiva su <span
class="math inline">\(A_{11}\)</span> e ottengo<br />
<span class="math inline">\(det(A) = a_{11} \cdot det(A_{11}) = a_{11}
\cdot a_{22} \cdot \ldots \cdot a_{nn}\)</span></p>
<p>pertanto <span class="math inline">\(det(A) = a_{11} \cdot
det(A_{11}) = a_{11} \cdot a_{22} \cdot \ldots \cdot a_{nn}\)</span></p>
<p><span class="math inline">\(\square\)</span></p>
<p>Con questi strumenti possiamo dimostrare il risultato secondo cui il
determinante caratterizza l’invertibilità o meno di una matrice.</p>
<p><strong>Teorema</strong><br />
Sia <span class="math inline">\(A \in M_n(K)\)</span>; vale che<br />
<span class="math inline">\(rg(A) &lt; n \iff det(A) = 0\)</span></p>
<p>equivalentemente</p>
<p><span class="math inline">\(rg(A) = n \iff det(A) \neq 0\)</span></p>
<p><strong>Dimostrazione</strong><br />
“<span class="math inline">\(\Rightarrow\)</span>” supponiamo che <span
class="math inline">\(rg(A) &lt; n\)</span>; sia <span
class="math inline">\(\tilde{A}\)</span> la matrice ottenuta da <span
class="math inline">\(A\)</span> applicando l’algoritmo di
gradinizzazione di Gauss; allora <span
class="math inline">\(\tilde{A}\)</span> ha una riga nulla, pertanto
<span class="math inline">\(det(\tilde{A}) = 0\)</span> e ciò è
equivalente a <span class="math inline">\(det(A) = 0\)</span>.</p>
<p><span class="math inline">\(\Leftarrow\)</span>” supponiamo che <span
class="math inline">\(det(A) = 0\)</span>; sia <span
class="math inline">\(\tilde{A}\)</span> la matrice ottenuta da <span
class="math inline">\(A\)</span> applicando l’algoritmo di
gradinizzazione di Gauss; allora <span
class="math inline">\(det(\tilde{A}) = 0\)</span>; <span
class="math inline">\(\tilde{A}\)</span> è a scala e dunque è
triangolare superiore, quindi il suo determinante è il prodotto dei suoi
elementi della diagonale; pertanto almeno un elemento della diagonale è
nullo, e quindi almeno un gradino di <span
class="math inline">\(\tilde{A}\)</span> è lungo almeno <span
class="math inline">\(2\)</span>, il che implica che <span
class="math inline">\(rg(\tilde{A}) &lt; n\)</span>, ovvero <span
class="math inline">\(rg(A) &lt; n\)</span>.</p>
<p><strong>Corollario</strong><br />
Sia <span class="math inline">\(A \in M_n(K)\)</span>; allora<br />
<span class="math inline">\(A\)</span> è invertibile <span
class="math inline">\(\iff\)</span> <span class="math inline">\(det(A)
\neq 0\)</span></p>
<p><strong>Dimostrazione</strong><br />
Vediamo ora che il determinante si può calcolare sviluppando rispetto a
una qualsiasi colonna o a una qualsiasi riga. Questo risultati sono
detti <strong>sviluppo di Laplace</strong> del determinante.</p>
<p><strong>Teorema</strong><br />
Sia <span class="math inline">\(A \in M_n(K)\)</span> e sia <span
class="math inline">\(k \in \{1, \dots, n\}\)</span>; allora<br />
<span class="math inline">\(det(A) = \displaystyle\sum_{i = 1}^n (-1)^{i
+ k} \cdot a_{ik} \cdot det(A_{ik})\)</span><br />
(sviluppo secondo la colonna <span
class="math inline">\(k\)</span>-esima)</p>
<p><strong>Teorema</strong><br />
Sia <span class="math inline">\(A \in M_n(K)\)</span> e sia <span
class="math inline">\(l \in \{1, \dots, n\}\)</span>; allora<br />
<span class="math inline">\(det(A) = \displaystyle\sum_{j = 1}^n (-1)^{l
+ j} \cdot a_{lj} \cdot det(A_{lj})\)</span><br />
(sviluppo secondo la riga <span
class="math inline">\(l\)</span>-esima)</p>
<p><strong>Corollario</strong><br />
Sia <span class="math inline">\(A \in M_n(K)\)</span>; allora <span
class="math inline">\(det(A) = det({}^t A)\)</span></p>
<p><strong>Dimostrazione</strong><br />
<span class="math inline">\({}^t A =\)</span> sviluppo lungo la prima
colonna di <span class="math inline">\({}^t A =\)</span><br />
<span class="math inline">\(=\)</span> sviluppo lungo la prima riga di
<span class="math inline">\(A = det(A)\)</span></p>
<p><strong>Osservazione</strong><br />
Come capire come prendere i segni nello sviluppo di Laplace?<br />
Basta osservare che il segno di <span class="math inline">\((-1)^{i +
j}\)</span> è</p>
<figure>
<img src="img/segni_laplace.png" width="352"
alt="segni nello sviluppo di Laplace" />
<figcaption aria-hidden="true">segni nello sviluppo di
Laplace</figcaption>
</figure>
<p><strong>Teorema</strong><br />
Siano <span class="math inline">\(A, B \in M_n(K)\)</span>; allora<br />
<span class="math inline">\(det(A \cdot B) = det(A) \cdot
det(B)\)</span><br />
(teorema di Binet)</p>
<p><strong>Corollario</strong><br />
Sia <span class="math inline">\(A \in M_n(K)\)</span> invertibile,
allora<br />
<span class="math inline">\(det(A^{-1}) = \dfrac{1}{det(A)} =
det(A)^{-1}\)</span></p>
<p><strong>Dimostrazione</strong><br />
Vale che <span class="math inline">\(A \cdot A^{-1} = 1_n\)</span>;
allora per il teorema di Binet e per la proprietà <span
class="math inline">\(D3\)</span> di normalizzazione vale che<br />
<span class="math inline">\(det(A \cdot A^{-1}) = det(1_n) =
1\)</span><br />
<span class="math inline">\(det(A \cdot A^{-1}) = det(A) \cdot
det(A)^{-1}\)</span><br />
<span class="math inline">\(det(A^{-1}) = \dfrac{1}{det(A)}\)</span></p>
<p>Andiamo ora a determinare una formula esplicita per le entrate della
matrice inversa di una matrice data.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_n(K)\)</span> e siamo <span
class="math inline">\(i, j \in \{1, \dots, n\}\)</span>; il
<strong>cofattore <span
class="math inline">\(i,j\)</span>-esimo</strong> di <span
class="math inline">\(A\)</span> è lo scalare <span
class="math inline">\((-1)^{i + j} \cdot det \ A_{ij}\)</span></p>
<p>dove ricordiamo che <span class="math inline">\(A_{ij}\)</span> è il
minore <span class="math inline">\(i, j\)</span>-esimo della matrice
<span class="math inline">\(A\)</span>.<br />
Definiamo la <strong>matrice dei cofattori</strong> di <span
class="math inline">\(A\)</span> come quella matrice<br />
<span class="math inline">\(cofA \in M_n(K)\)</span> il cui elemento di
posto <span class="math inline">\((i, j)\)</span> è il cofattore <span
class="math inline">\(i,j\)</span>-esimo di <span
class="math inline">\(A\)</span>.<br />
Dunque <span class="math inline">\((cofA)_{ij} = (-1)^{i + j} \cdot det
\ A_{ij}\)</span></p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(A \in M_n(K)\)</span>; allora</p>
<p><span class="math inline">\(A \cdot {}^t cof A = (det A) \cdot
1_n\)</span></p>
<p>in particolare se <span class="math inline">\(A\)</span> è
invertibile, allora</p>
<p><span class="math inline">\(A^{-1} = \dfrac{1}{det A} \cdot ({}^t cof
A)\)</span></p>
<p><strong>Dimostrazione</strong><br />
Calcoliamo l’entrata di posto <span class="math inline">\((i,
j)\)</span> della matrice <span class="math inline">\(A \cdot {}^t cof
A\)</span>:</p>
<p><span class="math inline">\(A_{(i)} \cdot ({}^t cof A)^{(j)}
=\)</span></p>
<p><span class="math inline">\(= \displaystyle\sum_{k = 1}^n a_{ik}
\cdot ({}^t cof A)_{kj} =\)</span></p>
<p><span class="math inline">\(= \displaystyle\sum_{k = 1}^n a_{ik}
\cdot (cof A)_{kj} =\)</span></p>
<p><span class="math inline">\(= \displaystyle\sum_{k = 1}^n a_{ik}
\cdot (-1)^{j + k} \cdot det A_{jk}\)</span></p>
<p>si verifica che</p>
<ul>
<li>se <span class="math inline">\(i = j\)</span> allora quello
precedente è l’espressione di <span
class="math inline">\(detA\)</span></li>
<li>se <span class="math inline">\(i \neq j\)</span> allora quello
precedente è l’espressione del determinante di una matrice con due righe
uguali, e quindi vale zero.</li>
</ul>
<p><span class="math inline">\(\square\)</span></p>
<h1 id="applicazioni-lineari">Applicazioni Lineari</h1>
<p><strong>Definizione</strong><br />
Siano <span class="math inline">\(V\)</span> e <span
class="math inline">\(V&#39;\)</span> due spazi vettoriali su <span
class="math inline">\(K\)</span>; una funzione</p>
<p><span class="math inline">\(f : V \rightarrow V&#39;\)</span></p>
<p>si dice <strong>applicazione lineare</strong> se valgono:</p>
<p><span class="math inline">\(AL1.\)</span>
(<strong>additività</strong>)</p>
<p><span class="math inline">\(\forall v_1, v_2 \in V : f(v_1 + v_2) =
f(v_1) + f(v_2)\)</span></p>
<p>rispettivamente somma in <span class="math inline">\(V\)</span> e
somma in <span class="math inline">\(V&#39;\)</span></p>
<p>“l’immagine della somma è la somma delle immagini”</p>
<p><span class="math inline">\(AL2.\)</span>
(<strong>omogeneità</strong>)</p>
<p><span class="math inline">\(\forall v \in V, \forall \lambda \in K :
f(\lambda v) = \lambda f(v)\)</span></p>
<p>rispettivamente moltiplicazione per uno scalare in <span
class="math inline">\(V\)</span> e moltiplicazione per uno scalare in
<span class="math inline">\(V&#39;\)</span></p>
<p>“l’immagine della moltiplicazione per uno scalare è la
moltiplicazione per uno scalare delle immagini”</p>
<p><strong>Esempio</strong><br />
Sia <span class="math inline">\(f : \mathbb{R}^2 \rightarrow
\mathbb{R}\)</span> definita da <span
class="math inline">\(f(\begin{pmatrix} x \\ y \end{pmatrix}) = x +
2y\)</span></p>
<p>vale che <span class="math inline">\(f\left(\begin{pmatrix} x_1 \\
y_1 \end{pmatrix} + \begin{pmatrix} x_2 \\ y_2 \end{pmatrix}\right)
=\)</span><br />
<span class="math inline">\(= (x_1x_2) + (2y_1y_2) =\)</span><br />
<span class="math inline">\(= (x_1 + 2y_1)(x_2 + 2y_2) =\)</span><br />
<span class="math inline">\(= f\left(\begin{pmatrix} x_1 \\ 2y_1
\end{pmatrix}\right) + f\left(\begin{pmatrix} x_2 \\ 2y_2
\end{pmatrix}\right)\)</span></p>
<p>quindi <span class="math inline">\(f\)</span> è additiva; similmente
si dimostra che <span class="math inline">\(f\)</span> è omogenea.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(A \in M_{m, n}(\mathbb{R})\)</span>;
allora <span class="math inline">\(A\)</span> definisce una funzione</p>
<p><span class="math inline">\(L_A : K^n \rightarrow K^m\)</span><br />
<span class="math inline">\(v \mapsto Av\)</span></p>
<p><strong>Proposizione</strong><br />
<span class="math inline">\(\forall A \in M_{m, n}(\mathbb{R})\)</span>,
la funzione <span class="math inline">\(L_A\)</span> è una applicazione
lineare.</p>
<p><strong>Dimostrazione</strong><br />
<span class="math inline">\(L_A (v_1 + v_2) = A \cdot (v_1 + v_2) = A
\cdot v_1 + A \cdot v_2 = L_A(v_1) + L_A(v_2)\)</span></p>
<p>similmente se <span class="math inline">\(\lambda \in K\)</span> e
<span class="math inline">\(v \in K^n\)</span>, vale <span
class="math inline">\(L_A(\lambda \cdot v) = \lambda \cdot
L_A(v)\)</span></p>
<p><strong>Esempio</strong> (rotazione nel piano di un angolo <span
class="math inline">\(\alpha\)</span> in senso antiorario)<br />
Sia <span class="math inline">\(\alpha \in \mathbb{R}\)</span> e
considero la matrice</p>
<p><span class="math display">\[
R_{\alpha} = \begin{pmatrix}
  \cos \alpha &amp; -\sin \alpha \\
  \sin \alpha &amp; \cos \alpha
\end{pmatrix}
\]</span></p>
<p>l’applicazione lineare <span class="math inline">\(L_{R_{\alpha}} :
\mathbb{R}^2 \rightarrow \mathbb{R}^2\)</span> è la rotazione di angolo
<span class="math inline">\(\alpha\)</span> in senso antiorario</p>
<figure>
<img src="img/esempio_rotazione_vettori.png" width="700"
alt="rotazione di un angolo \alpha in senso antiorario" />
<figcaption aria-hidden="true">rotazione di un angolo <span
class="math inline">\(\alpha\)</span> in senso antiorario</figcaption>
</figure>
<p><span class="math display">\[
L_{R_{\alpha}}\left(\begin{pmatrix} 1 \\ 0 \end{pmatrix}\right) =
\begin{pmatrix} \cos \alpha -\sin \alpha \\ \sin \alpha + \cos \alpha
\end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix}
\cos \alpha \\ \sin \alpha \end{pmatrix}
\]</span></p>
<p><span class="math display">\[
L_{R_{\alpha}}\left(\begin{pmatrix} 0 \\ 1 \end{pmatrix}\right) =
\begin{pmatrix} \cos \alpha &amp; -\sin \alpha \\ \sin \alpha &amp; \cos
\alpha \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} =
\begin{pmatrix} -\sin \alpha \\ \cos \alpha \end{pmatrix}
\]</span></p>
<p><strong>Definizione</strong><br />
(applicazioni lineare che “prendono le coordinate”)<br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale di
dimensione finita, <span class="math inline">\(dim V = n\)</span>; sia
<span class="math inline">\(B = \{v_1, \dots, v_n\}\)</span> una base di
<span class="math inline">\(V\)</span>; definiamo la funzione che prende
le <strong>coordinate rispetto a <span
class="math inline">\(B\)</span></strong> in questo modo:<br />
essa è la funzione</p>
<p><span class="math inline">\(F_B : V \rightarrow K^n\)</span></p>
<p>che agisce nel modo seguente: se <span class="math inline">\(v \in
V\)</span>, allora possiamo scrivere in maniera unica <span
class="math inline">\(v\)</span> come combinazione lineare <span
class="math inline">\(v_1 + \dots + v_n\)</span>:<br />
<span class="math inline">\(v = \lambda_1 v_1 + \dots + \lambda_n
v_n\)</span> con <span class="math inline">\(\lambda_1, \dots, \lambda_n
\in K\)</span><br />
allora definiamo <span class="math inline">\(F_B(v) = \begin{pmatrix}
\lambda_1 \\ \dots \\ \lambda_n \end{pmatrix}\)</span>;<br />
si può verificare che <span class="math inline">\(F_B\)</span> è
biettiva (quindi è invertibile);<br />
si dice quindi che <span class="math inline">\(F_B\)</span> è un
<strong>isomorfismo</strong> di spazi vettoriali.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V&#39;\)</span>
un’applicazione lineare; definiamo il <strong>nucleo</strong> di <span
class="math inline">\(f\)</span> come il sottoinsieme:</p>
<p><span class="math inline">\(ker f = \{v \in V : f(v) =
0\}\)</span></p>
<p>quindi <span class="math inline">\(ker f \subseteq V\)</span>;
definiamo l’<strong>immagine</strong> di <span
class="math inline">\(f\)</span> come il sottoinsieme:</p>
<p><span class="math inline">\(im f = \{v&#39; \in V&#39; : \text{
esiste } v \in V : f(v) = v&#39;\}\)</span></p>
<p>quindi <span class="math inline">\(im f \subseteq V&#39;\)</span></p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V&#39;\)</span>
un’applicazione lineare; allora <span class="math inline">\(ker
f\)</span> è sottospazio vettoriale di <span
class="math inline">\(V\)</span> e <span class="math inline">\(im
f\)</span> è sottospazio vettoriale di <span
class="math inline">\(V&#39;\)</span>.</p>
<p><strong>Dimostrazione</strong><br />
Consideriamo per primo <span class="math inline">\(ker f\)</span>:</p>
<p><span class="math inline">\(i.\)</span> verifichiamo che <span
class="math inline">\(f(0) = 0\)</span>; vale infatti che<br />
<span class="math inline">\(f(0) = f(0 + 0) \stackrel{AL1}{=} f(0) +
f(0)\)</span><br />
<span class="math inline">\(\Rightarrow f(0) = f(0) +
f(0)\)</span><br />
<span class="math inline">\(\Rightarrow f(0) = 0\)</span></p>
<p>quindi <span class="math inline">\(0 \in ker f\)</span></p>
<p><span class="math inline">\(ii.\)</span> siano <span
class="math inline">\(v_1, v_2 \in ker f\)</span>, dobbiamo mostrare che
<span class="math inline">\(v_1 + v_2 \in ker f\)</span>, ovvero che
<span class="math inline">\(f(v_1 + v_2) = 0\)</span>; per ipotesi <span
class="math inline">\(f(v_1) = 0\)</span> e <span
class="math inline">\(f(v_2) = 0\)</span>, allora</p>
<p><span class="math inline">\(f(v_1 + v_2) \stackrel{AL1}{=} f(v_1) +
f(v_2) = 0 + 0 = 0\)</span></p>
<p><span class="math inline">\(iii.\)</span> Sia <span
class="math inline">\(v \in ker f\)</span> e sia <span
class="math inline">\(\lambda \in K\)</span>, dobbiamo mostrare che
<span class="math inline">\(\lambda v \in ker f\)</span>, ovvero che
<span class="math inline">\(f(\lambda v) = 0\)</span>; per ipotesi <span
class="math inline">\(f(v) = 0\)</span>, allora</p>
<p><span class="math inline">\(f(\lambda v) \stackrel{AL2}{=} \lambda
f(v) = \lambda \cdot 0 = 0\)</span></p>
<p>quindi <span class="math inline">\(ker f\)</span> è sottospazio
vettoriale.</p>
<p>Consideriamo ora <span class="math inline">\(im f\)</span>:</p>
<p><span class="math inline">\(i.\)</span> vale che <span
class="math inline">\(0 = f(0)\)</span>, dunque <span
class="math inline">\(0 \in im f\)</span></p>
<p><span class="math inline">\(ii.\)</span> siano <span
class="math inline">\(v&#39;_1, v&#39;_2 \in im f\)</span>, dobbiamo
mostrare che <span class="math inline">\(v&#39;_1 + v&#39;_2 \in im
f\)</span>; per ipotesi esistono <span class="math inline">\(v_1, v_2
\in V\)</span> tali che <span class="math inline">\(f(v_1) =
v&#39;_1\)</span> e <span class="math inline">\(f(v_2) =
v&#39;_2\)</span>, allora</p>
<p><span class="math inline">\(f(v_1 + v_2) = f(v_1) + f(v_2) = v&#39;_1
+ v&#39;_2\)</span></p>
<p>pertanto <span class="math inline">\(v&#39;_1 + v&#39;_2\)</span> è
immagine di un elemento di <span class="math inline">\(V\)</span>,
quindi <span class="math inline">\(v&#39;_1 + v&#39;_2 \in im
f\)</span></p>
<p><span class="math inline">\(iii.\)</span> Sia <span
class="math inline">\(v&#39; \in V&#39;\)</span> e sia <span
class="math inline">\(\lambda \in K\)</span>, sia <span
class="math inline">\(v&#39; \in im f\)</span>, dobbiamo mostrare che
<span class="math inline">\(\lambda v&#39; \in im f\)</span>; per
ipotesi esiste <span class="math inline">\(v \in V\)</span> tale che
<span class="math inline">\(f(v) = v&#39;\)</span>, allora</p>
<p><span class="math inline">\(f(\lambda v) = \lambda f(v) = \lambda
v&#39;\)</span></p>
<p>pertanto <span class="math inline">\(\lambda v&#39;\)</span> è
immagine di un elemento di <span class="math inline">\(V\)</span>,
quindi <span class="math inline">\(\lambda v&#39; \in im f\)</span></p>
<p>dunque <span class="math inline">\(im f\)</span> è sottospazio
vettoriale.</p>
<hr />
<p>Nucleo e immagine determinano due importanti proprietà di <span
class="math inline">\(f\)</span> come funzione</p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V&#39;\)</span>
un’applicazione lineare; allora</p>
<ol type="1">
<li><span class="math inline">\(f\)</span> è iniettiva <span
class="math inline">\(\Leftrightarrow ker f = 0\)</span></li>
<li><span class="math inline">\(f\)</span> è suriettiva <span
class="math inline">\(\Leftrightarrow im f = V&#39;\)</span></li>
</ol>
<p><strong>Dimostrazione</strong><br />
<span class="math inline">\(2.\)</span> è una parafrasi del concetto di
suriettività.</p>
<p><span class="math inline">\(1.\)</span><br />
“<span class="math inline">\(\Rightarrow\)</span>”<br />
Supponiamo <span class="math inline">\(f\)</span> iniettiva e
dimostriamo che <span class="math inline">\(ker f = 0\)</span>; per
farlo consideriamo <span class="math inline">\(v \in ker f\)</span> e
mostriamo che deve essere <span class="math inline">\(v = 0\)</span>;
dato che <span class="math inline">\(v \in ker f\)</span>, abbiamo che
<span class="math inline">\(f(v) = 0\)</span>; d’altra parte <span
class="math inline">\(f(0) = 0\)</span>; dato che <span
class="math inline">\(f\)</span> è iniettiva, ciò è possibile solo se
<span class="math inline">\(v = 0\)</span>.<br />
“<span class="math inline">\(\Leftarrow\)</span>”<br />
Supponiamo <span class="math inline">\(ker f = 0\)</span> e mostriamo
che <span class="math inline">\(f\)</span> è iniettiva; per farlo
consideriamo <span class="math inline">\(v_1, v_2 \in V\)</span> tali
che <span class="math inline">\(f(v_1) = f(v_2)\)</span> e mostriamo che
<span class="math inline">\(v_1 = v_2\)</span>; se vale <span
class="math inline">\(f(v_1) = f(v_2)\)</span>, allora <span
class="math inline">\(f(v_1) - f(v_2) = 0\)</span>, quindi (uso <span
class="math inline">\(AL1/2\)</span>) <span class="math inline">\(f(v_1
- v_2) = 0\)</span>, dunque <span class="math inline">\(v_1 - v_2 \in
ker f\)</span>, pertanto, dato che <span class="math inline">\(ker
f\)</span> è dato in questo caso dal solo elemento neutro, abbiamo <span
class="math inline">\(v_1 - v_2 = 0\)</span>, ovvero <span
class="math inline">\(v_1 = v_2\)</span>.</p>
<p><strong>Teorema (di struttura per applicazioni
lineari)</strong><br />
Siano <span class="math inline">\(V\)</span> e <span
class="math inline">\(V&#39;\)</span> due spazi vettoriali su <span
class="math inline">\(K\)</span> di dimensione finita (non è necessario
che <span class="math inline">\(V\)</span> e <span
class="math inline">\(V&#39;\)</span> abbia la stessa dimensione); sia
<span class="math inline">\(B = \{b_1, b_2, \dots, b_n\}\)</span> una
base di <span class="math inline">\(V\)</span> e siano <span
class="math inline">\(v&#39;_1, v&#39;_2, \dots, v&#39;_n \in
V&#39;\)</span> vettori qualsiasi (non c’è alcuna restrizione,
potrebbero essere anche tutti uguali o tutti nulli); allora esiste
un’unica applicazione lineare <span class="math inline">\(f : V
\rightarrow V&#39;\)</span> tale che <span class="math inline">\(f(v_i)
= v&#39;_i\)</span> per ogni <span class="math inline">\(i \in \{1, 2,
\dots, n\}\)</span>.</p>
<p><strong>Dimostrazione</strong><br />
Supponiamo che una tale applicazione lineare esista ì; sia <span
class="math inline">\(v \in V\)</span> (vogliamo capire chi sia <span
class="math inline">\(f(v)\)</span>); per ipotesi, <span
class="math inline">\(B\)</span> è una base di <span
class="math inline">\(V\)</span>, quindi <span
class="math inline">\(v\)</span> si scrive in maniera unica come <span
class="math inline">\(v = \lambda_1 v_1 + \dots + \lambda_n v_n\)</span>
con <span class="math inline">\(\lambda_i \in K\)</span>; allora</p>
<p><span class="math inline">\(f(v) = f(\lambda_1 v_1 + \dots +
\lambda_n v_n) \stackrel{AL1}{=} f(\lambda_1 v_1) + \dots + f(\lambda_n
v_n) \stackrel{AL2}{=} \lambda_1 f(v_1) + \dots + \lambda_n f(v_n) =
\lambda_1 v&#39;_1 + \dots + \lambda_n v&#39;_n\)</span></p>
<p>quindi l’immagine di <span class="math inline">\(v \in V\)</span> è
univocamente determinata dalle proprietà che abbiamo supposto essere
vere per <span class="math inline">\(f\)</span>; pertanto, se <span
class="math inline">\(f\)</span> esiste, essa è unica; dobbiamo ora
mostrare che <span class="math inline">\(f\)</span> esiste; per farlo
usiamo il suggerimento che ci è dato dall’argomento usato appena qui
sopra, ovvero, se <span class="math inline">\(v \in V\)</span>,
definiamo <span class="math inline">\(f(v)\)</span> nel modo seguente:
scriviamo <span class="math inline">\(v\)</span> come combinazione
lineare in modo unico di <span class="math inline">\(B\)</span>, dunque
<span class="math inline">\(v = \lambda_1 v_1 + \dots + \lambda_n
v_n\)</span> e definiamo <span class="math inline">\(f(v) = \lambda_1
v&#39;_1 + \dots + \lambda_n v&#39;_n\)</span>, avendola definita in
questa maniera, segue immediatamente che <span
class="math inline">\(f(v_i) = v&#39;_i\)</span> per ogni <span
class="math inline">\(i \in \{1, 2, \dots, n\}\)</span>, infatti <span
class="math inline">\(v_i = 0 \cdot v_1 + \dots + \cdot v_i + \dots + 0
\cdot v_n\)</span>, quindi <span class="math inline">\(f(v_i) = 0 \cdot
v&#39;_1 + \dots + \cdot v&#39;_n = v&#39;_i\)</span>; l’ultima cosa che
dobbiamo mostrare è che <span class="math inline">\(f\)</span>, così
definita, è una applicazione lineare; siano quindi <span
class="math inline">\(u, v \in V\)</span>, dobbiamo mostrare che <span
class="math inline">\(f(u + v) = f(u) + f(v)\)</span>, scriviamo <span
class="math inline">\(u\)</span> e <span
class="math inline">\(v\)</span> come combinazioni lineari di <span
class="math inline">\(B\)</span>:<br />
<span class="math inline">\(u = \mu_1 v_1 + \dots + \mu_n
v_n\)</span><br />
<span class="math inline">\(v = \lambda_1 v_1 + \dots + \lambda_n
v_n\)</span><br />
da ciò segue che vale<br />
<span class="math inline">\(f(u) = \mu_1 v&#39;_1 + \dots + \mu_n
v&#39;_n\)</span><br />
<span class="math inline">\(f(v) = \lambda_1 v&#39;_1 + \dots +
\lambda_n v&#39;_n\)</span><br />
inoltre<br />
<span class="math inline">\((u + v) = (\mu_1 + \lambda_1) v_1 + \dots +
(\mu_n + \lambda_n) v_n\)</span><br />
quindi<br />
<span class="math inline">\(f(u + v) = (\mu_1 + \lambda_1) v&#39;_1 +
\dots + (\mu_n + \lambda_n) v&#39;_n =\)</span><br />
<span class="math inline">\(= (\mu_1 + v&#39;_1 + \dots + \mu_n +
v&#39;_n) + (\lambda_1 + v&#39;_1 + \dots + \lambda_n + v&#39;_n) = f(u)
+ f(v)\)</span></p>
<p>pertanto è additiva; analogamente si dimostra che <span
class="math inline">\(f\)</span> è omogenea.</p>
<p><strong>Esempio</strong><br />
Consideriamo in <span class="math inline">\(\mathbb{R}^2\)</span> la
base standard <span class="math inline">\(\xi = \{e_1,
e_2\}\)</span>,</p>
<p><span class="math inline">\(e_1 = \begin{pmatrix} 1 \\ 0
\end{pmatrix}, \quad e_2 = \begin{pmatrix} 0 \\ 1
\end{pmatrix}\)</span></p>
<p>consideriamo in <span class="math inline">\(\mathbb{R}^2\)</span> due
elementi</p>
<p><span class="math inline">\(w_1 = \begin{pmatrix} 2 \\ 3
\end{pmatrix}, \quad w_2 = \begin{pmatrix} -1 \\ 4
\end{pmatrix}\)</span></p>
<p>allora per il teorema di struttura delle applicazioni lineari esiste
ed è unica un’applicazione lineare <span class="math inline">\(f :
\mathbb{R}^2 \rightarrow \mathbb{R}^2\)</span> tale che<br />
<span class="math inline">\(f(e_1) = w_1\)</span> e <span
class="math inline">\(f(e_2) = w_2\)</span></p>
<p>chiediamoci: chi è l’immagine attraverso <span
class="math inline">\(f\)</span> di un generico elemento <span
class="math inline">\(\begin{pmatrix} x \\ y \end{pmatrix} \in
\mathbb{R}^2\)</span>?</p>
<p>vale che <span class="math inline">\(\begin{pmatrix} x \\ y
\end{pmatrix} = x e_1 + y e_2\)</span>; allora <span
class="math inline">\(f\left(\begin{pmatrix} x \\ y \end{pmatrix}\right)
= x w_1 + y w_2\)</span></p>
<p>quindi <span class="math inline">\(f\left(\begin{pmatrix} x \\ y
\end{pmatrix}\right) = x \begin{pmatrix} 2 \\ 3 \end{pmatrix} + y
\begin{pmatrix} -1 \\ 4 \end{pmatrix} = \begin{pmatrix} 2x -y \\ 3x + 4y
\end{pmatrix}\)</span></p>
<p><strong>Esempio</strong><br />
Non può esistere un’applicazione lineare <span class="math inline">\(f :
\mathbb{R}^2 \rightarrow \mathbb{R}^2\)</span> tale che</p>
<p><span class="math inline">\(f\left(\begin{pmatrix} 1 \\ 0
\end{pmatrix}\right) = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \quad
f\left(\begin{pmatrix} 0 \\ 1 \end{pmatrix}\right) = \begin{pmatrix} -1
\\ 1 \end{pmatrix}, \quad f\left(\begin{pmatrix} 1 \\ 1
\end{pmatrix}\right) = \begin{pmatrix} 12 \\ 17
\end{pmatrix}\)</span></p>
<p><strong>Osservazione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V&#39;\)</span> una
applicazione lineare tra spazi vettoriali di dimensione finita e sia
<span class="math inline">\(B = \{v_1, \dots, v_n\}\)</span> una base di
<span class="math inline">\(V\)</span>; allora <span
class="math inline">\(f(v_1), \dots, f(v_n)\)</span> sono un sistema di
generatori di <span class="math inline">\(im f\)</span>; infatti se
<span class="math inline">\(v&#39; \in im f\)</span>, allora esiste
<span class="math inline">\(v \in V\)</span> tale che <span
class="math inline">\(f(v) = v&#39;\)</span>; dato che <span
class="math inline">\(B\)</span> è una base, vale che <span
class="math inline">\(v = \lambda_1 v_1 + \dots + \lambda_n v_n\)</span>
per certi <span class="math inline">\(\lambda_1, \dots, \lambda_n \in
K\)</span>, quindi<br />
<span class="math inline">\(f(v) = f(\lambda_1 v_1 + \dots + \lambda_n
v_n) = f(\lambda_1 v_1) + \dots + f(\lambda_n v_n) = \lambda_1 f(v_1) +
\dots + \lambda_n f(v_n)\)</span></p>
<p>quindi <span class="math inline">\(v&#39;\)</span> è combinazione
lineare di <span class="math inline">\(f(v_1), \dots, f(v_n)\)</span>;
notiamo che abbiamo usato solo il fatto che <span
class="math inline">\(B\)</span> è n sistema di generatori per <span
class="math inline">\(V\)</span>.<br />
Sintetizzando: “le immagini di un sistema di generatori sono run sistema
di generatori per l’immagine dell’applicazione lineare”.</p>
<p><strong>Osservazione</strong><br />
Consideriamo una matrice <span class="math inline">\(A \in M_{m, n}
(K)\)</span>; allora abbiamo</p>
<p><span class="math inline">\(L_A : K^n \rightarrow K^m\)</span><br />
<span class="math inline">\(v \mapsto A \cdot v\)</span></p>
<p>se in <span class="math inline">\(K^n\)</span> prendiamo la base
standard <span class="math inline">\(\xi = \{e_1, \dots, e_n\}\)</span>,
dove</p>
<p><span class="math inline">\(e_i = \begin{pmatrix} 0 \\ \vdots \\ 1
&amp; (i) \text{ cioè l&#39;uno va in posizione i-esima}\\ \vdots \\ 0
&amp; \end{pmatrix}\)</span></p>
<p>allora vale che <span class="math inline">\(A \cdot e_i =
A^{(i)}\)</span>; dato che <span class="math inline">\(\xi\)</span> è
una base di <span class="math inline">\(K^n\)</span>, abbiamo che <span
class="math inline">\(L_A = span \Big( L_A(e_1), \dots, L_A(e_n) \Big) =
span \Big( A^{(1)}, \dots, A^{(n)} \Big)\)</span></p>
<p>pertanto <span class="math inline">\(dim \ im L_A = dim \ span \Big(
A^{(1)}, \dots, A^{(n)} \Big) = rg A\)</span></p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V&#39;\)</span>
applicazione lineare tra spazi vettoriali di dimensione finita,
definiamo il rango di <span class="math inline">\(f\)</span> come <span
class="math inline">\(dim im f\)</span>.</p>
<p>Data l’osservazione precedente, il rango di un’applicazione lineare è
una generalizzazione del rango di una matrice.</p>
<p><strong>Teorema (di dimensione per applicazioni
lineari)</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V&#39;\)</span>
applicazione lineare tra spazi vettoriali di dimensione finita, vale
allora<br />
<span class="math inline">\(dim V = dim \ ker f + dim \ im
f\)</span><br />
o, in altre parole<br />
<span class="math inline">\(dim V = dim \ ker f + rg f\)</span></p>
<p><strong>Dimostrazione</strong><br />
Sia <span class="math inline">\(n = dim V\)</span> e fissiamo una base
<span class="math inline">\(B_{ker}\)</span> di <span
class="math inline">\(ker f\)</span>; sia <span
class="math inline">\(B_{ker} = \{v_1, \dots, v_k\}\)</span>, dunque
<span class="math inline">\(dim \ ker f = k\)</span>; ora, per
costruzione <span class="math inline">\(v_1, \dots, v_k\)</span> sono
linearmente indipendenti, dunque essi possono essere estesi a una base
di <span class="math inline">\(V\)</span> (teorema di estensione); sia
essa</p>
<p><span class="math inline">\(B = \{v_1, \dots, v_k, v_{k+1}, \dots,
v_n\}\)</span></p>
<p>raggiungiamo il nostro scopo se riusciamo a mostrare che <span
class="math inline">\(\{f(v_{k+1}), \dots, f(v_n)\}\)</span> è una base
di <span class="math inline">\(im f\)</span>, perché in tal caso abbiamo
che <span class="math inline">\(dim \ im f = n - k\)</span> e dunque
<span class="math inline">\(dim V = n = k + (n - k) = dim \ ker f + dim
\ im f\)</span>; dimostriamo dunque che <span
class="math inline">\(\{f(v_{k+1}), \dots, f(v_n)\}\)</span> è una base
di <span class="math inline">\(im f\)</span>;<br />
cominciamo mostrando che tali elementi sono linearmente indipendenti;
supponiamo quindi che esista una loro combinazione lineare nulla:<br />
<span class="math inline">\(a_{k+1} f(v_{k+1}) + \dots + a_n f(v_n) =
0\)</span> per certi <span class="math inline">\(a_{k+1}, \dots, a_n \in
K\)</span><br />
allora, dato che <span class="math inline">\(f\)</span> è lineare<br />
<span class="math inline">\(f(a_{k+1} v_{k+1} + \dots + a_n v_n) =
0\)</span><br />
allora <span class="math inline">\(a_{k+1} v_{k+1} + \dots + a_n v_n \in
ker f\)</span>, quindi<br />
<span class="math inline">\(a_{k+1} v_{k+1} + \dots + a_n v_n = b_1 v_1
+ \dots + b_k v_k\)</span> per certi <span class="math inline">\(b_1,
\dots, b_k \in K\)</span> dato che <span class="math inline">\(\{v_1,
\dots, v_k\}\)</span> è una base del nucleo, pertanto<br />
<span class="math inline">\(-b_1 v_1 - \dots -b_k v_k + a_{k+1} v_{k+1}
+ \dots + a_n v_n = 0\)</span><br />
e questa è una combinazione lineare nulla di <span
class="math inline">\(\{v_1, \dots, v_n\}\)</span>, la quale è una base
di <span class="math inline">\(V\)</span> e pertanto l’unica possibilità
è che sia<br />
<span class="math inline">\(-b_1 = 0, \dots, -b_k = 0, a_{k+1} = 0,
\dots, a_n = 0\)</span><br />
quindi i particolare <span class="math inline">\(a_{k+1} = 0, \dots, a_n
= 0\)</span>, e dunque <span class="math inline">\(f(v_{k+1}), \dots,
f(v_n)\)</span> sono linearmente indipendenti.</p>
<p>Dimostriamo che <span class="math inline">\(\{f(v_{k+1}), \dots,
f(v_n)\}\)</span> sono un sistema di generatori per <span
class="math inline">\(im f\)</span>; dall’osservazione precedente
sappiamo che <span class="math inline">\(\{f(v_1), \dots,
f(v_n)\}\)</span> è un sistema di generatori per <span
class="math inline">\(im f\)</span> dato che <span
class="math inline">\(\{v_1, \dots, v_n\}\)</span> è una base di <span
class="math inline">\(V\)</span>,<br />
d’altro canto, dato che <span class="math inline">\(v_1, \dots, v_k \in
ker f\)</span>:<br />
<span class="math inline">\(\{f(v_1), \dots, f(v_k), f(v_{k+1}), \dots,
f(v_n)\}\)</span><br />
pertanto <span class="math inline">\(span(f(v_1), \dots, f(v_n)) =
span(f(v_{k+1}), \dots, f(v_n))\)</span>, pertanto<br />
<span class="math inline">\(im f = span(f(v_{k+1}), \dots,
f(v_n))\)</span>.</p>
<p><strong>Esempio</strong><br />
Supponiamo che <span class="math inline">\(f : \mathbb{R}^3 \rightarrow
\mathbb{R}^4\)</span> sia un’applicazione lineare; allora supponiamo che
sicuramente <span class="math inline">\(f\)</span> non può essere
suriettiva; infatti per il teorema di dimensione abbiamo <span
class="math inline">\(dim \ im f = dim \ \mathbb{R}^3 - dim \ ker f \leq
3\)</span>, mentre <span class="math inline">\(dim \ \mathbb{R}^4 =
4\)</span> e quindi non potrà mai essere che <span
class="math inline">\(im f = \mathbb{R}^4\)</span>.</p>
<p><strong>Osservazione</strong><br />
Sia <span class="math inline">\(A \in M_{m, n} (K)\)</span> e considero
il sistema lineare omogeneo <span class="math inline">\(AX = 0\)</span>;
interpretiamo le sue soluzioni in termini dell’applicazione lineare
<span class="math inline">\(L_A\)</span>:<br />
{soluzioni di <span class="math inline">\(AX = 0\)</span>} = {<span
class="math inline">\(s \in K^n : A \cdot s = 0\)</span>} = {<span
class="math inline">\(s \in K^n : L_A(s) = 0\)</span>} = <span
class="math inline">\(ker L_A\)</span></p>
<p><strong>Corollario</strong><br />
Sia <span class="math inline">\(A \in M_{m, n} (K)\)</span>, allora la
dimensione del sottospazio vettoriale <span class="math inline">\(W
\subseteq K^n\)</span> delle soluzioni del sistema lineare omogeneo
<span class="math inline">\(AX = 0\)</span> è uguale a <span
class="math inline">\(n - rg A\)</span> (questo colma il vuoto lasciato
nella dimostrazione del teorema di struttura per sistemi lineari
qualsiasi).</p>
<p><strong>Dimostrazione</strong><br />
Abbiamo visto che <span class="math inline">\(W = ker L_A\)</span>; per
il teorema di dimensione, ricordando che <span class="math inline">\(L_A
: K^n \rightarrow K^m\)</span>, abbiamo <span class="math inline">\(dim
K^n = dim \ ker L_A + dim \ im L_A\)</span><br />
<span class="math inline">\(\Rightarrow n = dim W + rg L_A\)</span>
<span class="math inline">\(= dim W + rg A\)</span><br />
<span class="math inline">\(\Rightarrow dim W = n - rg A\)</span></p>
<p><strong>Osservazione</strong><br />
Sia <span class="math inline">\(A \in M_{m, n} (K)\)</span>,
consideriamo <span class="math inline">\(L_A : K^n \rightarrow
K^m\)</span>; dato che <span class="math inline">\(b \in K^m\)</span>,
interpretiamo che cosa significhi dire che <span class="math inline">\(b
\in im L_A\)</span>; vale che<br />
<span class="math inline">\(b \in im L_A \iff\)</span> esiste <span
class="math inline">\(s \in K^n\)</span> tale che <span
class="math inline">\(L_A(s) = b\)</span><br />
<span class="math inline">\(\iff\)</span> esiste <span
class="math inline">\(s \in K^n\)</span> tale che <span
class="math inline">\(A \cdot s = b\)</span><br />
<span class="math inline">\(\iff\)</span> il sistema lineare <span
class="math inline">\(AX = b\)</span> è compatibile.</p>
<p><strong>Corollario</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V&#39;\)</span>
applicazione lineare tra spazi di dimensione finita e supponiamo <span
class="math inline">\(dim V = dim V?\)</span>; allora le seguenti
condizioni sono equivalenti:</p>
<ol type="1">
<li><span class="math inline">\(f\)</span> è iniettiva</li>
<li><span class="math inline">\(f\)</span> è suriettiva</li>
</ol>
<p><strong>Dimostrazione</strong><br />
<span class="math inline">\(1. \Rightarrow 2.\)</span><br />
Supponiamo <span class="math inline">\(f\)</span> iniettiva, allora
<span class="math inline">\(ker f = (0)\)</span>, allora dal teorema di
dimensione<br />
<span class="math inline">\(dim V = dim \ ker f + dim \ im
f\)</span><br />
quindi <span class="math inline">\(dim \ im f = dim V = dim
V&#39;\)</span>, pertanto <span class="math inline">\(im f =
V&#39;\)</span> e dunque <span class="math inline">\(f\)</span> è
suriettiva.</p>
<p><span class="math inline">\(2. \Rightarrow 1.\)</span><br />
Supponiamo <span class="math inline">\(f\)</span> suriettiva, allora
<span class="math inline">\(im f = V&#39;\)</span>, allora dal teorema
di dimensione<br />
<span class="math inline">\(dim V = dim \ ker f + dim \ im f = dim \ ker
f + dim V&#39;\)</span><br />
<span class="math inline">\(\Rightarrow\)</span> <span
class="math inline">\(dim \ ker f = 0\)</span><br />
<span class="math inline">\(\Rightarrow\)</span> <span
class="math inline">\(ker f = (0)\)</span><br />
<span class="math inline">\(\Rightarrow\)</span> <span
class="math inline">\(f\)</span> è iniettiva.</p>
<p><strong>Corollario</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V&#39;\)</span>
applicazione lineare tra spazi di dimensione finita, e sia <span
class="math inline">\(dim V = dim V&#39;\)</span>, allora<br />
<span class="math inline">\(f\)</span> iniettiva <span
class="math inline">\(\iff\)</span> <span
class="math inline">\(f\)</span> suriettiva <span
class="math inline">\(\iff\)</span> <span
class="math inline">\(f\)</span> è biettiva <span
class="math inline">\(\iff\)</span> <span
class="math inline">\(f\)</span> è invertibile</p>
<h2 id="matrici-associate">Matrici associate</h2>
<p>Introduciamo ora un’operazione che associa a un’applicazione lineare
(tra spazi di dimensione finita) una matrice.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V&#39;\)</span>
applicazione lineare tra spazi vettoriali di dimensione finita, sia
<span class="math inline">\(B\)</span> una base di <span
class="math inline">\(V\)</span> e sia <span
class="math inline">\(C\)</span> una base di <span
class="math inline">\(V&#39;\)</span>; siano <span
class="math inline">\(B = \{v_1, \dots, v_n\}\)</span>, <span
class="math inline">\(C = \{w_1, \dots, w_m\}\)</span>,<br />
definiamo la <strong>matrice associata</strong> ad <span
class="math inline">\(f\)</span> rispetto alle basi <span
class="math inline">\(B\)</span> e <span
class="math inline">\(C\)</span> come la matrice <span
class="math inline">\(M_C^B (f) \in M_{m, n} (K)\)</span> ottenuta nella
maniera seguente:<br />
per ogni <span class="math inline">\(v_i \in B\)</span>, scriviamo <span
class="math inline">\(f(v_i)\)</span> come combinazione lineare di <span
class="math inline">\(w_1, \dots, w_m\)</span>; i coefficienti di tale
matrice formano la colonna <span class="math inline">\(i\)</span>-esima
di <span class="math inline">\(M_C^B (f)\)</span>; in altre parole</p>
<p><span class="math inline">\(M_C^B (f) = \begin{pmatrix}  \vdots &amp;
&amp; &amp; &amp; \vdots \\  \vdots &amp; &amp; &amp; &amp; \vdots
\\  \text{coordinate di } f(v_1) \text{ rispetto a } C &amp; \dots &amp;
\dots &amp; \dots &amp; \text{coordinate di } f(v_n) \text{ rispetto a }
C \\  \vdots &amp; &amp; &amp; &amp; \vdots \\  \vdots &amp; &amp; &amp;
&amp; \vdots \end{pmatrix}\)</span></p>
<blockquote>
<p>Esempi</p>
</blockquote>
<p><strong>Teorema</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V&#39;\)</span>
applicazione lineare tra spazi di dimensione finita; sia <span
class="math inline">\(B\)</span> una base di <span
class="math inline">\(V\)</span> e sia <span
class="math inline">\(C\)</span> una base di <span
class="math inline">\(V&#39;\)</span>; sia <span class="math inline">\(v
\in V\)</span> e supponiamo che <span
class="math inline">\(\begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n
\end{pmatrix}\)</span> siano le coordinate di <span
class="math inline">\(v\)</span> rispetto a <span
class="math inline">\(B\)</span> (ovvero <span class="math inline">\(B =
\{v_1, \dots, v_n\}\)</span> e <span class="math inline">\(v = \alpha_1
v_1 + \dots + \alpha_n v_n\)</span>), quindi <span
class="math inline">\(\begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n
\end{pmatrix} \in K^n\)</span>; allora le coordinate di <span
class="math inline">\(f(v)\)</span> rispetto a <span
class="math inline">\(C\)</span> sono date da <span
class="math inline">\(M_C^B (f) \begin{pmatrix} \alpha_1 \\ \vdots \\
\alpha_n \end{pmatrix}\)</span>.</p>
<p><strong>Dimostrazione</strong><br />
Segue dalla definizione.</p>
<p><strong>Teorema</strong><br />
Siano <span class="math inline">\(f : V \rightarrow V&#39;\)</span> e
<span class="math inline">\(g : V&#39; \rightarrow V&#39;&#39;\)</span>
applicazioni lineari tra spazi di dimensione finita e siano <span
class="math inline">\(B\)</span> una base di <span
class="math inline">\(V\)</span>, <span class="math inline">\(C\)</span>
una base di <span class="math inline">\(V&#39;\)</span>, <span
class="math inline">\(D\)</span> una base di <span
class="math inline">\(V&#39;&#39;\)</span>, allora possiamo considerare
<span class="math inline">\(g \circ f : V \rightarrow
V&#39;&#39;\)</span> e abbiamo</p>
<p><span class="math inline">\(M_D^B (g \circ f) = M_D^{\rlap{/}{C}} (g)
\cdot M_{\rlap{/}{C}}^B (f)\)</span></p>
<p><strong>Corollario</strong><br />
Sia <span class="math inline">\(V\)</span> uno spazio vettoriale di
dimensione finita, siano <span class="math inline">\(B\)</span> e <span
class="math inline">\(C\)</span> basi di <span
class="math inline">\(V\)</span>, allora</p>
<p><span class="math inline">\(M_B^C (id_V) \cdot M_C^B (id_V) = M_B^B
(id_V \cdot id_V) = M_B^B (id_V) = 1_n\)</span></p>
<p>quindi <span class="math inline">\(M_B^C (id_V)\)</span> è l’inversa
di <span class="math inline">\(M_C^B (id_V)\)</span>.</p>
<hr />
<p>Data <span class="math inline">\(f : V \rightarrow V&#39;\)</span>
applicazione lineare tra spazi di dimensione finita.<br />
Supponiamo <span class="math inline">\(dim V = n\)</span>, <span
class="math inline">\(dim V&#39; = m\)</span>, fissiamo basi <span
class="math inline">\(B\)</span> di <span
class="math inline">\(V\)</span>$ e <span
class="math inline">\(C\)</span> di <span
class="math inline">\(V&#39;\)</span>,<br />
allora è determinata <span class="math inline">\(M_C^B
(f)\)</span><br />
(le sue colonne sono le coordinate degli elementi di <span
class="math inline">\(B\)</span> attraverso <span
class="math inline">\(f\)</span> rispetto a <span
class="math inline">\(C\)</span>).<br />
Abbiamo <span class="math inline">\(M_C^B (f) \in M_{m, n}
(K)\)</span></p>
<p><strong>Proprietà</strong><br />
<span class="math inline">\(i.\)</span> <span
class="math inline">\(M_B^B (id_V) = 1_n\)</span> (attenzione <span
class="math inline">\(M_C^B (id)\)</span> non è necessariamente <span
class="math inline">\(1_n\)</span>)</p>
<p><span class="math inline">\(ii.\)</span> <span
class="math inline">\(M_C^B (\text{applicazione nulla}) = \text{matrice
nulla}\)</span></p>
<p><span class="math inline">\(iii.\)</span> Se <span
class="math inline">\(f : V \rightarrow V&#39;\)</span> e <span
class="math inline">\(g : V&#39; \rightarrow V&#39;&#39;\)</span> sono
lineari,<br />
con <span class="math inline">\(B\)</span> base di <span
class="math inline">\(V\)</span>, <span class="math inline">\(C\)</span>
base di <span class="math inline">\(V&#39;\)</span>, <span
class="math inline">\(D\)</span> base di <span
class="math inline">\(V&#39;&#39;\)</span>, allora<br />
<span class="math inline">\(M_D^B (g \circ f) = M_D^C (g) \cdot M_C^B
(f)\)</span><br />
(prodotto righe per colonne)</p>
<p><span class="math inline">\(iv.\)</span> <span
class="math inline">\(M_C^B (id)\)</span> è l’inversa di <span
class="math inline">\(M_B^C (id)\)</span></p>
<p><span class="math inline">\(v.\)</span> <span
class="math inline">\(M_C^B (f + g) = M_C^B (f) + M_C^B (g)\)</span> per
ogni <span class="math inline">\(f, g : V \rightarrow
V&#39;\)</span></p>
<p><span class="math inline">\(vi.\)</span> <span
class="math inline">\(M_C^B (\lambda \cdot f) = \lambda \cdot M_C^B
(f)\)</span></p>
<p><strong>Osservazione</strong><br />
Se <span class="math inline">\(f : V \rightarrow V\)</span>,con <span
class="math inline">\(dim V = n\)</span>, è un isomorfismo (ovvero <span
class="math inline">\(f\)</span> è un’applicazione lineare ed è
biettiva, quindi invertibile), allora <span class="math inline">\(f^{-1}
: V&#39; \rightarrow V\)</span> è anch’essa lineare ed abbiamo che, se
<span class="math inline">\(B\)</span> è base di <span
class="math inline">\(V\)</span>, allora</p>
<p><span class="math inline">\(M_B^B (f) \cdot M_B^B (f^{-1}) = M_B^B (f
\cdot f^{-1}) = M_B^B (id) = 1_n\)</span></p>
<p>quindi <span class="math inline">\(M_B^B (f)\)</span> è invertibile e
la sua inversa è <span class="math inline">\(M_B^B (f^{-1})\)</span>,
ovvero</p>
<p><span class="math inline">\(\Big( M_B^B (f) \Big)^{-1} = M_B^B
(f^{-1})\)</span></p>
<p>RIcordiamo inoltre che se <span class="math inline">\(f : V
\rightarrow V&#39;\)</span> e <span class="math inline">\(B\)</span> è
base di <span class="math inline">\(V\)</span> e <span
class="math inline">\(C\)</span> è base di <span
class="math inline">\(V&#39;\)</span>, se <span class="math inline">\(v
\in V\)</span> e <span class="math inline">\(\begin{pmatrix} \alpha_1 \\
\vdots \\ \alpha_n \end{pmatrix}\)</span> è un vettore <span
class="math inline">\(m \times 1\)</span> ed è il vettore delle
coordinate di <span class="math inline">\(v\)</span> rispetto a <span
class="math inline">\(B\)</span>, abbiamo che <span
class="math inline">\(M_C^B (id_V) \cdot \begin{pmatrix} \alpha_1 \\
\vdots \\ \alpha_n \end{pmatrix}\)</span> è il vettore delle coordinate
di <span class="math inline">\(id_V (v) = v\)</span> rispetto alla base
<span class="math inline">\(C\)</span>. Pertanto, <span
class="math inline">\(M_C^B (id_V)\)</span> è la matrice del
<strong>cambio di base</strong>.</p>
<p>Da tutti questi risultati deriviamo che, se <span
class="math inline">\(f : V \rightarrow V&#39;\)</span> è una
applicazione lineare tra spazi vettoriali di dimensione finita e<br />
<span class="math inline">\(B\)</span> e <span
class="math inline">\(\tilde{B}\)</span> sono basi di <span
class="math inline">\(V\)</span><br />
<span class="math inline">\(C\)</span> e <span
class="math inline">\(\tilde{C}\)</span> sono basi di <span
class="math inline">\(V&#39;\)</span><br />
allora abbiamo che<br />
<span class="math inline">\(M_{\tilde{C}}^{\tilde{B}} (f) =
M_{\tilde{C}}^{\tilde{B}} (id_{V&#39;} \cdot f \cdot id_V) =
M_{\tilde{C}}^C (id_{v&#39;}) \cdot M_C^B (f) \cdot M_B^{\tilde{B}}
(id_V)\)</span><br />
Pertanto, se consideriamo <span class="math inline">\(M_C^B
(f)\)</span>, possiamo ottenere <span
class="math inline">\(M_{\tilde{C}}^{\tilde{B}} (f)\)</span>
moltiplicando a destra e a sinistra <span class="math inline">\(M_C^B
(f)\)</span> per die matrici di cambio di base.<br />
In particolare, se <span class="math inline">\(f : V \rightarrow
V\)</span> (attenzione, dominio e codominio qui coincidono) e se <span
class="math inline">\(B\)</span> e <span
class="math inline">\(C\)</span> sono due basi di <span
class="math inline">\(V\)</span>, allora<br />
<span class="math inline">\(M_C^C (f) = M_C^B (id_V) \cdot M_B^B (f)
\cdot M_B^C (id_V)\)</span><br />
Notiamo che se <span class="math inline">\(P = M_B^C (id_V)\)</span>,
allora <span class="math inline">\(M_C^B (id_V) = M_B^C (id_V)^{-1} =
P^{-1}\)</span><br />
Quindi l’uguaglianza precedente si può scrivere come<br />
<span class="math inline">\(M_C^C (f) = P^{-1} \cdot M_B^B (f) \cdot
P\)</span></p>
<p><strong>Definizione</strong><br />
Due matrici quadrate <span class="math inline">\(A, B \in M_n
(K)\)</span> si dicono <strong>simili</strong> se esiste una matrice
invertibile <span class="math inline">\(P \in M_n (K)\)</span> tale che
<span class="math inline">\(B = P^{-1} \cdot A \cdot P\)</span></p>
<p>Pertanto possiamo riassumere quanto ottenuto finora dicendo che se
<span class="math inline">\(f : V \rightarrow V\)</span> è
un’applicazione lineare con <span class="math inline">\(dim V =
n\)</span> e <span class="math inline">\(B\)</span> e <span
class="math inline">\(C\)</span> sono basi di <span
class="math inline">\(V\)</span>, allora <span
class="math inline">\(M_B^B (f) = M_C^C (f)\)</span> sono simili e
vale<br />
<span class="math inline">\(M_C^C (f) = P^{-1} \cdot M_B^B (f) \cdot
P\)</span><br />
dove <span class="math inline">\(P = M_B^C (id_V)\)</span>.</p>
<p>Questo risultato ci consente quindi di determinare la matrice
associata ad una applicazione lineare rispetto a una base differente da
quella che potremmo aver considerato in partenza. La speranza è di
riuscire a trovare basi rispetto alle quali l’applicazione lineare abbia
una forma abbastanza semplice.<br />
Prima di passare a questo argomento, concludiamo con un risultato
generale.</p>
<p><strong>Definizione</strong><br />
Siano <span class="math inline">\(V, V&#39;\)</span> due spazi
vettoriali su <span class="math inline">\(K\)</span> di dimensione,
<span class="math inline">\(dim V = n\)</span>, <span
class="math inline">\(dim V&#39; = m\)</span>, definiamo</p>
<p><span class="math inline">\(\mathcal{L} (V, V&#39;) =\)</span>
{applicazioni lineari da <span class="math inline">\(V\)</span> in <span
class="math inline">\(V&#39;\)</span>}</p>
<p>abbiamo che, definendo la somma tra applicazioni in maniera
“puntuale” [<span class="math inline">\((f + g)(v) = f(v) +
g(v)\)</span>] e analogamente la moltiplicazione di una applicazione
lineare per uno scalare [<span class="math inline">\((\lambda f)(v) =
\lambda f(v)\)</span>], allora <span class="math inline">\(\mathcal{L}
(V, V&#39;)\)</span> diventa uno spazio vettoriale su <span
class="math inline">\(K\)</span>.</p>
<p><strong>Teorema</strong><br />
Nelle ipotesi della definizione precedente, fissato <span
class="math inline">\(B\)</span> una base di <span
class="math inline">\(V\)</span> e <span
class="math inline">\(C\)</span> una base di <span
class="math inline">\(V&#39;\)</span>, abbiamo che</p>
<p><span class="math inline">\(\mathcal{L} (V, V&#39;) \to M_{m, n}
(K)\)</span><br />
<span class="math inline">\(f \mapsto M_C^B (f)\)</span></p>
<p>è un’applicazione lineare ed è biettiva, ovvero è un isomorfismo.</p>
<h2 id="diagonalizzazione">Diagonalizzazione</h2>
<p><strong>Esempio</strong></p>
<p>Consideriamo nel piano <span
class="math inline">\(\mathbb{R}^2\)</span> la riflessione rispetto
all’asse delle ordinate, ovvero:</p>
<figure>
<img src="img/riflessione_rispetto_ordinate.png" width="400"
alt="riflessione rispetto all’asse delle ordinate" />
<figcaption aria-hidden="true">riflessione rispetto all’asse delle
ordinate</figcaption>
</figure>
<p>la riflessione <span class="math inline">\(\rho\)</span> è
un’applicazione lineare e se <span class="math inline">\(\xi = \{e_1,
e_2\}\)</span> è la base standard di <span
class="math inline">\(\mathbb{R}^2\)</span>, allora</p>
<p><span class="math inline">\(M_{\xi}^{\xi} (\rho) = \begin{pmatrix} -1
&amp; 0 \\ 0 &amp; 1 \end{pmatrix}\)</span></p>
<p>se ora considerassimo una retta <span
class="math inline">\(\ell\)</span> che passa per <span
class="math inline">\((0, 0)\)</span> e la riflessione <span
class="math inline">\(\rho_{\ell}\)</span>, rispetto alla retta <span
class="math inline">\(\ell\)</span>, abbiamo che</p>
<figure>
<img src="img/riflessione_rispetto_retta.png" width="500"
alt="riflessione rispetto alla retta \ell" />
<figcaption aria-hidden="true">riflessione rispetto alla retta <span
class="math inline">\(\ell\)</span></figcaption>
</figure>
<p>dal disegno possiamo notare che non è immediato comprendere come sia
fatta <span class="math inline">\(M_{\xi}^{\xi} (\rho_{\ell})\)</span>;
possiamo interpretare questa difficoltà nel determinare <span
class="math inline">\(M_{\xi}^{\xi} (\rho_{\ell})\)</span> come il fatto
di non aver scelto una base “adeguata” all’applicazione lineare;
considero ora una base “personalizzata” rispetto all’applicazione
lineare:</p>
<figure>
<img src="img/cambio_base_retta.png" width="500"
alt="base personalizzata" />
<figcaption aria-hidden="true">base personalizzata</figcaption>
</figure>
<p>allora se <span class="math inline">\(B = \{v_1, v_2\}\)</span>,
abbiamo che <span class="math inline">\(B\)</span> è una base di <span
class="math inline">\(\mathbb{R}^2\)</span> e cha vale <span
class="math inline">\(M_B^B (\rho_{\ell}) = \begin{pmatrix} 1 &amp; 0 \\
0 &amp; -1 \end{pmatrix}\)</span></p>
<p>Notiamo pertanto che cambiare base può essere efficace nell’ottenere
una matrice associata sufficientemente semplice. Notiamo inoltre che la
base che “ha funzionato” è costituita da vettori che sono mandati
dall’applicazione lineare in multipli di se stessi. Vediamo che questa
si rivelerà l’idea chiave.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V\)</span>
un’applicazione lineare con <span class="math inline">\(dim V\)</span>
finita; uno scalare <span class="math inline">\(\lambda \in K\)</span>
si dice <strong>autovalore</strong> (eigenvalue in inglese) per <span
class="math inline">\(f\)</span> se esiste <span class="math inline">\(v
\in V, v \neq 0\)</span>, tale che <span class="math inline">\(f(v) =
\lambda v\)</span></p>
<p><strong>Osservazione</strong><br />
Considerando <span class="math inline">\(\rho_{\ell}\)</span> come
nell’esempio precedente, abbiamo che <span
class="math inline">\(1\)</span> e <span
class="math inline">\(-1\)</span> sono autovalori di <span
class="math inline">\(\rho_{\ell}\)</span> (infatti <span
class="math inline">\(\rho_{\ell} (v_1) = 1 \cdot v_1\)</span> e <span
class="math inline">\(\rho_{\ell} (v_2) = (-1) \cdot v_2\)</span> ed
entrambi <span class="math inline">\(v_1\)</span> e <span
class="math inline">\(v_2\)</span> sono non nulli).</p>
<p><strong>Definizione</strong><br />
Data <span class="math inline">\(f\)</span> come sopra, l’insieme degli
autovalori di <span class="math inline">\(f\)</span> si dice
<strong>spettro</strong> di <span class="math inline">\(f\)</span> e si
indica <span class="math inline">\(Sp (f)\)</span>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(f\)</span> come sopra e sia <span
class="math inline">\(\lambda\)</span> un autovalore di <span
class="math inline">\(f\)</span>; diciamo che <span
class="math inline">\(v \in V\)</span> è un <strong>autovettore</strong>
(eigenvector in inglese) di <span class="math inline">\(f\)</span>
relativo a <span class="math inline">\(\lambda\)</span> se <span
class="math inline">\(f(v) = \lambda v\)</span>; definiamo
l’<strong>autospazio</strong> (eigenspace in inglese) di <span
class="math inline">\(\lambda\)</span> l’insieme degli autovettori di
<span class="math inline">\(\lambda\)</span> e lo denotiamo <span
class="math inline">\(Aut (\lambda)\)</span>.</p>
<p><strong>Osservazione</strong><br />
Affinché <span class="math inline">\(\lambda\)</span> sia autovalore,
deve esistere <span class="math inline">\(v \in V\)</span>, <span
class="math inline">\(v \neq 0\)</span>, tale che <span
class="math inline">\(f(v) = \lambda v\)</span>; se <span
class="math inline">\(\lambda\)</span> è autovalore; consideriamo
autovettore relativo a <span class="math inline">\(\lambda\)</span> ogni
vettore <span class="math inline">\(w \in V\)</span> tale che <span
class="math inline">\(f(w) = \lambda w\)</span>; in particolare <span
class="math inline">\(f(0) = \lambda \cdot 0 = 0\)</span>, dunque vale
che <span class="math inline">\(0 \in Aut (\lambda)\)</span> per ogni
autovalore <span class="math inline">\(\lambda\)</span>.</p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V\)</span>
applicazione lineare, con <span class="math inline">\(dim V\)</span>
finita, sia <span class="math inline">\(\lambda\)</span> un autovalore
di <span class="math inline">\(f\)</span>; allora l’autospazio di <span
class="math inline">\(\lambda\)</span>, <span class="math inline">\(Aut
(\lambda)\)</span>, è un sottospazio vettoriale di <span
class="math inline">\(V\)</span>.</p>
<p><strong>Dimostrazione</strong><br />
<span class="math inline">\(1.\)</span> Sia <span
class="math inline">\(v \in V\)</span>, sia <span
class="math inline">\(\mu \in K\)</span> e sia <span
class="math inline">\(v \in Aut (\lambda)\)</span>; dobbiamo mostrare
che <span class="math inline">\(\mu \cdot v \in Aut (\lambda)\)</span>;
per ipotesi <span class="math inline">\(f(v) = \lambda v\)</span>;
ora<br />
<span class="math inline">\(\mu \cdot v \in Aut (\lambda) \iff f(\mu
\cdot v) = \lambda \cdot (\mu \cdot v)\)</span><br />
d’altra parte <span class="math inline">\(f(\mu \cdot v) = \mu \cdot
f(v) = \mu \lambda v = \lambda \cdot (\mu \cdot v)\)</span></p>
<p><span class="math inline">\(2.\)</span> Siano <span
class="math inline">\(v_1, v_2 \in V\)</span>, e supponiamo <span
class="math inline">\(v_1, v_2 \in Aut (\lambda)\)</span>; dobbiamo
mostrare che <span class="math inline">\(v_1 + v_2 \in Aut
(\lambda)\)</span>; per ipotesi <span class="math inline">\(f(v_1) =
\lambda v_1\)</span> e <span class="math inline">\(f(v_2) = \lambda
v_2\)</span>, pertanto<br />
<span class="math inline">\(f(v_1 + v_2) = f(v_1) + f(v_2) = \lambda v_1
+ \lambda v_2 = \lambda (v_1 + v_2)\)</span><br />
dunque <span class="math inline">\(v_1 + v_2 \in Aut
(\lambda)\)</span></p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V\)</span>
applicazione lineare, con <span class="math inline">\(dim V\)</span>
finita e siano <span class="math inline">\(\lambda\)</span> e <span
class="math inline">\(\mu\)</span> due autovalori definiti di <span
class="math inline">\(f\)</span>; siano <span class="math inline">\(v_1
\in Aut (\lambda)\)</span> e <span class="math inline">\(v_2 \in Aut
(\mu)\)</span>; e supponiamo che <span class="math inline">\(v_1 \neq
0\)</span> e <span class="math inline">\(v_2 \neq 0\)</span>; allora
<span class="math inline">\(v_1\)</span> e <span
class="math inline">\(v_2\)</span> sono linearmente indipendenti.</p>
<p>L’obiettivo delle nostre considerazioni sarà capire se, data una
applicazione lineare, <span class="math inline">\(f : V \rightarrow
V\)</span> sia possibile determinare una base <span
class="math inline">\(B\)</span> di <span
class="math inline">\(V\)</span> tutta costituita da autovettori. In tal
caso, infatti, la matrice <span class="math inline">\(M_B^B (f)\)</span>
è <strong>diagonale</strong>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V\)</span>
applicazione lineare, <span class="math inline">\(dim V\)</span> finita;
<span class="math inline">\(f\)</span> si dice
<strong>diagonalizzabile</strong> se esiste una base <span
class="math inline">\(B\)</span> di <span
class="math inline">\(V\)</span> tale che <span
class="math inline">\(M_B^B (f)\)</span> sia diagonale.</p>
<h2 id="diagonalizzabilità">Diagonalizzabilità</h2>
<p><strong>Osservazione</strong><br />
Ricordiamo che se <span class="math inline">\(f : V \rightarrow
V\)</span> è applicazione lineare, <span class="math inline">\(dim
V\)</span> finita e e <span class="math inline">\(N = M_B^B (f)\)</span>
ed <span class="math inline">\(N&#39; = M_C^C (f)\)</span>, allora</p>
<p><span class="math inline">\(N&#39; = P^{-1} \cdot N \cdot
P\)</span></p>
<p>dove <span class="math inline">\(P\)</span> è una matrice
invertibile; pertanto possiamo dire che <span
class="math inline">\(f\)</span> è diagonalizzabile se e solo se, presa
una sua matrice associata <span class="math inline">\(M_B^B (f)\)</span>
rispetto ad una base <span class="math inline">\(B\)</span> di <span
class="math inline">\(V\)</span>, tale matrice è simile a una matrice
diagonale, ovvero se esiste <span class="math inline">\(P\)</span>
invertibile tale che <span class="math inline">\(P^{-1} \cdot M_B^B (f)
\cdot P\)</span> è diagonale.</p>
<p><strong>Proposizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V\)</span>
applicazione lineare, <span class="math inline">\(dim V\)</span> finita,
allora è diagonalizzabile se e solo se esiste una base <span
class="math inline">\(B\)</span> di <span
class="math inline">\(V\)</span> costituita tutta da autovettori.</p>
<p><strong>Dimostrazione</strong><br />
Se <span class="math inline">\(B = \{v_1, \dots, v_n\}\)</span> è una
base costituita da autovettori; e per ogni <span class="math inline">\(i
\in \{1, \dots, n\}\)</span>, <span class="math inline">\(v_i\)</span> è
un autovettore associato all’autovalore <span
class="math inline">\(\lambda_i\)</span> (ovvero vale che <span
class="math inline">\(f(v_i) = \lambda \cdot v_i\)</span>), allora</p>
<p><span class="math inline">\(M_B^B (f) = \begin{pmatrix} \lambda_1
&amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; \lambda_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots
&amp; \lambda_n \end{pmatrix}\)</span></p>
<p>ovvero tale matrice è diagonale (notiamo che non abbiamo supposto che
i <span class="math inline">\(\{\lambda_i\}\)</span> siano tutti
distinti).</p>
<p>Per comprendere se una tale base può esistere, andiamo a ripensare
agli autospazi in maniera differente. In particolare, andiamo a
ridimostrare che gli autospazi sono sottospazi vettoriali in una maniera
alternativa.</p>
<p>Sia <span class="math inline">\(f : V \rightarrow V\)</span>
applicazione lineare, <span class="math inline">\(dim V\)</span> finita
e sia <span class="math inline">\(\lambda \in K\)</span> un autovalore
di <span class="math inline">\(f\)</span>. Allora per definizione esiste
<span class="math inline">\(v \in V\)</span> con <span
class="math inline">\(v \neq 0\)</span> tale che <span
class="math inline">\(f(v) = \lambda \cdot v\)</span>. Ora<br />
<span class="math inline">\(f(v) = \lambda \cdot v \Longleftrightarrow
f(v) - \lambda v = 0 \Leftrightarrow f(v) - \lambda \cdot id_v (v) = 0
\Leftrightarrow (f - \lambda \cdot id) (v) = 0\)</span>.</p>
<p>Per definizione <span class="math inline">\((f - \lambda \cdot id) :=
f_{\lambda}\)</span></p>
<p>Allora <span class="math inline">\(f_{\lambda} : V \rightarrow
V\)</span> e <span class="math inline">\(v \in ker f_{\lambda}\)</span>.
Pertanto <span class="math inline">\(ker f_{\lambda} \neq (0)\)</span>,
perché <span class="math inline">\(v \neq 0\)</span>. Ciò significa che
<span class="math inline">\(f_{\lambda}\)</span> non è iniettiva.
Pertanto <span class="math inline">\(f_{\lambda}\)</span> non è
invertibile e dunque per qualsiasi base <span
class="math inline">\(B\)</span> di <span
class="math inline">\(V\)</span> vale che</p>
<p><span class="math inline">\(M_B^B (f_{\lambda})\)</span> non è
invertibile<br />
<span class="math inline">\(\Leftrightarrow\)</span> <span
class="math inline">\(det M_B^B (f_{\lambda}) = 0\)</span><br />
<span class="math inline">\(\Leftrightarrow det M_B^B (f - \lambda \cdot
id) = 0\)</span><br />
<span class="math inline">\(\Leftrightarrow det \Big(M_B^B (f)- \lambda
\cdot M_B^B (id) \Big) = 0\)</span><br />
<span class="math inline">\(\Leftrightarrow det \Big(M_B^B (f) - \lambda
\cdot 1_n \Big) = 0\)</span></p>
<p>Pertanto gli autovalori di <span class="math inline">\(f\)</span>
sono tutti e soli i valori <span class="math inline">\(\lambda \in
K\)</span> tali che <span class="math inline">\(det \Big(M_B^B (f) -
\lambda \cdot 1_n \Big) = 0\)</span> per qualsiasi base <span
class="math inline">\(B\)</span> di <span
class="math inline">\(V\)</span>.<br />
Inoltre <span class="math inline">\(Aut(\lambda) = \{v \in V : f(v) =
\lambda \cdot v\} = \{v \in V : (f - \lambda \cdot id)(v) = 0\} = \{v
\in V : f_{\lambda}(v) = 0\} = ker f_{\lambda}\)</span></p>
<p>Pertanto, essendo <span class="math inline">\(Aut(\lambda)\)</span>
il nucleo di una applicazione lineare, riotteniamo il risultato tale che
<span class="math inline">\(Aut(\lambda)\)</span> è un sottospazio
vettoriale di <span class="math inline">\(V\)</span>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V\)</span>
applicazione lineare, <span class="math inline">\(dim V\)</span> finita,
sia <span class="math inline">\(\lambda \in K\)</span> un autovalore di
<span class="math inline">\(f\)</span>, il numero <span
class="math inline">\(dim_k\ Aut(\lambda)\)</span> è detto la
<strong>molteplicità generica</strong> dell’autovettore <span
class="math inline">\(\lambda\)</span>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V\)</span>
applicazione lineare, <span class="math inline">\(dim V\)</span> finita
e consideriamo ora <span class="math inline">\(\lambda\)</span> come un
parametro, una variabile; forniamo il determinante</p>
<p><span class="math inline">\(det (M_B^B (f) - \lambda \cdot
id)\)</span></p>
<p>dove <span class="math inline">\(B\)</span> è una qualsiasi base di
<span class="math inline">\(V\)</span>; questa quantità è un polinomio
in <span class="math inline">\(\lambda\)</span> a coefficienti in <span
class="math inline">\(K\)</span> ed è detto il <strong>polinomio
caratteristico</strong> di <span class="math inline">\(f\)</span> ed è
denotato <span class="math inline">\(P_f (\lambda)\)</span>.</p>
<p><strong>Esempio</strong><br />
Consideriamo <span class="math inline">\(f : \mathbb{R}^2 \rightarrow
\mathbb{R}^2\)</span></p>
<p><span class="math display">\[
f =
\left(
  \begin{pmatrix}
    x \\
    y
  \end{pmatrix}
\right) =
\begin{pmatrix}
  x + y \\
  2x + 2y
\end{pmatrix}
\]</span></p>
<p>se consideriamo la base standard <span
class="math inline">\(\xi\)</span> di <span
class="math inline">\(\mathbb{R}^2\)</span>, <span
class="math inline">\(\xi = \{e_1, e_2\}\)</span>, allora</p>
<p><span class="math display">\[
M_{\xi}^{\xi} (f) =
\begin{pmatrix}
  1 &amp; 1 \\
  2 &amp; 2
\end{pmatrix}
\]</span></p>
<p>il polinomio caratteristico è dunque</p>
<p><span class="math display">\[
P_f (\lambda) = det \Big( M_{\xi}^{\xi} (f) - \lambda \cdot 1_2 \Big) =
\]</span></p>
<p><span class="math display">\[
= det \left(
  \begin{pmatrix}
    1 &amp; 1 \\
    2 &amp; 2
  \end{pmatrix} -
  \lambda \begin{pmatrix}
    1 &amp; 0 \\
    0 &amp; 1
  \end{pmatrix}
\right) =
\]</span></p>
<p><span class="math display">\[
= det \begin{pmatrix}
  \begin{pmatrix}
    1 - \lambda &amp; 1 \\
    2 &amp; 2 - \lambda
  \end{pmatrix}
\end{pmatrix} =
\]</span></p>
<p><span class="math display">\[
= (1 - \lambda)(2 - \lambda) - 1 \cdot 2 =
\]</span></p>
<p><span class="math display">\[
= 2 - \lambda - 2 \lambda + \lambda^2 - 2 = \lambda^2 - 3 \lambda
\]</span></p>
<p><strong>Osservazione</strong><br />
Per come abbiamo caratterizzato gli autovalori, abbiamo che gli
autovalori di <span class="math inline">\(f\)</span> sono tutti e solo
quei <span class="math inline">\(\overline{\lambda} \in K\)</span> tali
per cui <span class="math inline">\(P_f (\overline{\lambda}) =
0\)</span>, ovvero sono tutte e sole le radici del polinomio
caratteristico dell’applicazione <span
class="math inline">\(f\)</span>.</p>
<p><strong>Esempio</strong><br />
(continuando da quello prima)</p>
<p><span class="math inline">\(P_f (\lambda) = \lambda^2 - 3 \lambda =
\lambda \cdot (\lambda - 3)\)</span></p>
<p>quindi le radici di <span class="math inline">\(P_f\)</span> sono
<span class="math inline">\(\{0, 3\}\)</span>, otteniamo quindi che
<span class="math inline">\(0\)</span> e <span
class="math inline">\(3\)</span> sono gli autovalori di <span
class="math inline">\(f\)</span>; esistono dunque autovettori non nulli
<span class="math inline">\(v_1\)</span> (rispetto a <span
class="math inline">\(0\)</span>) e <span
class="math inline">\(v_2\)</span> (rispetto a <span
class="math inline">\(3\)</span>), dato che autovettori non nulli
rispetto ad autovettori distinti sono linearmente indipendenti,
otteniamo che <span class="math inline">\(\{v_1, v_2\}\)</span> sono
vettori linearmente indipendenti, e pertanto <span
class="math inline">\(B = \{v_1, v_2\}\)</span> è una base di <span
class="math inline">\(\mathbb{R}^2\)</span> e quindi <span
class="math inline">\(f\)</span> ammette una base di autovettori, ovvero
è diagonalizzabile; possiamo calcolare <span
class="math inline">\(v_1\)</span> e <span
class="math inline">\(v_2\)</span> determinando <span
class="math inline">\(ker (f_0) = ker (f - 0 \cdot id) = ker f\)</span>
e <span class="math inline">\(ker (f_3) = ker (f - 3 \cdot id)\)</span>
che sono rispettivamente <span class="math inline">\(Aut(0)\)</span> e
<span class="math inline">\(Aut(3)\)</span>.</p>
<p><strong>Definizione</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V\)</span>
applicazione lineare, <span class="math inline">\(dim V\)</span> finita
e sia <span class="math inline">\(P_f (\lambda)\)</span> il suo
polinomio caratteristico, supponiamo che <span
class="math inline">\(\overline{\lambda} \in K\)</span> sia un
autovalore per <span class="math inline">\(f\)</span>, ovvero <span
class="math inline">\(P_f (\overline{\lambda}) = 0\)</span>; per il
teorema di Ruffini vale che <span class="math inline">\(P_f (\lambda) =
(\lambda - \overline{\lambda}) \cdot g(\lambda)\)</span>; definiamo la
<strong>molteplicità algebrica</strong> di <span
class="math inline">\(\overline{\lambda}\)</span> come quel numero <span
class="math inline">\(m\)</span> tale per cui <span
class="math inline">\(P_f (\lambda) = (\lambda - \overline{\lambda})^m
\cdot \tilde{g}(\lambda)\)</span> e <span class="math inline">\(\lambda
- \overline{\lambda}\)</span> non divide <span
class="math inline">\(\tilde{g}(\lambda)\)</span>.</p>
<p><strong>Esempio</strong><br />
Se <span class="math inline">\(P_f (\lambda) = (\lambda - 5)^2 \cdot
(\lambda + 1)^3\)</span>, allora le radici di <span
class="math inline">\(P_f (\lambda)\)</span> sono <span
class="math inline">\(5\)</span> e <span
class="math inline">\(-1\)</span> e la molteplicità algebrica di <span
class="math inline">\(5\)</span> è <span
class="math inline">\(2\)</span>, mente la molteplicità algebrica di
<span class="math inline">\(-1\)</span> è <span
class="math inline">\(3\)</span>.</p>
<p><strong>Notazione</strong><br />
Denotiamo la molteplicità geometrica di <span
class="math inline">\(\overline{\lambda} =
m_g(\overline{\lambda})\)</span>. Definiamo la molteplicità algebrica di
<span class="math inline">\(\overline{\lambda} =
m_a(\overline{\lambda})\)</span>.</p>
<p><strong>Proposizione</strong><br />
Se <span class="math inline">\(f : V \rightarrow V\)</span> è
applicazione lineare con <span class="math inline">\(dim V\)</span>
finita e <span class="math inline">\(\overline{\lambda}\)</span> è un
autovalore per <span class="math inline">\(f\)</span>, allora <span
class="math inline">\(m_g(\overline{\lambda}) \leq
m_a(\overline{\lambda})\)</span>.</p>
<p><strong>Osservazione</strong><br />
Se <span class="math inline">\(f : V \rightarrow V\)</span> è
applicazione lineare con <span class="math inline">\(dim V = n\)</span>,
allora <span class="math inline">\(P_f (\lambda)\)</span> è un polinomio
di grado esattamente <span class="math inline">\(n\)</span>; pertanto la
somma delle molteplicità algebriche degli autovalori di <span
class="math inline">\(f\)</span> è al più <span
class="math inline">\(n\)</span>.</p>
<p>Supponiamo di avere un’applicazione lineare <span
class="math inline">\(f : V \rightarrow V\)</span> con <span
class="math inline">\(dim V = n\)</span> tale per cui il polinomio
caratteristico si scompone nel prodotto di <span
class="math inline">\(n\)</span> fattori lineari distinti, ovvero <span
class="math inline">\(P_f (\lambda) = (\lambda - \alpha_1) \cdot
(\lambda - \alpha_2) \cdot \ \ldots \ \cdot (\lambda -
\alpha_n)\)</span>, con <span class="math inline">\(\alpha_1, \alpha_2,
\dots, \alpha_n\)</span> tutti distinti. Allora <span
class="math inline">\(\alpha_1, \alpha_2, \dots, \alpha_n\)</span> sono
le radici di <span class="math inline">\(P_f (\lambda)\)</span> e dunque
sono autovalori per <span class="math inline">\(f\)</span>. Per ciascuno
di tali autovalori esiste almeno un autovettore non nullo. In questo
modo determiniamo <span class="math inline">\(v_1, v_2, \dots,
v_n\)</span> con <span class="math inline">\(v_i\)</span> autovettore
relativo ad <span class="math inline">\(\alpha_i\)</span>. Ora, <span
class="math inline">\(v_1, v_2, \dots, v_n\)</span> sono autovettori non
nulli relativi ad autovalori differenti, quindi sono linearmente
indipendenti; essendo essi <span class="math inline">\(n\)</span>
vettori in uno spazio vettoriale di dimensione <span
class="math inline">\(n\)</span>, essi sono una base di <span
class="math inline">\(V\)</span>. Pertanto in questo caso <span
class="math inline">\(f\)</span> è diagonalizzabile. Notiamo inoltre che
<span class="math inline">\(m_a (\alpha_i) = 1,\ \forall i\)</span> e
dunque deve essere <span class="math inline">\(m_g (\alpha_i) = 1,\
\forall i\)</span>, il che implica che <span class="math inline">\(dim \
Aut(\alpha_i) = 1,\ \forall i\)</span>, e dato che <span
class="math inline">\(v_1 \in Aut (\alpha_i)\)</span> e <span
class="math inline">\(v_i \neq 0\)</span>, quindi <span
class="math inline">\(Aut (\alpha_i) = span(v_i)\)</span>.</p>
<p>Vale un teorema più generale:</p>
<p><strong>Teorema (criterio di diagonalizzazione)</strong><br />
Sia <span class="math inline">\(f : V \rightarrow V\)</span>
applicazione lineare con <span class="math inline">\(dim V\)</span>
finita. Allora <span class="math inline">\(f\)</span> è diagonalizzabile
se e solo se valgono le seguenti proprietà:</p>
<ol type="1">
<li><span class="math inline">\(P_f (\lambda)\)</span> si scompone
completamente in fattori di primo grado (non necessariamente
distinti).</li>
<li>per ogni autovalore <span
class="math inline">\(\overline{\lambda}\)</span> (ovvero per ogni
radice di <span class="math inline">\(P_f (\lambda)\)</span>) vale che
<span class="math inline">\(m_g(\overline{\lambda}) =
m_a(\overline{\lambda})\)</span>.</li>
</ol>
<p>(quindi <span class="math inline">\(1.\)</span> dice che <span
class="math inline">\(P_f (\lambda) = (\lambda - \alpha_1)^{m_1} \cdot
(\lambda - \alpha_2)^{m_2} \cdot \dots \cdot (\lambda -
\alpha_n)^{m_k}\)</span> e <span class="math inline">\(2.\)</span> dice
che <span class="math inline">\(m_i = dim \ Aut(\alpha_i)\)</span> per
ogni <span class="math inline">\(i \in \{1, 2, \dots, k\}\)</span>)</p>
<p><strong>Esempio</strong><br />
Consideriamo la seguente applicazione lineare</p>
<p><span class="math inline">\(f : \mathbb{R}^3 \rightarrow
\mathbb{R}^3\)</span></p>
<p><span class="math inline">\(f \left(\begin{pmatrix} x \\ y \\ z
\end{pmatrix} \right) = \begin{pmatrix} 2x -3z \\ -y \\ -3x +2z
\end{pmatrix}\)</span></p>
<p>allora se <span class="math inline">\(\xi\)</span> è la base standard
di <span class="math inline">\(\mathbb{R}^3\)</span>, vale che</p>
<p><span class="math inline">\(M_{\xi}^{\xi} (f) = \begin{pmatrix} 2
&amp; 0 &amp; -3 \\ 0 &amp; -1 &amp; 0 \\ -3 &amp; 0 &amp; 2
\end{pmatrix}\)</span></p>
<p>vogliamo comprendere se <span class="math inline">\(f\)</span> sia
diagonalizzabile o meno; calcoliamo <span class="math inline">\(P_f
(\lambda)\)</span></p>
<p><span class="math inline">\(P_f (\lambda) = det \Big( M_{\xi}^{\xi} -
\lambda \cdot 1_3 \Big) =\)</span></p>
<p><span class="math inline">\(= det \begin{pmatrix} 2 - \lambda &amp; 0
&amp; -3 \\ 0 &amp; -1 -\lambda &amp; 0 \\ -3 &amp; 0 &amp; 2 - \lambda
\end{pmatrix} =\)</span></p>
<p><span class="math inline">\(= (-1 - \lambda) det \begin{pmatrix} 2 -
\lambda &amp; -3 \\ -3 &amp; 2 - \lambda \end{pmatrix} =\)</span></p>
<p><span class="math inline">\(= -(1 + \lambda) \cdot \Big[ (2 -
\lambda)^2 - 9 \Big] =\)</span></p>
<p><span class="math inline">\(= -(1 + \lambda) \cdot \Big[ \lambda^2 -4
\lambda -5 \Big] =\)</span></p>
<p><span class="math inline">\(= -(1 + \lambda) (\lambda + 1) (\lambda -
5) =\)</span></p>
<p><span class="math inline">\(= -(1 + \lambda)^2 (\lambda -
5)\)</span></p>
<p>abbiamo quindi che le radici di <span class="math inline">\(P_f
(\lambda)\)</span> sono <span class="math inline">\(-1\)</span> e <span
class="math inline">\(-5\)</span>, ovvero <span class="math inline">\(Sp
(f) = \{ -1, -5 \}\)</span>; abbiamo che <span class="math inline">\(P_f
(\lambda)\)</span> si scompone completamente in fattori di grado <span
class="math inline">\(1\)</span>, e vale che<br />
<span class="math inline">\(m_a (-1) = 2, \quad m_a (5) =
1\)</span><br />
per vedere se <span class="math inline">\(f\)</span> sia
diagonalizzabile o meno, dobbiamo verificare se<br />
<span class="math inline">\(m_g (-1) = 2, \quad m_g (5) =
1\)</span><br />
sicuramente <span class="math inline">\(m_g (5) = 1\)</span> perché dal
fatto che <span class="math inline">\(5\)</span> è autovalore segue che
<span class="math inline">\(m_g (5) \geq 1\)</span> e in generale <span
class="math inline">\(m_g (5) \leq m_a (5) = 1\)</span>; resta da
verificare se <span class="math inline">\(m_g (-1) = 2 \iff dim \
Aut(-1) = 2\)</span><br />
per calcolare <span class="math inline">\(Aut (-1)\)</span>
consideriamo</p>
<p><span class="math inline">\(M_{\xi}^{\xi} (f) - (-1) 1_3
=\)</span></p>
<p><span class="math inline">\(M_{\xi}^{\xi} (f) + 1_3 =\)</span></p>
<p><span class="math inline">\(= \begin{pmatrix} 3 &amp; 0 &amp; -3 \\ 0
&amp; 0 &amp; 0 \\ -3 &amp; 0 &amp; 3 \end{pmatrix}\)</span></p>
<p><span class="math inline">\(ker \begin{pmatrix} 3 &amp; 0 &amp; -3 \\
0 &amp; 0 &amp; 0 \\ -3 &amp; 0 &amp; 3 \end{pmatrix} = span
\left(\begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 1 \\ 0
\\ 1 \end{pmatrix} \right)\)</span></p>
<p>pertanto <span class="math inline">\(dim \ ker (f - (-1) \cdot id) =
2\)</span>, ovvero <span class="math inline">\(mg (-1) = 2\)</span>;
quindi per il teorema precedente <span class="math inline">\(f\)</span>
è diagonalizzabile.</p>
<h1 id="geometria-affine">Geometria Affine</h1>
<p><strong>Spazio Affine:</strong></p>
<p>Sia <span class="math inline">\(V\)</span> uno spazio vettoriale su
un campo <span class="math inline">\(K\)</span> (ad esempio <span
class="math inline">\(K = \mathbb{R}\)</span>). Un insieme <span
class="math inline">\(A\)</span> si dice <strong>spazio affine</strong>
su <span class="math inline">\(V\)</span> se esiste una funzione <span
class="math inline">\(\sigma: A \times A \to V\)</span> (denotata <span
class="math inline">\(\sigma(P,Q) = \vec{PQ}\)</span> per ogni coppia
<span class="math inline">\(P, Q \in A\)</span>) che soddisfa:</p>
<ul>
<li><strong>SA1 (Esistenza e Unicità):</strong> <span
class="math inline">\(\forall P \in A\)</span> e <span
class="math inline">\(\forall v \in V\)</span>, esiste un unico <span
class="math inline">\(Q \in A\)</span> tale che <span
class="math inline">\(v = \vec{PQ}\)</span>.</li>
<li><strong>SA2 (Relazione di Chasles):</strong> <span
class="math inline">\(\forall P, Q, R \in A\)</span>, vale <span
class="math inline">\(\vec{PQ} + \vec{QR} = \vec{PR}\)</span>.</li>
</ul>
<p>Gli elementi di <span class="math inline">\(A\)</span> si dicono
<strong>punti</strong>.</p>
<p>Quando consideriamo <span class="math inline">\(A\)</span> come
spazio affine, lo denotiamo con <span
class="math inline">\(\mathbb{A}^n_K\)</span> e i suoi elementi come
vettori riga. Quando consideriamo <span
class="math inline">\(K^n\)</span> come spazio vettoriale, denotiamo i
suoi elementi come vettori colonna.</p>
<p><strong>Lemma:</strong> Sia <span class="math inline">\(A\)</span>
uno spazio affine su <span class="math inline">\(V\)</span>. Allora:</p>
<ol type="1">
<li><span class="math inline">\(\forall P \in A\)</span>, <span
class="math inline">\(\vec{PP} = 0\)</span> (vettore nullo in <span
class="math inline">\(V\)</span>).</li>
<li><span class="math inline">\(\forall P, Q \in A\)</span>, <span
class="math inline">\(\vec{PQ} = -\vec{QP}\)</span>.</li>
</ol>
<p><strong>Dimensione:</strong><br />
Sia <span class="math inline">\(A\)</span> uno spazio affine su <span
class="math inline">\(V\)</span>, con <span
class="math inline">\(V\)</span> di dimensione finita. La dimensione di
<span class="math inline">\(A\)</span> (dim <span
class="math inline">\(A\)</span>) è definita come la dimensione di <span
class="math inline">\(V\)</span>: dim <span
class="math inline">\(A\)</span> = dim <span
class="math inline">\(V\)</span>.</p>
<ul>
<li>dim <span class="math inline">\(A = 0\)</span>:
<strong>Punto</strong> affine</li>
<li>dim <span class="math inline">\(A = 1\)</span>:
<strong>Retta</strong> affine</li>
<li>dim <span class="math inline">\(A = 2\)</span>:
<strong>Piano</strong> affine</li>
<li>dim <span class="math inline">\(A = 3\)</span>:
<strong>Spazio</strong> affine</li>
</ul>
<p><strong>Riferimento Affine:</strong> Un riferimento affine su <span
class="math inline">\(A\)</span> è una coppia <span
class="math inline">\((O, B)\)</span>, dove <span
class="math inline">\(O \in A\)</span> e <span
class="math inline">\(B\)</span> è una base di <span
class="math inline">\(V\)</span>. Il punto <span
class="math inline">\(O\)</span> si dice <strong>origine</strong> del
riferimento affine. Dato un riferimento affine <span
class="math inline">\((O, B)\)</span> e dato <span
class="math inline">\(P \in A\)</span>, le coordinate di <span
class="math inline">\(P\)</span> rispetto al riferimento <span
class="math inline">\((O, B)\)</span> sono la <span
class="math inline">\(n\)</span>-upla <span class="math inline">\((p_1,
\dots, p_n)\)</span>, dove $n = $ dim <span
class="math inline">\(V\)</span>, data dalle coordinate nella base <span
class="math inline">\(B\)</span> del vettore <span
class="math inline">\(\vec{OP}\)</span>.</p>
<p><strong>Riferimento Standard:</strong> <span
class="math inline">\((O, E)\)</span>, dove <span
class="math inline">\(O = (0, \dots, 0)\)</span> ed <span
class="math inline">\(E\)</span> è la base standard di <span
class="math inline">\(K^n\)</span>. Le coordinate rispetto a <span
class="math inline">\((O, E)\)</span> sono dette <strong>coordinate
standard</strong>.</p>
<p><strong>Sottospazi Affini:</strong><br />
Sia <span class="math inline">\(A\)</span> uno spazio affine su <span
class="math inline">\(V\)</span>. Consideriamo <span
class="math inline">\(Q \in A\)</span> e <span class="math inline">\(W
\subseteq V\)</span> un sottospazio vettoriale. Il <strong>sottospazio
affine</strong> passante per <span class="math inline">\(Q\)</span> e
parallelo a <span class="math inline">\(W\)</span> è l’insieme <span
class="math inline">\(S = \{P \in A : \vec{QP} \in W\}\)</span>. <span
class="math inline">\(W\)</span> è la <strong>giacitura</strong> di
<span class="math inline">\(S\)</span>.</p>
<p><strong>Dimensione di un Sottospazio Affine:</strong> La dimensione
di <span class="math inline">\(S\)</span> è definita come la dimensione
di <span class="math inline">\(W\)</span>: dim <span
class="math inline">\(S\)</span> = dim <span
class="math inline">\(W\)</span>.</p>
<p><strong>Proposizione:</strong> Sia <span
class="math inline">\(A\)</span> uno spazio affine su <span
class="math inline">\(V\)</span> e <span class="math inline">\(S
\subseteq A\)</span> un sottospazio affine di giacitura <span
class="math inline">\(W\)</span> passante per <span
class="math inline">\(Q\)</span>. Allora:</p>
<ol type="1">
<li><span class="math inline">\(Q \in S\)</span></li>
<li><span class="math inline">\(P_1, P_2 \in S \implies \vec{P_1P_2} \in
W\)</span></li>
<li><span class="math inline">\(\forall R \in S\)</span>, <span
class="math inline">\(S\)</span> è il sottospazio affine passante per
<span class="math inline">\(R\)</span> di giacitura <span
class="math inline">\(W\)</span>.</li>
</ol>
<p><strong>Iperpiano:</strong> Sia <span
class="math inline">\(A\)</span> uno spazio affine su <span
class="math inline">\(V\)</span> e <span class="math inline">\(S
\subseteq A\)</span> un sottospazio affine. Se dim <span
class="math inline">\(A = n\)</span> e dim <span class="math inline">\(S
= n-1\)</span>, allora <span class="math inline">\(S\)</span> si dice un
<strong>iperpiano</strong>.</p>
<p><strong>Teorema:</strong> Gli insiemi di soluzioni di sistemi lineari
compatibili sono sottospazi affini le cui giaciture sono gli insiemi
delle soluzioni dei sistemi lineari omogenei associati.</p>
<p><strong>Equazioni per Sottospazi Affini:</strong></p>
<p><strong>Teorema:</strong> Sia <span class="math inline">\(A \in
M_{m,n}(K)\)</span> e <span class="math inline">\(b \in K^m\)</span>.
Supponiamo che il sistema lineare <span class="math inline">\(Ax =
b\)</span> sia compatibile e sia <span class="math inline">\(S\)</span>
l’insieme delle sue soluzioni. Allora <span
class="math inline">\(S\)</span> è un sottospazio affine, la cui
giacitura è il sottospazio vettoriale <span
class="math inline">\(W\)</span> delle soluzioni del sistema lineare
omogeneo associato <span class="math inline">\(Ax = 0\)</span>. Inoltre,
dim <span class="math inline">\(S\)</span> = dim <span
class="math inline">\(W = n - rg(A)\)</span>. Le equazioni <span
class="math inline">\(Ax = b\)</span> si dicono <strong>equazioni
cartesiane</strong> per <span class="math inline">\(S\)</span>.</p>
<p><strong>Equazioni Parametriche:</strong> Sia <span
class="math inline">\(P = (x_1, \dots, x_n)\)</span> un generico punto
di <span class="math inline">\(\mathbb{A}^n\)</span>. Sia <span
class="math inline">\(S\)</span> un sottospazio affine passante per
<span class="math inline">\(Q = (q_1, \dots, q_n)\)</span> e di
giacitura <span class="math inline">\(W \subseteq K^n\)</span>. Sia
<span class="math inline">\(W = \text{Span}(w_1, \dots, w_k)\)</span>,
dove <span class="math inline">\(w_i = (w_{i1}, \dots, w_{in})\)</span>.
L’espressione:</p>
<p><span class="math inline">\(x_j = q_j + t_1w_{1j} + \dots +
t_kw_{kj}, \quad j = 1, \dots, n\)</span></p>
<p>si dice <strong>equazione parametrica</strong> di <span
class="math inline">\(S\)</span>, dove <span class="math inline">\(t_1,
\dots, t_k\)</span> sono i parametri.</p>
<p><strong>Geometria del Piano Affine (<span
class="math inline">\(\mathbb{A}^2\)</span>)</strong>:</p>
<p>In <span class="math inline">\(\mathbb{A}^2\)</span>, dim <span
class="math inline">\(A = 2\)</span>. Sia <span
class="math inline">\(S\)</span> un sottospazio affine. Abbiamo tre
possibilità:</p>
<ul>
<li>dim <span class="math inline">\(S = 0\)</span>: <span
class="math inline">\(S\)</span> è un punto.</li>
<li>dim <span class="math inline">\(S = 1\)</span>: <span
class="math inline">\(S\)</span> è una retta.</li>
<li>dim <span class="math inline">\(S = 2\)</span>: <span
class="math inline">\(S = \mathbb{A}^2\)</span>.</li>
</ul>
<p><strong>Retta in <span
class="math inline">\(\mathbb{A}^2\)</span></strong>:</p>
<p>Una retta <span class="math inline">\(S\)</span> è determinata da un
punto <span class="math inline">\(Q = (q_1, q_2)\)</span> e dalla sua
giacitura <span class="math inline">\(W\)</span>. Dato che dim <span
class="math inline">\(S = 1\)</span>, allora dim <span
class="math inline">\(W = 1\)</span>, quindi <span
class="math inline">\(W = \text{Span}(v)\)</span>, dove <span
class="math inline">\(v = (l, m)\)</span>. Le equazioni parametriche di
un punto <span class="math inline">\(P = (x, y) \in S\)</span> sono:</p>
<ul>
<li><span class="math inline">\(x = q_1 + lt\)</span></li>
<li><span class="math inline">\(y = q_2 + mt\)</span></li>
</ul>
<p>Per ottenere l’equazione cartesiana, consideriamo il vettore <span
class="math inline">\(\vec{QP} = (x - q_1, y - q_2)\)</span> e imponiamo
che sia linearmente dipendente con <span class="math inline">\(v = (l,
m)\)</span>. Questo equivale a imporre che la matrice <span
class="math inline">\(\begin{pmatrix} x - q_1 &amp; l \\ y - q_2 &amp; m
\end{pmatrix}\)</span> abbia rango 1, ovvero che il suo determinante sia
nullo:</p>
<p><span class="math inline">\(m(x - q_1) - l(y - q_2) = 0\)</span></p>
<p>Questa è l’equazione cartesiana della retta.</p>
<p><strong>Rette Parallele in <span
class="math inline">\(\mathbb{A}^2\)</span></strong>:</p>
<p>Due rette <span class="math inline">\(r\)</span> e <span
class="math inline">\(s\)</span> in <span
class="math inline">\(\mathbb{A}^2\)</span> sono parallele se e solo se
hanno la stessa giacitura.</p>
<p><strong>Rette Incidenti in <span
class="math inline">\(\mathbb{A}^2\)</span></strong>:</p>
<p>Due rette distinte non parallele in <span
class="math inline">\(\mathbb{A}^2\)</span> si intersecano in un unico
punto.</p>
<p><strong>Geometria dello Spazio Affine (<span
class="math inline">\(\mathbb{A}^3\)</span>)</strong>:</p>
<p>In <span class="math inline">\(\mathbb{A}^3\)</span>, dim <span
class="math inline">\(A = 3\)</span>. Sia <span
class="math inline">\(S\)</span> un sottospazio affine. Abbiamo quattro
possibilità:</p>
<ul>
<li>dim <span class="math inline">\(S = 0\)</span>: <span
class="math inline">\(S\)</span> è un punto.</li>
<li>dim <span class="math inline">\(S = 1\)</span>: <span
class="math inline">\(S\)</span> è una retta.</li>
<li>dim <span class="math inline">\(S = 2\)</span>: <span
class="math inline">\(S\)</span> è un piano.</li>
<li>dim <span class="math inline">\(S = 3\)</span>: <span
class="math inline">\(S = \mathbb{A}^3\)</span>.</li>
</ul>
<p><strong>Retta in <span
class="math inline">\(\mathbb{A}^3\)</span></strong>:</p>
<p>Una retta in <span class="math inline">\(\mathbb{A}^3\)</span>
passante per <span class="math inline">\(Q = (q_1, q_2, q_3)\)</span> e
con giacitura <span class="math inline">\(W = \text{Span}(v)\)</span>,
dove <span class="math inline">\(v = (l, m, n)\)</span>, ha equazioni
parametriche:</p>
<ul>
<li><span class="math inline">\(x = q_1 + lt\)</span></li>
<li><span class="math inline">\(y = q_2 + mt\)</span></li>
<li><span class="math inline">\(z = q_3 + nt\)</span></li>
</ul>
<p>Per ottenere le equazioni cartesiane, imponiamo che la matrice <span
class="math inline">\(\begin{pmatrix} x - q_1 &amp; l \\ y - q_2 &amp; m
\\ z - q_3 &amp; n \end{pmatrix}\)</span> abbia rango 1.</p>
<p><strong>Piano in <span
class="math inline">\(\mathbb{A}^3\)</span></strong>:</p>
<p>Un piano in <span class="math inline">\(\mathbb{A}^3\)</span>
passante per <span class="math inline">\(Q = (q_1, q_2, q_3)\)</span> e
con giacitura <span class="math inline">\(W = \text{Span}(v_1,
v_2)\)</span>, dove <span class="math inline">\(v_1 = (l_1, m_1,
n_1)\)</span> e <span class="math inline">\(v_2 = (l_2, m_2,
n_2)\)</span> sono linearmente indipendenti, ha equazioni
parametriche:</p>
<ul>
<li><span class="math inline">\(x = q_1 + l_1t_1 + l_2t_2\)</span></li>
<li><span class="math inline">\(y = q_2 + m_1t_1 + m_2t_2\)</span></li>
<li><span class="math inline">\(z = q_3 + n_1t_1 + n_2t_2\)</span></li>
</ul>
<p>Per ottenere l’equazione cartesiana, imponiamo che la matrice <span
class="math inline">\(\begin{pmatrix} x - q_1 &amp; l_1 &amp; l_2 \\ y -
q_2 &amp; m_1 &amp; m_2 \\ z - q_3 &amp; n_1 &amp; n_2
\end{pmatrix}\)</span> abbia rango 2.</p>
<p><strong>Prodotto Scalare</strong>:</p>
<p>Il prodotto scalare tra due vettori <span class="math inline">\(v =
(v_1, \dots, v_n)\)</span> e <span class="math inline">\(w = (w_1,
\dots, w_n)\)</span> in <span
class="math inline">\(\mathbb{R}^n\)</span> è definito come:</p>
<p><span class="math inline">\(v \cdot w = v_1w_1 + \dots +
v_nw_n\)</span></p>
<p><strong>Norma</strong>: <span class="math inline">\(||v|| = \sqrt{v
\cdot v}\)</span></p>
<p><strong>Disuguaglianza di Cauchy-Schwarz</strong>: <span
class="math inline">\(|v \cdot w| \le ||v|| \cdot ||w||\)</span></p>
<p><strong>Corollario</strong>:<br />
<span class="math inline">\(- ||v|| \cdot ||w|| \le v \cdot w \le ||v||
\cdot ||w||\)</span><br />
<span class="math inline">\(\Rightarrow -1 \le \dfrac{v \cdot w}{||v||
\cdot ||w||} \le 1\)</span></p>
<p><strong>Base Ortonormale</strong>: Una base <span
class="math inline">\(B\)</span> di <span
class="math inline">\(\mathbb{R}^n\)</span> è ortonormale se i suoi
vettori sono a due a due ortogonali e hanno norma 1.</p>
<p><strong>Teorema Spettrale</strong>: Ogni matrice simmetrica <span
class="math inline">\(A \in M_n(\mathbb{R})\)</span> ammette una base
ortonormale di autovettori.</p>
<p>Cioè, ogni matrice simmetrica <span class="math inline">\(A \in
M_n(\mathbb{R})\)</span> (cioè <span class="math inline">\(A =
A^T\)</span>) è diagonalizzabile mediante una matrice ortogonale. In
altre parole, esiste una matrice ortogonale <span
class="math inline">\(P\)</span> (cioè <span class="math inline">\(P^T =
P^{-1}\)</span>) tale che <span class="math inline">\(P^T A P =
D\)</span>, dove <span class="math inline">\(D\)</span> è una matrice
diagonale. Gli elementi sulla diagonale di <span
class="math inline">\(D\)</span> sono gli autovalori di <span
class="math inline">\(A\)</span>, e le colonne di <span
class="math inline">\(P\)</span> sono gli autovettori corrispondenti,
che formano una base ortonormale di <span
class="math inline">\(\mathbb{R}^n\)</span>.</p>
<hr />
<p><a href="https://norbedo.xyz">norbedo.xyz</a><br />
<a
href="https://github.com/giovanni-norbedo">github.com/giovanni-norbedo</a><br />
<a
href="https://www.linkedin.com/in/norbedo">www.linkedin.com/in/norbedo</a></p>
</body>
</html>
